{"cells":[{"cell_type":"markdown","id":"bcaa0c44-0d89-4be3-96c8-f87bada85ad4","metadata":{},"outputs":[],"source":["<p>\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"c5c03771-5b02-4388-b65e-7d0e6c918749","metadata":{},"outputs":[],"source":["**<h1>Lab: Image processing with Pillow</h1>**\n"]},{"cell_type":"markdown","id":"e91e76d3-6497-4de3-ad1d-df1431a82bba","metadata":{},"outputs":[],"source":["Estimated time needed: **60** minutes\n"]},{"cell_type":"markdown","id":"034e1299-3740-4150-aee9-2364a9b08668","metadata":{},"outputs":[],"source":["<h2>Objectives</h2>\n"]},{"cell_type":"markdown","id":"1deda4d8-661f-468c-8351-cd66f5a9f4a0","metadata":{},"outputs":[],"source":["Image processing and computer vision tasks include displaying, cropping, flipping, rotating,  image segmentation, classification, image restoration,  image recognition, image generation.  Also, working with images via the cloud requires storing, transmitting, and gathering images through the internet. \n","\n","Python is an excellent choice as it has many image processing tools, computer vision and artificial intelligence libraries. Finally, it has many libraries for working with files in the cloud and on the internet.\n","\n","A digital image is simply a file in your computer. In this lab, you will gain an understanding of these files and learn to work with these files with the Pillow Library (PIL).\n"]},{"cell_type":"markdown","id":"ae4d13c4-1841-4662-8b6d-2c1259d79c82","metadata":{},"outputs":[],"source":["Table of Contents:\n","- [Image Files and Paths](#Image-Files-and-Paths)\n","- [Load Images in Python](#Load-Images-in-Python)  \n","- [Plotting an Image](#Plotting-an-Image)  \n","- [Grayscale Images, Quantization and Color Channels](#Grayscale-Images,-Quantization-and-Color-Channels)  \n","- [PIL Images into NumPy Arrays](#PIL-Images-into-NumPy-Arrays)\n"]},{"cell_type":"markdown","id":"6818287c-9a28-469f-bff0-8eaacc0de5f0","metadata":{},"outputs":[],"source":["----\n"]},{"cell_type":"markdown","id":"60f269a4-cc1d-4d3d-95f0-edc6d035373a","metadata":{},"outputs":[],"source":["Install required libraries for image processing and display\n"]},{"cell_type":"code","id":"db2d0015-0e5d-4432-9e44-86ce88d87e86","metadata":{},"outputs":[],"source":["!pip install Pillow\n!pip install matplotlib"]},{"cell_type":"markdown","id":"202c112f-4a0b-4a0f-b1b1-314f31a853a7","metadata":{},"outputs":[],"source":["Download the images for the lab:\n"]},{"cell_type":"code","id":"2788178f-06f1-44b2-8337-e0c32ffd7536","metadata":{},"outputs":[],"source":["!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/lenna.png -O lenna.png\n!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/baboon.png -O baboon.png\n!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/barbara.png -O barbara.png  "]},{"cell_type":"markdown","id":"c17b4671-1e57-47cf-9632-5ae49372176e","metadata":{},"outputs":[],"source":["First, let's define a helper function to concatenate two images side-by-side. You will not need to understand the code below at this moment, but this function will be used repeatedly in this tutorial to showcase the results.\n"]},{"cell_type":"code","id":"b1afd032-5aa6-42e4-b6dd-76e61b915caa","metadata":{},"outputs":[],"source":["def get_concat_h(im1, im2):\n    #https://note.nkmk.me/en/python-pillow-concat-images/\n    dst = Image.new('RGB', (im1.width + im2.width, im1.height))\n    dst.paste(im1, (0, 0))\n    dst.paste(im2, (im1.width, 0))\n    return dst"]},{"cell_type":"markdown","id":"4bdb7ff2-a9b9-46e5-89c6-04a7ba294f11","metadata":{},"outputs":[],"source":["## Image Files and Paths  \n"]},{"cell_type":"markdown","id":"daf8d0d3-0ce2-4df3-9b22-6328b23de6e3","metadata":{},"outputs":[],"source":["An image is stored as a file on your computer. Below, we define `my_image` as the filename of a file in this directory.\n"]},{"cell_type":"code","id":"c8b94415-9860-4f94-94cc-bb4a1ab56551","metadata":{},"outputs":[],"source":["my_image = \"lenna.png\""]},{"cell_type":"markdown","id":"bf5eb31f-4fd4-401a-a388-43b403544a55","metadata":{},"outputs":[],"source":["A filename consists of two parts: the name of the file and the extension, separated by a full stop (`.`). The extension specifies the format of the Image. There are two popular image formats: Joint Photographic Expert Group image (or `.jpg`, `.jpeg`) and Portable Network Graphics (or `.png`). These file types make it simpler to work with images. For example, it compresses the image, taking less spaces on your drive to store the image.\n"]},{"cell_type":"markdown","id":"29d4e797-d984-4d5f-bec1-2db270f51f2b","metadata":{},"outputs":[],"source":["Image files are stored in the file system of your computer. The location of it is specified using a \"path\", which is often unique. You can find the path of your current working directory with Python's `os` module. The `os` module provides functions to interact with the file system, e.g. creating or removing a directory (folder), listing its contents, changing and identifying the current working directory. \n"]},{"cell_type":"code","id":"00bd60b8-5ec6-4779-9775-b583633fb3b2","metadata":{},"outputs":[],"source":["import os\ncwd = os.getcwd()\ncwd "]},{"cell_type":"markdown","id":"6da50022-cd0b-44a1-b3ea-e8c66b0a1972","metadata":{},"outputs":[],"source":["The \"path\" to an image can be found using the following line of code.\n"]},{"cell_type":"code","id":"fea95b19-0a00-446f-8075-73a6e7494f02","metadata":{},"outputs":[],"source":["image_path = os.path.join(cwd, my_image)\nimage_path"]},{"cell_type":"markdown","id":"5cee719f-6d40-4e27-a999-b6733e3c30c4","metadata":{},"outputs":[],"source":["## Load Images in Python\n"]},{"cell_type":"markdown","id":"dea5f926-7936-4381-8b30-94d3d2218d65","metadata":{},"outputs":[],"source":["Pillow (PIL) library is a popular library for loading images in Python. In addition, many other libraries such as \"Keras\" and \"PyTorch\" use this library to work with images. The `Image` module provides functions to load images from and saving images to the file system. Let's import it from `PIL`.\n"]},{"cell_type":"code","id":"34909385-50a4-4358-a7ad-eb7cdc8cadf6","metadata":{},"outputs":[],"source":["from PIL import Image"]},{"cell_type":"markdown","id":"18514893-005c-473e-a804-d3f28eacca38","metadata":{},"outputs":[],"source":["If the image is in the current working directory, you can load the image as follows using the image's filename and create a PIL Image object:\n"]},{"cell_type":"code","id":"58ce6c1f-b921-4784-be8f-1fd7316df211","metadata":{},"outputs":[],"source":["image = Image.open(my_image)\ntype(image)"]},{"cell_type":"markdown","id":"6b40a60d-a98d-449d-86aa-ae531ee2a0ce","metadata":{},"outputs":[],"source":["If you are working in a Jupyter environment, you can view the image by calling the variable itself.  \n"]},{"cell_type":"code","id":"9346f2ca-017d-4699-9c63-843f6aacf36b","metadata":{},"outputs":[],"source":["image"]},{"cell_type":"markdown","id":"c430d513-8f27-4361-9590-646fd8e65814","metadata":{},"outputs":[],"source":["##  Plotting an Image \n"]},{"cell_type":"markdown","id":"fcf8bd4c-c247-43b0-a02b-a78a1f38d6d2","metadata":{},"outputs":[],"source":["You can also use <code>imshow</code> method from the <code>matplotlib</code> library to display the image.\n"]},{"cell_type":"code","id":"3770e7e0-5838-42bf-adb3-6ff4a1943b20","metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt"]},{"cell_type":"code","id":"c94e4e51-7adb-4b99-8f59-54d886b748d3","metadata":{},"outputs":[],"source":["plt.figure(figsize=(10,10))\nplt.imshow(image)\nplt.show()"]},{"cell_type":"markdown","id":"a7b3988a-e79a-48e5-9736-293a204b8a81","metadata":{},"outputs":[],"source":["You can also load the image using its full path. This comes in handy if the image is not in your working directory.\n"]},{"cell_type":"code","id":"4a34ff30-8510-492b-b92d-41f84d4ca436","metadata":{},"outputs":[],"source":["image = Image.open(image_path)"]},{"cell_type":"markdown","id":"08d12929-c77e-4401-94fa-4e1e8de1b348","metadata":{},"outputs":[],"source":["We can use the attributes of the image object to get information. The attribute format is the extension or format of the image.\n"]},{"cell_type":"markdown","id":"9003d318-49d9-4aa8-85db-8b9bf1b4f65f","metadata":{},"outputs":[],"source":["The attribute `size` returns a tuple, the first element is the number of pixels that comprise the width and the second element is the number of pixels that make up the height of the image.   \n"]},{"cell_type":"code","id":"9dc58390-9a72-44ab-8a56-f449de21950b","metadata":{},"outputs":[],"source":["print(image.size)"]},{"cell_type":"markdown","id":"17ee99d0-fee9-41d7-9d9d-388d481c1291","metadata":{},"outputs":[],"source":["This is a string specifying the pixel format used. In this case, it's “RGB”. RGB is a color space where red, green, and blue are added together to produce other colors.\n","\n"]},{"cell_type":"code","id":"5546e268-2a29-41a5-9fb2-839e6db0a5b6","metadata":{},"outputs":[],"source":["print(image.mode)"]},{"cell_type":"markdown","id":"189c0a1a-e32a-494b-b6f3-9716b5bea3be","metadata":{},"outputs":[],"source":["The `Image.open` method does not load image data into the computer memory. The `load` method of `PIL` object reads the file content, decodes it, and expands the image into memory.\n"]},{"cell_type":"code","id":"15af55e7-34fb-4bc1-aa99-1ed0e83d63f7","metadata":{},"outputs":[],"source":["im = image.load() "]},{"cell_type":"markdown","id":"f7a329b1-c0c9-4334-9668-51a6d96787fb","metadata":{},"outputs":[],"source":["We can then check the intensity of the image at the $x$-th column and $y$-th row:\n"]},{"cell_type":"code","id":"401664e0-31f1-4565-b047-08f9530f739e","metadata":{},"outputs":[],"source":["x = 0\ny = 1\nim[y,x]"]},{"cell_type":"markdown","id":"289058bf-1aaa-4ef1-b665-3a1c62b9ed38","metadata":{},"outputs":[],"source":["We will use the array form to access the elements; it is slightly different.\n"]},{"cell_type":"markdown","id":"d9cf456f-7905-4b62-89f5-1d926e20e5f0","metadata":{},"outputs":[],"source":["You can save the image in `jpg` format using the following command.\n"]},{"cell_type":"code","id":"d2910b60-68e0-4309-ad0e-8ab3771647cb","metadata":{},"outputs":[],"source":["image.save(\"lenna.jpg\")"]},{"cell_type":"markdown","id":"a90e16bf-3f38-497d-8bf5-cb9b27bdb434","metadata":{},"outputs":[],"source":["## Grayscale Images, Quantization and Color Channels  \n"]},{"cell_type":"markdown","id":"e28edcc3-a4fd-4169-b28f-3a1e21e24552","metadata":{},"outputs":[],"source":["### Grayscale Images\n"]},{"cell_type":"markdown","id":"07caf1fb-5687-405a-8f9a-0c145df7c0ed","metadata":{},"outputs":[],"source":["The `ImageOps` module contains several ‘ready-made’ image processing operations. This module is somewhat experimental, and most operators only work with grayscale and/or RGB images.\n"]},{"cell_type":"code","id":"e2293662-9a44-4ba3-ac86-6fcdc6ee5f5b","metadata":{},"outputs":[],"source":["from PIL import ImageOps "]},{"cell_type":"markdown","id":"90dd4102-89c2-42ed-b214-c8ceefdfc14c","metadata":{},"outputs":[],"source":["Grayscale images have pixel values representing the amount of light or intensity of that pixel. Light shades of gray have a high-intensity while darker shades have a lower intensity, i.e, white has the highest intensity and black the lowest.\n"]},{"cell_type":"code","id":"5f3899a1-0cdd-4cf0-b774-417641da4ef4","metadata":{},"outputs":[],"source":["image_gray = ImageOps.grayscale(image) \nimage_gray "]},{"cell_type":"markdown","id":"f05fa545-e77a-4dfc-949a-024b9c96cdb7","metadata":{},"outputs":[],"source":["The mode is `L` for grayscale.\n"]},{"cell_type":"code","id":"b20ec89f-6f3a-456a-a70a-c9266efd42af","metadata":{},"outputs":[],"source":["image_gray.mode"]},{"cell_type":"markdown","id":"d24917b9-edb5-4365-ad73-1f41983855e2","metadata":{},"outputs":[],"source":["### Quantization\n"]},{"cell_type":"markdown","id":"ae0ca462-ac55-49b6-bf69-682cb69ddfd4","metadata":{},"outputs":[],"source":["The Quantization of an image is the number of unique intensity values any given pixel of the image can take. For a grayscale image, this means the number of different shades of gray. Most images have 256 different levels. You can decrease the levels using the method `quantize`. Let's repeatably cut the number of levels in half and observe what happens:\n"]},{"cell_type":"markdown","id":"113bbeec-b3d7-44bb-9f4d-7944c9d98ff8","metadata":{},"outputs":[],"source":["Half the levels do not make a noticable difference.\n"]},{"cell_type":"code","id":"6ee3d663-c620-4b2d-b037-de1d3ce1b48d","metadata":{},"outputs":[],"source":["quantized_image = image_gray.quantize(256 // 2)\n\nplt.figure(figsize=(6, 6))\nplt.imshow(quantized_image.convert('RGB'))\nplt.title(\"Quantized Image (128 Levels)\")\nplt.axis(\"off\")\nplt.show()"]},{"cell_type":"markdown","id":"ed3f9d3a-048e-4ca7-8821-ca9087265077","metadata":{},"outputs":[],"source":["Let’s continue dividing the number of values by two and compare it to the original image.\n"]},{"cell_type":"code","id":"f93d7b24-f54a-47af-9664-8ecf887a6a70","metadata":{},"outputs":[],"source":["for n in range(3, 8):\n    # Get quantized version\n    quantized = image_gray.quantize(256 // 2**n)\n\n    # Convert both to RGB before concatenating for display\n    combined = get_concat_h(image_gray.convert('RGB'), quantized.convert('RGB'))\n\n    # Display using matplotlib\n    plt.figure(figsize=(10, 6))\n    plt.imshow(combined)\n    plt.title(f\"256 Quantization Levels (Left) vs {256 // 2**n} Levels (Right)\")\n    plt.axis(\"off\")\n    plt.show()"]},{"cell_type":"markdown","id":"64b6c67a-f148-4702-ac59-0fae0723e10c","metadata":{},"outputs":[],"source":["### Color Channels  \n"]},{"cell_type":"markdown","id":"4c6293af-dc50-4144-9178-ad2d81a30f44","metadata":{},"outputs":[],"source":["We can also work with the different color channels. Consider the following image:\n"]},{"cell_type":"code","id":"33b7cd9b-9dcb-4e7c-8829-8974d1e75e65","metadata":{},"outputs":[],"source":["baboon = Image.open('baboon.png')\nbaboon"]},{"cell_type":"markdown","id":"16cc996f-dbc5-4eae-aa29-19e914e5ecee","metadata":{},"outputs":[],"source":["We can obtain the different RGB color channels and assign them to the variables <code>red</code>, <code>green</code>, and <code>blue</code>:\n"]},{"cell_type":"code","id":"b6cd5293-a099-44ed-b696-4008342d7477","metadata":{},"outputs":[],"source":["red, green, blue = baboon.split()"]},{"cell_type":"markdown","id":"f28707d2-a35e-46ac-b2a4-47be56f9b7ff","metadata":{},"outputs":[],"source":["Plotting the color image next to the red channel as a grayscale, we see that regions with red have higher intensity values.\n"]},{"cell_type":"code","id":"9588b6a2-1314-46cb-91c7-db2b1957e0bf","metadata":{},"outputs":[],"source":["get_concat_h(baboon, red)"]},{"cell_type":"markdown","id":"f805491d-f177-44a9-b17a-7389fd56e81d","metadata":{},"outputs":[],"source":["We can do the same for the blue and green channels:\n"]},{"cell_type":"code","id":"82a177f2-854e-4f01-9327-88da66a99364","metadata":{},"outputs":[],"source":["get_concat_h(baboon, blue)"]},{"cell_type":"code","id":"ebcaadfe-405a-40d0-820a-4c1ca7c74c4e","metadata":{},"outputs":[],"source":["get_concat_h(baboon, green)"]},{"cell_type":"markdown","id":"216212a3-2585-4023-bf1c-868bd00643ca","metadata":{},"outputs":[],"source":["## PIL Images into NumPy Arrays\n"]},{"cell_type":"markdown","id":"3ff7b8f5-7a63-4cc9-83ab-9f329707f285","metadata":{},"outputs":[],"source":["NumPy is a library for Python, allowing you to work with multi-dimensional arrays and matrices. We can convert a PIL image to a NumPy array. We use <code>asarray()</code> or <code>array</code> function from NumPy to convert PIL images into NumPy arrays. \n","\n","First, let's import the numpy module:\n"]},{"cell_type":"code","id":"4969d14c-82d4-4a88-87cd-1a9f17c135ec","metadata":{},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","id":"057ac678-62b1-484f-b76f-6b9fe0b42e4e","metadata":{},"outputs":[],"source":["We apply it to the <code>PIL</code> image we get a numpy array:\n"]},{"cell_type":"code","id":"21327d83-cc92-4d31-9a5f-d989df4ed928","metadata":{},"outputs":[],"source":["array= np.asarray(image)\nprint(type(array))"]},{"cell_type":"markdown","id":"1ddcc1df-a8f6-42c7-80e4-c51cbdfa8b2d","metadata":{},"outputs":[],"source":["`np.asarray` turns the original image into a numpy array. Often, we don't want to manipulate the image directly, but instead, create a copy of the image to manipulate. The `np.array` method creates a new copy of the image, such that the original one will remain unmodified.\n"]},{"cell_type":"code","id":"2c7aee50-af4e-4ae7-8a8b-40bf88bcce8e","metadata":{},"outputs":[],"source":["array = np.array(image)"]},{"cell_type":"markdown","id":"1170cf8a-9c17-43cb-bbd8-29dcc88400eb","metadata":{},"outputs":[],"source":["The attribute  <code>shape</code> of a `numpy.array` object returns a tuple corresponding to the dimensions of it, the first element gives the number of rows or height of the image,  the second is element is the number of columns or width of the image. The final element is the number of colour channels.\n"]},{"cell_type":"code","id":"31727cd0-abb0-4b99-a5bc-6378d8b4ea3e","metadata":{},"outputs":[],"source":["# summarize shape\nprint(array.shape)"]},{"cell_type":"markdown","id":"65798b25-f38a-4957-ab2d-1617d6eba8a2","metadata":{},"outputs":[],"source":["or <code>(rows, columns, colors)</code>. Each element in the color axis  corresponds to the following value  <code>(R, G, B)</code> format.\n"]},{"cell_type":"markdown","id":"f581835f-0de3-4b0e-9274-b352d6e33fc2","metadata":{},"outputs":[],"source":["We can view the intensity values by printing out the array, they range from 0 to 255 or $2^{8}$ (8-bit).\n"]},{"cell_type":"code","id":"b581ee87-6a56-49b6-baa0-15398b3b4060","metadata":{},"outputs":[],"source":["print(array)"]},{"cell_type":"markdown","id":"4bb63fcb-8b44-4b17-8132-6118e710b16f","metadata":{},"outputs":[],"source":["The Intensity values are  8-bit unsigned datatype.\n"]},{"cell_type":"code","id":"91a1370e-a914-4d67-9cfb-36fa34bd1e4b","metadata":{},"outputs":[],"source":["array[0, 0]"]},{"cell_type":"markdown","id":"0ab513a3-1ccd-4f5e-bea8-55716904d515","metadata":{},"outputs":[],"source":["We can find the maximum and minimum intensity value of the array:      \n"]},{"cell_type":"code","id":"87d0742e-6981-4e9f-99ce-38e4afd3cc85","metadata":{},"outputs":[],"source":["array.min()"]},{"cell_type":"code","id":"7c35fe27-60df-448c-b76c-2a463dba506a","metadata":{},"outputs":[],"source":["array.max()"]},{"cell_type":"markdown","id":"c447991d-18f3-4f7e-990d-0f9a19b3e6fd","metadata":{},"outputs":[],"source":["### Indexing  \n"]},{"cell_type":"markdown","id":"d75bfff2-b570-44b9-8805-94a11821961a","metadata":{},"outputs":[],"source":["You can plot the array as an image:\n"]},{"cell_type":"code","id":"aae99ee2-cc0c-42be-842d-d06540c1c65a","metadata":{},"outputs":[],"source":["plt.figure(figsize=(10,10))\nplt.imshow(array)\nplt.show()"]},{"cell_type":"markdown","id":"81d1f37d-4a5e-4bbe-bc15-c56e09499eaf","metadata":{},"outputs":[],"source":["We can use numpy slicing, for example, we can return the first 256 rows corresponding to the top half of the image:\n"]},{"cell_type":"code","id":"1762eac0-87d2-442a-b7ae-03229c11f786","metadata":{},"outputs":[],"source":["rows = 256"]},{"cell_type":"code","id":"b7f31f57-a5ba-41dc-9575-3e2e9215a5e4","metadata":{},"outputs":[],"source":["plt.figure(figsize=(10,10))\nplt.imshow(array[0:rows,:,:])\nplt.show()"]},{"cell_type":"markdown","id":"64b56fda-9e60-4ddd-ab08-d744b7d30a67","metadata":{},"outputs":[],"source":["We can also return the first 256 columns corresponding to the first half of the image.\n"]},{"cell_type":"code","id":"acac98f7-f59e-4736-8c30-dc6c45f3655f","metadata":{},"outputs":[],"source":["columns = 256"]},{"cell_type":"code","id":"9c9490cf-ac06-489f-bf5e-98561e7c906d","metadata":{},"outputs":[],"source":["plt.figure(figsize=(10,10))\nplt.imshow(array[:,0:columns,:])\nplt.show()"]},{"cell_type":"markdown","id":"16e0cf69-b5be-4bef-8330-efa5789f0bdb","metadata":{},"outputs":[],"source":["If you want to reassign an array to another variable, you should use the `copy` method (we will cover this in the next section).\n"]},{"cell_type":"code","id":"46d03d51-dca4-490e-aa14-d2bb6629ddd5","metadata":{},"outputs":[],"source":["A = array.copy()\nplt.imshow(A)\nplt.show()"]},{"cell_type":"markdown","id":"e5514631-8bd1-4b47-8f99-17553878a989","metadata":{},"outputs":[],"source":["If we do not apply the method copy(), the variable will point to the same location in memory. Consider the array B. If we set all values of array A to zero, as B points to A, the values of B will be zero too:\n"]},{"cell_type":"code","id":"0304e2ec-31ec-425b-989b-545ab8c5bc84","metadata":{},"outputs":[],"source":["B = A\nA[:,:,:] = 0\nplt.imshow(B)\nplt.show()"]},{"cell_type":"markdown","id":"fe3419cb-81e2-4239-afc6-f3ccd30b08ea","metadata":{},"outputs":[],"source":["We can also work with the different color channels. Consider the baboon image: \n"]},{"cell_type":"code","id":"5a85b1a1-ce85-4802-be4b-b2b13eefe48b","metadata":{},"outputs":[],"source":["baboon_array = np.array(baboon)\nplt.figure(figsize=(10,10))\nplt.imshow(baboon_array)\nplt.show()"]},{"cell_type":"markdown","id":"f33c886e-f7f4-44a2-a371-608347a14741","metadata":{},"outputs":[],"source":["We can plot the red channel as intensity values of the red channel.\n"]},{"cell_type":"code","id":"28b4e3aa-34b5-4c09-be18-639a38ec1190","metadata":{},"outputs":[],"source":["baboon_array = np.array(baboon)\nplt.figure(figsize=(10,10))\nplt.imshow(baboon_array[:,:,0], cmap='gray')\nplt.show()"]},{"cell_type":"markdown","id":"2a1d4ecb-bbb6-468d-ba99-46045fed90b5","metadata":{},"outputs":[],"source":["Or we can create a new array and set all but the red color channels to zero. Therefore, when we display the image it appears red:\n"]},{"cell_type":"code","id":"a2e42f22-61bd-497a-8427-40c01d71b753","metadata":{},"outputs":[],"source":["baboon_red=baboon_array.copy()\nbaboon_red[:,:,1] = 0\nbaboon_red[:,:,2] = 0\nplt.figure(figsize=(10,10))\nplt.imshow(baboon_red)\nplt.show()"]},{"cell_type":"markdown","id":"cb94a750-78fc-4eb3-ab26-3070cfbd700c","metadata":{},"outputs":[],"source":["We can do the same for blue:\n"]},{"cell_type":"code","id":"059620c0-3433-40a6-8697-4deb48fe3258","metadata":{},"outputs":[],"source":["baboon_blue=baboon_array.copy()\nbaboon_blue[:,:,0] = 0\nbaboon_blue[:,:,1] = 0\nplt.figure(figsize=(10,10))\nplt.imshow(baboon_blue)\nplt.show()"]},{"cell_type":"markdown","id":"1b6d7241-898d-46af-8f1f-5095e3af2574","metadata":{},"outputs":[],"source":["### Question 1: \n","Use the image `lenna.png` from this lab or take any image you like.\n","\n","Open the image and create a PIL Image object called `blue_lenna`, convert the image into a numpy array we can manipulate called `blue_array`, get the blue channel out of it, and finally plot the image\n"]},{"cell_type":"code","id":"64642f69-8fad-4f56-aa9d-e637b0e2c93c","metadata":{},"outputs":[],"source":["# write your code here\n"]},{"cell_type":"markdown","id":"bfa81422-ebb9-4553-b1e5-912ab7797abf","metadata":{},"outputs":[],"source":["Double-click **here** for a hint.\n","\n","<!-- The hint is below:\n","\n","blue_array[:,:,2] = 0\n","\n","-->\n"]},{"cell_type":"markdown","id":"abc1b332-2100-4030-bc82-3d6ef1132481","metadata":{},"outputs":[],"source":["Double-click **here** for the solution.\n","\n","<!-- The answer is below:\n","\n","blue_lenna = Image.open('lenna.png')\n","blue_array = np.array(blue_lenna)\n","blue_array[:,:,2] = 0\n","plt.figure(figsize=(10,10))\n","plt.imshow(blue_array)\n","plt.show()\n","\n","-->\n"]},{"cell_type":"markdown","id":"0db42d11-03dd-4496-8af1-3144450a7459","metadata":{},"outputs":[],"source":["<h2>Authors</h2>\n"]},{"cell_type":"markdown","id":"d3abb063-cdc2-4c81-8ef6-23493da5de37","metadata":{},"outputs":[],"source":[" [Joseph Santarcangelo]( https://www.linkedin.com/in/joseph-s-50398b136/) has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"]},{"cell_type":"markdown","id":"16b35dda-3e3b-40d1-89e6-6f6298823732","metadata":{},"outputs":[],"source":[" [Nayef Abou Tayoun](https://www.linkedin.com/in/nayefaboutayoun/) has a master of management in artificial intelligence degree, focusing on using machine learning and computer vision. \n"]},{"cell_type":"markdown","id":"f9d24d89-18b7-4c05-a180-9822e89cc4d2","metadata":{},"outputs":[],"source":["# References \n"]},{"cell_type":"markdown","id":"d4452d47-bb7f-43fd-a38d-9cb02e1303ef","metadata":{},"outputs":[],"source":["[1]  Images were taken from: https://homepages.cae.wisc.edu/~ece533/images/\n","    \n","[2]  <a href='https://pillow.readthedocs.io/en/stable/index.html'>Pillow Docs</a>\n","\n","[3]  <a href='https://opencv.org/'>Open CV</a>\n","\n","[4] Gonzalez, Rafael C., and Richard E. Woods. \"Digital image processing.\" (2017).\n"]},{"cell_type":"markdown","id":"79557131-4fec-4fbe-b716-56313f8e9e6e","metadata":{},"outputs":[],"source":["<!--<h2>Change Log</h2>-->\n"]},{"cell_type":"markdown","id":"8efe2924-4ead-4a26-bd70-aa87e2a73543","metadata":{},"outputs":[],"source":["<!--\n","<table>\n","    <tr>\n","        <th>Date (YYYY-MM-DD)</th>\n","        <th>Version</th>\n","        <th>Changed By</th>\n","        <th>Change Description</th>\n","    </tr>\n","    <tr>\n","        <td>2020-07-20</td>\n","        <td>0.2</td>\n","        <td>Azim</td>\n","        <td>Modified Multiple Areas</td>\n","    </tr>\n","    <tr>\n","        <td>2020-07-17</td>\n","        <td>0.1</td>\n","        <td>Azim</td>\n","        <td>Created Lab Template</td>\n","    </tr>\n","    <tr>\n","        <td>2021-03-06</td>\n","        <td>0.3</td>\n","        <td>Nayef</td>\n","        <td>Modified some codes</td>\n","    </tr>\n","</table>\n","-->\n"]},{"cell_type":"markdown","id":"596f7939-3688-4ede-b3dd-034a2c7bfcc0","metadata":{},"outputs":[],"source":["\n","<h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"},"prev_pub_hash":"f616094739f13ffec02f84527c9cef065e4485e3e8070b37ecf6f30f63bbc24c"},"nbformat":4,"nbformat_minor":4}