{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33c76ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers datasets numpy pandas matplotlib seaborn tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "972338f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 노트북에서 사용할 라이브러리 임포트\n",
    "\n",
    "# 딥러닝 모델 및 계산 관련\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 처리 및 관리\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 유틸리티\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import textwrap\n",
    "\n",
    "# 시각화 스타일 설정 \n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "try:\n",
    "    plt.rc('font', family='NanumGothic') \n",
    "    plt.rc('axes', unicode_minus=False) # 마이너스 부호 깨짐 방지\n",
    "except:\n",
    "    print(\"나눔고딕 폰트를 찾을 수 없습니다. 기본 폰트로 설정됩니다. 그래프에 한글이 깨질 수 있습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e53dd1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Human gen text # : 100\n",
      "\n",
      "--- Sample text ---\n",
      "LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20\n",
      "million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a\n",
      "spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the\n",
      "disappointment of gossip columnists around the world, the young actor says he has no plans to\n",
      "fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those\n",
      "people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or\n",
      "something similar,\" he told an Australian interviewer earlier this month. \"I don't think I'll be\n",
      "particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books\n",
      "and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see\n",
      "the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box\n",
      "office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and\n",
      "publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in an\n",
      "interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first\n",
      "five Potter films have been held in a trust fund which he has not been able to touch. Despite his\n",
      "growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are\n",
      "always looking to say 'kid star goes off the rails,'\" he told reporters last month. \"But I try very\n",
      "hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard\n",
      "in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and\n",
      "he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter's latest\n",
      "» . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\"\n",
      "about author Rudyard Kipling and his son, due for release later this year. He will also appear in\n",
      "\"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he\n",
      "made his stage debut playing a tortured teenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced\n",
      "for even closer media scrutiny now that he's legally an adult: \"I just think I'm going to be more\n",
      "sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights\n",
      "reserved.This material may not be published, broadcast, rewritten, or redistributed.\n"
     ]
    }
   ],
   "source": [
    "# tldr_news(datasetnotfounderror) => cnn_dailymail\n",
    "# article , highlight , id 로 구성됨\n",
    "dataset = load_dataset(\"cnn_dailymail\",\"3.0.0\",split='train[:100]')\n",
    "\n",
    "human_texts = [example['article'] for example in dataset]\n",
    "\n",
    "print(f\"Total Human gen text # : {len(human_texts)}\")\n",
    "print(\"\\n--- Sample text ---\")\n",
    "\n",
    "wrapped_text = textwrap.fill(human_texts[0], width = 100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c69cd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AI 텍스트 생성 중:   0%|          | 0/3 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 텍스트 생성 중:  33%|███▎      | 1/3 [00:04<00:09,  4.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 텍스트 생성 중:  67%|██████▋   | 2/3 [00:09<00:04,  4.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 텍스트 생성 중: 100%|██████████| 3/3 [00:13<00:00,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total AI gen text #: 3\n",
      "\n",
      "--- Sample text ---\n",
      "The quick brown fox jumps over the lazy dog. The fox then runs to his home and looks for his\n",
      "friends. They will be waiting as they sleep.  The fox picks up an apple, tosses it in his mouth and\n",
      "slowly rolls over a dead tree.  He sits up and looks up from where he is waiting for his friends.\n",
      "\"Where's the apple?\" he asks, turning around.  The old fox answers, looking disappointed and scared\n",
      "at the fox. The old fox looks at him like he is angry. He laughs and looks down at the apple. He\n",
      "does not have an answer.  The fox goes back to his normal life. He does not have any friends, but he\n",
      "is always at\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm import tqdm\n",
    "import textwrap\n",
    "\n",
    "# 모델 이름 지정하기\n",
    "model_name = \"openai-community/gpt2\"\n",
    "\n",
    "# 토크나이저와 모델 불러오기\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# 패딩 토큰 설정 (모델이 문장 길이를 맞출 때 사용)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 장치 설정 (GPU 우선으로, 없으면 cpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# AI 텍스트 생성을 위한 원본 데이터 \n",
    "human_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is a field of computer science that focuses on creating intelligent machines.\",\n",
    "    \"Hugging Face is a company that provides tools for building applications using machine learning.\"\n",
    "]\n",
    "\n",
    "\n",
    "# AI 텍스트 생성 루프\n",
    "ai_texts = []\n",
    "for text in tqdm(human_texts, desc=\"AI 텍스트 생성 중\"):\n",
    "    # 프롬프트로 사용할 앞 15 단어 추출\n",
    "    prompt = \" \".join(text.split()[:15])\n",
    "\n",
    "    # 프롬프트 토큰화\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device) # <--- 'generator.tokenizer'가 아닌 'tokenizer' 사용\n",
    "\n",
    "    # 텍스트 생성\n",
    "    output = model.generate( \n",
    "        **inputs,\n",
    "        max_length=150,\n",
    "        do_sample=True, # more human-like\n",
    "        top_k=50, #\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "    # 생성된 텍스트 디코딩\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    ai_texts.append(generated_text)\n",
    "\n",
    "# 8. 결과 확인\n",
    "print(f\"\\nTotal AI gen text #: {len(ai_texts)}\")\n",
    "print(\"\\n--- Sample text ---\")\n",
    "# textwrap을 사용해 보기 좋게 출력\n",
    "wrapped_text = textwrap.fill(ai_texts[0], width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675edf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Generate Complete!!!!!!!!\n",
      "Total data : 6\n",
      "\n",
      " [Generated sample (top 5)]\n",
      "                                                text  label\n",
      "0  Artificial intelligence is a field of computer...      1\n",
      "1       The quick brown fox jumps over the lazy dog.      0\n",
      "2  Hugging Face is a company that provides tools ...      0\n",
      "3  Artificial intelligence is a field of computer...      0\n",
      "4  Hugging Face is a company that provides tools ...      1\n",
      "\n",
      "Dataset Info.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6 entries, 0 to 5\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    6 non-null      object\n",
      " 1   label   6 non-null      int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 224.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# 1-3. Dataframe으로 합치기 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Label\n",
    "human_labels = [0] * len(human_texts)\n",
    "ai_labels = [1] * len(ai_texts)\n",
    "\n",
    "# Combine \n",
    "all_texts = human_texts + ai_texts\n",
    "all_labels = human_labels + ai_labels\n",
    "\n",
    "# Pandas Dataframe\n",
    "data = {\n",
    "    'text': all_texts,\n",
    "    'label' : all_labels\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Shuffle using frac = 1\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Check\n",
    "print(\"Data Generate Complete!!!!!!!!\")\n",
    "print(f\"Total data : {len(df)}\")\n",
    "\n",
    "print(\"\\n [Generated sample (top 5)]\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset Info.\")\n",
    "df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef55ea37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 분할 완료!!!!!!!!!\n",
      "훈련 데이터 개수: 4개\n",
      "검증 데이터 개수: 2개\n",
      "\n",
      "텍스트 토큰화 완료!!!!!!!!!!\n",
      "\n",
      "[토큰화된 훈련 데이터 정보]\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "\n",
      "[첫 번째 훈련 데이터의 input_ids (일부)]\n",
      "tensor([  101, 20164, 10932, 10289,  1110,   170,  1419,  1115,  2790,  5537,\n",
      "         1111,  1459,  4683,  1606,  3395,  3776,   119,   102,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 사용할 모델의 토크나이저\n",
    "# 'bert-base-cased': 영어 텍스트 분류\n",
    "model_name = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#  데이터셋 into 훈련용, 검증용\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2, # 전체 데이터셋의 20%가 val\n",
    "    stratify=df['label'], #훈련/검증 데이터셋의 라벨 비율(0과 1의 비율)을 원본과 동일하게 유지\n",
    "    random_state=42 # 난수 시드는 고정 for 재현\n",
    ")\n",
    "\n",
    "print(\"데이터 분할 완료!!!!!!!!!\")\n",
    "print(f\"훈련 데이터 개수: {len(train_df)}개\")\n",
    "print(f\"검증 데이터 개수: {len(val_df)}개\")\n",
    "\n",
    "# 텍스트 데이터를 토큰화하여 모델 입력 형태로 변환\n",
    "# PyTorch 텐서 형태로 반환\n",
    "train_encodings = tokenizer(\n",
    "    list(train_df['text']),\n",
    "    truncation=True,      # 문장이 최대 길이를 넘으면 자르기\n",
    "    padding=True,         # 문장 길이를 맞추기 위해 패딩 추가\n",
    "    max_length=128,       # 최대 길이 설정\n",
    "    return_tensors='pt'   # 파이토치 텐서로 반환\n",
    ")\n",
    "\n",
    "val_encodings = tokenizer(\n",
    "    list(val_df['text']),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=128,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# 5. 토큰화 결과 확인\n",
    "print(\"\\n텍스트 토큰화 완료!!!!!!!!!!\")\n",
    "print(\"\\n[토큰화된 훈련 데이터 정보]\")\n",
    "# input_ids: 토큰의 숫자 ID\n",
    "# attention_mask: 패딩된 부분은 모델이 무시하도록 알려주는 역할\n",
    "print(train_encodings.keys())\n",
    "print(\"\\n[첫 번째 훈련 데이터의 input_ids (일부)]\")\n",
    "print(train_encodings['input_ids'][0][:20]) # 첫 번째 문장의 앞 20개 토큰 ID 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f41dd239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader 생성 완료!!!\n",
      "훈련 데이터 로더에서 배치 하나씩 꺼내보기\n",
      "\n",
      "[배치의 Key 확인]\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "[배치의 Shape 확인]\n",
      "Input IDs shape: torch.Size([2, 128])\n",
      "Labels shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# PyTorch를 위한 커스텀 Dataset 클래스 정의\n",
    "class AITextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # encodings 딕셔너리의 각 값(텐서)에 대해 idx에 해당하는 슬라이스를 가져옴\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        # 해당 아이템의 레이블을 텐서 형태로 추가\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 총 길이 반환\n",
    "        return len(self.labels)\n",
    "\n",
    "# 훈련 및 검증 데이터셋 생성\n",
    "# pandas의 iloc을 사용해 라벨 리스트를 올바르게 가져온다.\n",
    "train_labels = train_df['label'].iloc[:].tolist()\n",
    "val_labels = val_df['label'].iloc[:].tolist()\n",
    "\n",
    "train_dataset = AITextDataset(train_encodings, train_labels)\n",
    "val_dataset = AITextDataset(val_encodings, val_labels)\n",
    "\n",
    "# DataLoader 생성\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# batch_size는 모델이 한 번에 학습할 데이터의 양\n",
    "# 작은 데이터셋이므로 2로 설정하기\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# DataLoader 작동 확인\n",
    "print(\"DataLoader 생성 완료!!!\")\n",
    "print(\"훈련 데이터 로더에서 배치 하나씩 꺼내보기\")\n",
    "\n",
    "# iter와 next를 사용해 데이터 로더에서 첫 번째 배치를 꺼냄\n",
    "data_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"\\n[배치의 Key 확인]\")\n",
    "print(data_batch.keys())\n",
    "\n",
    "print(\"\\n[배치의 Shape 확인]\")\n",
    "# input_ids의 shape: [배치 사이즈, 최대 문장 길이]\n",
    "print(\"Input IDs shape:\", data_batch['input_ids'].shape)\n",
    "# labels의 shape: [배치 사이즈]\n",
    "print(\"Labels shape:\", data_batch['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d64523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "모델 및 옵티마이저 설정이 완료!!!!!!!!!!\n",
      "사용 모델: bert-base-cased\n",
      "라벨 개수: 2개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SeungWoo\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 가져오기\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "# 분류를 위한 BERT 모델 불러오기\n",
    "# model_name은 이전에 토크나이저를 불러올 때 사용한 이름과 동일해야 함\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2 #사람(0), AI(1)\n",
    ")\n",
    "\n",
    "# 모델을 GPU 또는 CPU 장치로 이동\n",
    "model.to(device)\n",
    "\n",
    "# 옵티마이저 설정\n",
    "# model.parameters()는 모델이 학습할 모든 파라미터(가중치)를 의미합니다.\n",
    "# lr = learning rate = alpha\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 5. 설정 완료 확인\n",
    "print(\"------------------------\")\n",
    "print(\"모델 및 옵티마이저 설정이 완료!!!!!!!!!!\")   \n",
    "print(f\"사용 모델: {model_name}\")\n",
    "print(f\"라벨 개수: {model.config.num_labels}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "065e649c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 훈련 손실 (Avg Train Loss): 0.8032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 검증 손실 (Avg Val Loss): 0.6519\n",
      "검증 정확도 (Val Accuracy): 0.5000\n",
      "--- Epoch 2/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 훈련 손실 (Avg Train Loss): 0.5870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 검증 손실 (Avg Val Loss): 0.5528\n",
      "검증 정확도 (Val Accuracy): 1.0000\n",
      "--- Epoch 3/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 훈련 손실 (Avg Train Loss): 0.5621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 검증 손실 (Avg Val Loss): 0.5395\n",
      "검증 정확도 (Val Accuracy): 1.0000\n",
      "\n",
      "훈련이 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. 필요한 라이브러리 추가\n",
    "from tqdm import tqdm # 학습 진행 상황을 보여주는 라이브러리\n",
    "import numpy as np\n",
    "\n",
    "# 2. 훈련 루프 설정\n",
    "# 에폭(Epoch): 전체 데이터를 몇 번 반복해서 학습할지 결정합니다.\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"--- Epoch {epoch + 1}/{epochs} ---\")\n",
    "\n",
    "    # --- 훈련 단계 (Training) ---\n",
    "    model.train() # 모델을 훈련 모드로 설정\n",
    "    total_train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        # 이전 배치의 그래디언트 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 데이터를 device로 이동\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # 모델에 데이터 입력\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        # 손실(loss) 계산\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # 역전파 (손실을 기반으로 그래디언트 계산)\n",
    "        loss.backward()\n",
    "        # 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"평균 훈련 손실 (Avg Train Loss): {avg_train_loss:.4f}\")\n",
    "\n",
    "    # --- 검증 단계 (Validation) ---\n",
    "    model.eval() # 모델을 평가 모드로 설정\n",
    "    total_val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad(): # 그래디언트 계산 비활성화\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            # 데이터를 device로 이동\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # 모델에 데이터 입력\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "            # 손실 및 예측 결과(logits) 가져오기\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            # 정확도 계산\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = correct_predictions / len(val_dataset)\n",
    "    print(f\"평균 검증 손실 (Avg Val Loss): {avg_val_loss:.4f}\")\n",
    "    print(f\"검증 정확도 (Val Accuracy): {val_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n훈련이 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1035fd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 새로운 문장 예측 테스트 ---\n",
      "입력 문장 1: \"I just came back from my trip to Jeju island. The weather was amazing and the food was delicious.\"\n",
      "모델 예측 : 사람이 작성한 글 (Human)\n",
      "\n",
      "입력 문장 2: \"The integration of artificial intelligence into daily life has accelerated transformative changes across various sectors.\"\n",
      "모델 예측 : 사람이 작성한 글 (Human)\n"
     ]
    }
   ],
   "source": [
    "# 1. 예측 결과를 해석하기 위한 라벨 맵 정의\n",
    "label_map = {0: \"사람이 작성한 글 (Human)\", 1: \"AI가 생성한 글 (AI)\"}\n",
    "\n",
    "# 2. 예측 함수 정의\n",
    "def predict(text):\n",
    "    # 모델을 평가 모드로 설정\n",
    "    model.eval()\n",
    "\n",
    "    # 입력된 텍스트를 토큰화\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt' # 파이토치 텐서로 반환\n",
    "    )\n",
    "\n",
    "    # 모든 텐서를 device로 이동\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    # 그래디언트 계산 비활성화\n",
    "    with torch.no_grad():\n",
    "        # 모델에 입력을 전달하여 예측 수행\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # 모델의 출력(logits)에서 가장 높은 값의 인덱스를 예측 라벨로 선택\n",
    "    predicted_label_id = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "    # 라벨 ID를 실제 라벨 이름으로 변환하여 반환\n",
    "    return label_map[predicted_label_id]\n",
    "\n",
    "\n",
    "# 3. 새로운 문장으로 예측 테스트\n",
    "test_sentence_1 = \"I just came back from my trip to Jeju island. The weather was amazing and the food was delicious.\"\n",
    "test_sentence_2 = \"The integration of artificial intelligence into daily life has accelerated transformative changes across various sectors.\"\n",
    "\n",
    "prediction_1 = predict(test_sentence_1)\n",
    "prediction_2 = predict(test_sentence_2)\n",
    "\n",
    "print(\"\\n--- 새로운 문장 예측 테스트 ---\")\n",
    "print(f\"입력 문장 1: \\\"{test_sentence_1}\\\"\")\n",
    "print(f\"모델 예측 : {prediction_1}\\n\")\n",
    "\n",
    "print(f\"입력 문장 2: \\\"{test_sentence_2}\\\"\")\n",
    "print(f\"모델 예측 : {prediction_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d19b608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️ 1단계: 100개의 테스트 데이터셋을 생성합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AI 테스트 텍스트 생성 중:   0%|          | 0/50 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:   2%|▏         | 1/50 [00:00<00:23,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:   4%|▍         | 2/50 [00:00<00:23,  2.04it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:   6%|▌         | 3/50 [00:01<00:23,  1.99it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:   8%|▊         | 4/50 [00:01<00:22,  2.02it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  10%|█         | 5/50 [00:02<00:21,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  12%|█▏        | 6/50 [00:02<00:21,  2.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  14%|█▍        | 7/50 [00:03<00:21,  2.02it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  16%|█▌        | 8/50 [00:03<00:20,  2.08it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  18%|█▊        | 9/50 [00:04<00:19,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  20%|██        | 10/50 [00:04<00:19,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  22%|██▏       | 11/50 [00:05<00:18,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  24%|██▍       | 12/50 [00:05<00:19,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  26%|██▌       | 13/50 [00:06<00:19,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  28%|██▊       | 14/50 [00:07<00:18,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  30%|███       | 15/50 [00:07<00:19,  1.80it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  32%|███▏      | 16/50 [00:08<00:19,  1.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  34%|███▍      | 17/50 [00:08<00:18,  1.80it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  36%|███▌      | 18/50 [00:09<00:17,  1.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  38%|███▊      | 19/50 [00:09<00:16,  1.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  40%|████      | 20/50 [00:10<00:15,  1.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  42%|████▏     | 21/50 [00:10<00:15,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  44%|████▍     | 22/50 [00:11<00:15,  1.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  46%|████▌     | 23/50 [00:11<00:14,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  48%|████▊     | 24/50 [00:12<00:13,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  50%|█████     | 25/50 [00:12<00:12,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  52%|█████▏    | 26/50 [00:13<00:12,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  54%|█████▍    | 27/50 [00:13<00:11,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  56%|█████▌    | 28/50 [00:14<00:11,  1.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  58%|█████▊    | 29/50 [00:15<00:11,  1.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  60%|██████    | 30/50 [00:15<00:10,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  62%|██████▏   | 31/50 [00:16<00:10,  1.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  64%|██████▍   | 32/50 [00:16<00:09,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  66%|██████▌   | 33/50 [00:17<00:09,  1.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  68%|██████▊   | 34/50 [00:17<00:08,  1.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  70%|███████   | 35/50 [00:18<00:08,  1.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  72%|███████▏  | 36/50 [00:19<00:08,  1.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  74%|███████▍  | 37/50 [00:19<00:07,  1.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  76%|███████▌  | 38/50 [00:20<00:06,  1.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  78%|███████▊  | 39/50 [00:20<00:06,  1.77it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  80%|████████  | 40/50 [00:21<00:05,  1.73it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  82%|████████▏ | 41/50 [00:21<00:05,  1.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  84%|████████▍ | 42/50 [00:22<00:04,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  86%|████████▌ | 43/50 [00:22<00:03,  1.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  88%|████████▊ | 44/50 [00:23<00:03,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  90%|█████████ | 45/50 [00:23<00:02,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  92%|█████████▏| 46/50 [00:24<00:02,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  94%|█████████▍| 47/50 [00:24<00:01,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  96%|█████████▌| 48/50 [00:25<00:01,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중:  98%|█████████▊| 49/50 [00:25<00:00,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "AI 테스트 텍스트 생성 중: 100%|██████████| 50/50 [00:26<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 총 100개의 테스트 데이터셋 생성을 완료했습니다.\n",
      "\n",
      " 2단계: 훈련된 모델로 전체 데이터에 대한 추론을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "추론 진행 중: 100%|██████████| 100/100 [00:05<00:00, 18.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추론을 완료했습니다.\n",
      "\n",
      "3단계: 최종 성능 평가 결과를 보여드립니다...\n",
      "\n",
      "--- 최종 모델 평가 결과 ---\n",
      "Total accuracy: 60.00%\n",
      "\n",
      "[상세 분류 리포트]\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Human (class 0)       0.56      1.00      0.71        50\n",
      "   AI (class 1)       1.00      0.20      0.33        50\n",
      "\n",
      "       accuracy                           0.60       100\n",
      "      macro avg       0.78      0.60      0.52       100\n",
      "   weighted avg       0.78      0.60      0.52       100\n",
      "\n",
      "\n",
      "[예측 결과 샘플 (상위 10개)]\n",
      "                                                text  label  predicted_label\n",
      "0  I have a dentist appointment scheduled for Tue...      0                0\n",
      "1  I'm trying to eat healthier by including more ...      0                0\n",
      "2  Yesterday, I spent the entire afternoon readin...      1                0\n",
      "3  I'm not feeling well, so I might take a sick d...      0                0\n",
      "4  I'm excited to see my family during the holida...      1                0\n",
      "5  She volunteers at the local animal shelter eve...      0                0\n",
      "6  The history of this city is fascinating and fu...      0                0\n",
      "7  It's important to get enough sleep for good he...      0                0\n",
      "8  He explained the complex topic in a very simpl...      1                0\n",
      "9  The final episode of the series was a bit disa...      0                0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. 필요한 라이브러리 추가\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 1단계: 더 큰 테스트 데이터셋 생성 (총 100개)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\" 1단계: 100개의 테스트 데이터셋을 생성합니다...\")\n",
    "\n",
    "# 50개의 새로운 '사람' 문장 샘플 (필요에 따라 더 추가하거나 변경 가능)\n",
    "test_human_texts = [\n",
    "    \"I'm planning a vacation to Italy next month.\",\n",
    "    \"This new coffee shop has the best espresso I've ever tasted.\",\n",
    "    \"Yesterday, I spent the entire afternoon reading a book in the park.\",\n",
    "    \"My favorite movie of all time is The Shawshank Redemption.\",\n",
    "    \"Learning a new language can be challenging, but it's also very rewarding.\",\n",
    "    \"I need to go grocery shopping after work today.\",\n",
    "    \"The final episode of the series was a bit disappointing.\",\n",
    "    \"My computer is running so slow, I think it's time for an upgrade.\",\n",
    "    \"She has been practicing the piano every day for the past five years.\",\n",
    "    \"We went hiking in the mountains and the view from the top was breathtaking.\",\n",
    "    \"I'm not feeling well, so I might take a sick day tomorrow.\",\n",
    "    \"The traffic was terrible this morning on my way to the office.\",\n",
    "    \"He explained the complex topic in a very simple and understandable way.\",\n",
    "    \"I finally finished the puzzle that I've been working on for a week.\",\n",
    "    \"It's important to get enough sleep for good health.\",\n",
    "    \"The museum has a new exhibition on ancient Egyptian artifacts.\",\n",
    "    \"She sent me a postcard from her trip to Paris.\",\n",
    "    \"I'm trying to eat healthier by including more vegetables in my diet.\",\n",
    "    \"He is a talented artist who paints beautiful landscapes.\",\n",
    "    \"I accidentally deleted an important file from my computer.\",\n",
    "    \"The weather forecast predicts rain for this weekend.\",\n",
    "    \"She volunteers at the local animal shelter every Saturday.\",\n",
    "    \"I'm looking forward to the concert next Friday.\",\n",
    "    \"He always makes me laugh with his great sense of humor.\",\n",
    "    \"I have a dentist appointment scheduled for Tuesday afternoon.\",\n",
    "    \"This recipe requires a few ingredients that I don't have.\",\n",
    "    \"The library is a quiet place to study and concentrate.\",\n",
    "    \"I need to remember to charge my phone before I go to bed.\",\n",
    "    \"They are renovating their house, so it's a bit messy right now.\",\n",
    "    \"I enjoy listening to classical music while I work.\",\n",
    "    \"The history of this city is fascinating and full of interesting stories.\",\n",
    "    \"I'm taking a course to improve my public speaking skills.\",\n",
    "    \"She is known for her generosity and kindness to others.\",\n",
    "    \"I misplaced my keys and I can't find them anywhere.\",\n",
    "    \"The park is a great place for a picnic on a sunny day.\",\n",
    "    \"He is allergic to peanuts, so we need to be careful with the food.\",\n",
    "    \"I'm trying to reduce my screen time and read more books.\",\n",
    "    \"The new restaurant downtown has received excellent reviews.\",\n",
    "    \"She has a remarkable talent for learning new languages quickly.\",\n",
    "    \"I need to do my laundry this evening.\",\n",
    "    \"The view of the sunset from the beach was absolutely stunning.\",\n",
    "    \"He is training for a marathon, so he runs every morning.\",\n",
    "    \"I'm saving money to buy a new car next year.\",\n",
    "    \"The customer service representative was very helpful and polite.\",\n",
    "    \"I love the smell of freshly baked bread.\",\n",
    "    \"She adopted a cute puppy from the animal shelter.\",\n",
    "    \"I have to prepare a presentation for the meeting tomorrow.\",\n",
    "    \"The documentary about wildlife was both educational and entertaining.\",\n",
    "    \"He fixed the broken chair with some glue and a few screws.\",\n",
    "    \"I'm excited to see my family during the holidays.\"\n",
    "]\n",
    "\n",
    "\n",
    "# AI 텍스트 생성을 위한 오리지널 GPT-2 모델 파이프라인 다시 로드\n",
    "# (우리가 훈련시킨 모델이 아닌, 초기 데이터 생성용 모델)\n",
    "text_generator = pipeline(\"text-generation\", model=\"openai-community/gpt2\", device=device)\n",
    "\n",
    "# 50개의 'AI' 문장 생성\n",
    "test_ai_texts = []\n",
    "for text in tqdm(test_human_texts, desc=\"AI 테스트 텍스트 생성 중\"):\n",
    "    prompt = \" \".join(text.split()[:15]) # 처음 15단어를 프롬프트로 사용\n",
    "    generated_outputs = text_generator(prompt, max_length=len(text.split()) + 20, num_return_sequences=1)\n",
    "    generated_text = generated_outputs[0]['generated_text']\n",
    "    test_ai_texts.append(generated_text)\n",
    "\n",
    "# 데이터프레임으로 결합\n",
    "human_df_test = pd.DataFrame({'text': test_human_texts, 'label': 0})\n",
    "ai_df_test = pd.DataFrame({'text': test_ai_texts, 'label': 1})\n",
    "test_df = pd.concat([human_df_test, ai_df_test], ignore_index=True)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True) # 데이터 섞기\n",
    "\n",
    "print(f\" 총 {len(test_df)}개의 테스트 데이터셋 생성을 완료했습니다.\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2단계: 훈련된 모델로 전체 테스트 데이터셋 추론\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n 2단계: 훈련된 모델로 전체 데이터에 대한 추론을 시작합니다...\")\n",
    "\n",
    "predictions = []\n",
    "# test_df의 각 텍스트에 대해 예측 수행\n",
    "for text in tqdm(test_df['text'], desc=\"추론 진행 중\"):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True, padding=True)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    pred_label_id = torch.argmax(outputs.logits, dim=1).item()\n",
    "    predictions.append(pred_label_id)\n",
    "\n",
    "# 예측 결과를 데이터프레임에 추가\n",
    "test_df['predicted_label'] = predictions\n",
    "print(\"추론을 완료했습니다.\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3단계: 정확도 계산 및 결과 확인\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n3단계: 최종 성능 평가 결과를 보여드립니다...\")\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(test_df['label'], test_df['predicted_label'])\n",
    "\n",
    "# 상세 리포트 생성\n",
    "report = classification_report(test_df['label'], test_df['predicted_label'], target_names=['Human (class 0)', 'AI (class 1)'])\n",
    "\n",
    "print(\"\\n--- 최종 모델 평가 결과 ---\")\n",
    "print(f\"Total accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\n[상세 분류 리포트]\")\n",
    "print(report)\n",
    "\n",
    "print(\"\\n[예측 결과 샘플 (상위 10개)]\")\n",
    "# 실제 정답(label)과 모델의 예측(predicted_label)을 비교해볼 수 있도록 출력\n",
    "print(test_df.head(10))\n",
    "\n",
    "# 참고: 현재 모델은 단 6개의 데이터로만 훈련되었기 때문에,정확도가 낮음\n",
    "# 이 코드는 '평가 파이프라인'이 잘 작동하는지를 확인하는 데 의의가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745a23f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
