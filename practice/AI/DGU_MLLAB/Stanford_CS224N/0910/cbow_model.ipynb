{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe07a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 Corpus : ['I love natural language processing', 'Word2vec is a fascinating model', 'Understanding embeddings is key']\n"
     ]
    }
   ],
   "source": [
    "# Corpus : 학습시킬 데이터\n",
    "corpus = [\"I love natural language processing\", \"Word2vec is a fascinating model\", \"Understanding embeddings is key\"]\n",
    "\n",
    "print(f\"원본 Corpus : {corpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f80236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화된 corpus: [['i', 'love', 'natural', 'language', 'processing'], ['word2vec', 'is', 'a', 'fascinating', 'model'], ['understanding', 'embeddings', 'is', 'key']]\n"
     ]
    }
   ],
   "source": [
    "#토큰화\n",
    "tokenized_corpus = []\n",
    "for sentence in corpus:\n",
    "    tokens = sentence.lower().split()\n",
    "    tokenized_corpus.append(tokens)\n",
    "\n",
    "print(f\"토큰화된 corpus: {tokenized_corpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1701c531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (고유 단어 집합) : {'key', 'love', 'language', 'i', 'natural', 'is', 'a', 'understanding', 'processing', 'embeddings', 'word2vec', 'model', 'fascinating'}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary set 만들기\n",
    "vocabulary = set() # set은 중복 단어 허용 x인거 이용\n",
    "for tokens in tokenized_corpus:\n",
    "    vocabulary.update(tokens)\n",
    "\n",
    "print(f\"Vocabulary (고유 단어 집합) : {vocabulary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ee15c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary(정렬) : ['a', 'embeddings', 'fascinating', 'i', 'is', 'key', 'language', 'love', 'model', 'natural', 'processing', 'understanding', 'word2vec']\n",
      "\n",
      "단어 -> ID 딕셔너리 : {'a': 0, 'embeddings': 1, 'fascinating': 2, 'i': 3, 'is': 4, 'key': 5, 'language': 6, 'love': 7, 'model': 8, 'natural': 9, 'processing': 10, 'understanding': 11, 'word2vec': 12}\n",
      "\n",
      "ID->단어 딕셔너리: {0: 'a', 1: 'embeddings', 2: 'fascinating', 3: 'i', 4: 'is', 5: 'key', 6: 'language', 7: 'love', 8: 'model', 9: 'natural', 10: 'processing', 11: 'understanding', 12: 'word2vec'}\n"
     ]
    }
   ],
   "source": [
    "# integer id부여하기 : 두 종류의 딕셔너리 만들기\n",
    "sorted_vocab = sorted(list(vocabulary))\n",
    "\n",
    "word_to_id = {word: i for i, word in enumerate(sorted_vocab)}\n",
    "id_to_word = {i: word for i,word in enumerate(sorted_vocab)}\n",
    "\n",
    "print(f\"Vocabulary(정렬) : {sorted_vocab}\")\n",
    "print(f\"\\n단어 -> ID 딕셔너리 : {word_to_id}\")\n",
    "print(f\"\\nID->단어 딕셔너리: {id_to_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "571346a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW 학습 데이터 쌍 (첫 5개만):\n",
      "Context: ['love', 'natural'] -> center: i\n",
      "Context: ['i', 'natural', 'language'] -> center: love\n",
      "Context: ['i', 'love', 'language', 'processing'] -> center: natural\n",
      "Context: ['love', 'natural', 'processing'] -> center: language\n",
      "Context: ['natural', 'language'] -> center: processing\n"
     ]
    }
   ],
   "source": [
    "# CBow모델의 핵심은 context로 center예측\n",
    "# 즉, (context, center) pair로 만들어줘야\n",
    "# window 크기는 2로 설정\n",
    "\n",
    "window_size = 2\n",
    "cbow_pairs = []\n",
    "\n",
    "for tokens in tokenized_corpus:\n",
    "    for i, center_word in enumerate(tokens):\n",
    "        context_words = []\n",
    "        for j in range(i-window_size, i+window_size+1):\n",
    "            if i==j or j<0 or j>= len(tokens):\n",
    "                continue\n",
    "            context_words.append(tokens[j])\n",
    "        center_word_id = word_to_id[center_word]\n",
    "        context_word_ids = [word_to_id[word] for word in context_words]\n",
    "\n",
    "        cbow_pairs.append((context_word_ids,center_word_id))\n",
    "print(\"CBOW 학습 데이터 쌍 (첫 5개만):\")\n",
    "for pair in cbow_pairs[:5]:\n",
    "    context_words = [id_to_word[id_] for id_ in pair[0]]\n",
    "    center_word = id_to_word[pair[1]]\n",
    "    print(f\"Context: {context_words} -> center: {center_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82dd6c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.4.1-cp38-cp38-win_amd64.whl (199.4 MB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\seungwoo\\anaconda3\\lib\\site-packages (from torch) (2021.7.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\seungwoo\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Collecting typing-extensions>=4.8.0\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\seungwoo\\anaconda3\\lib\\site-packages (from torch) (2.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\seungwoo\\anaconda3\\lib\\site-packages (from torch) (3.0.12)\n",
      "Requirement already satisfied: sympy in c:\\users\\seungwoo\\anaconda3\\lib\\site-packages (from torch) (1.8)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\seungwoo\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\seungwoo\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: typing-extensions, torch\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.0\n",
      "    Uninstalling typing-extensions-3.10.0.0:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.0\n",
      "Successfully installed torch-2.4.1 typing-extensions-4.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae0e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW(\n",
      "  (embeddings): Embedding(13, 10)\n",
      "  (linear): Linear(in_features=10, out_features=13, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#cbow using pytorch\n",
    "#주변단어들의 임베딩 벡터들을 평균내서 -> score 계산 -> 중심단어의 점수와 가까워지도록\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 6. CBOW 모델 정의\n",
    "# 파이토치의 nn.Module을 상속받아 모델 클래스 만들기\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        # super().__init__()는 nn.Module의 초기화 함수를 먼저 실행하라는 거\n",
    "        super(CBOW, self).__init__()\n",
    "        \n",
    "        # 1. 임베딩 레이어 (Embedding Layer)\n",
    "        # vocab_size: 전체 단어 개수\n",
    "        # embedding_dim: 각 단어를 표현할 벡터의 차원(크기)\n",
    "        # 이 레이어는 단어 ID를 받아서 해당 단어의 임베딩 벡터를 찾아줌\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # 2. 선형 레이어 (Linear Layer)\n",
    "        # 평균낸 임베딩 벡터를 입력으로 받아, 전체 단어에 대한 점수(logits)를 출력\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    # 모델이 데이터를 입력받았을 때 순서대로 실행되는 부분\n",
    "    def forward(self, context_word_ids):\n",
    "        # context_word_ids로 임베딩 벡터들을 조회함\n",
    "        context_embs = self.embeddings(context_word_ids)\n",
    "        \n",
    "        # 벡터들의 평균을 계산, dim=0은 텐서의 첫번째 차원을 기준으로 평균을 내라는 거\n",
    "        mean_embs = torch.mean(context_embs, dim=0)\n",
    "        \n",
    "        # 평균낸 벡터를 선형 레이어에 통과시켜 점수를 계산\n",
    "        logits = self.linear(mean_embs)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# 모델 학습에 필요한 파라미터(Parameter) 설정\n",
    "VOCAB_SIZE = len(vocabulary)\n",
    "EMBEDDING_DIM = 10 # 각 단어를 10차원 벡터로 표현\n",
    "\n",
    "# 모델 객체 생성\n",
    "model = CBOW(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa4b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 시작!\n",
      "Epoch 10/100 | Total Loss: 16.6847\n",
      "Epoch 20/100 | Total Loss: 7.3471\n",
      "Epoch 30/100 | Total Loss: 3.9937\n",
      "Epoch 40/100 | Total Loss: 2.7365\n",
      "Epoch 50/100 | Total Loss: 2.2091\n",
      "Epoch 60/100 | Total Loss: 1.9520\n",
      "Epoch 70/100 | Total Loss: 1.8096\n",
      "Epoch 80/100 | Total Loss: 1.7228\n",
      "Epoch 90/100 | Total Loss: 1.6658\n",
      "Epoch 100/100 | Total Loss: 1.6263\n",
      "학습 완료!\n"
     ]
    }
   ],
   "source": [
    "# 7. 모델 학습\n",
    "import torch.optim as optim\n",
    "\n",
    "# 손실 함수와 옵티마이저(최적화 도구)를 정의함.\n",
    "# CrossEntropyLoss는 분류 문제에서 주로 사용하는 손실 함수임\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# Adam은 파라미터를 효율적으로 업데이트해주는 최적화 알고리즘 중 하나\n",
    "# model.parameters()는 학습시킬 모든 파라미터(임베딩, 선형층의 가중치 등)를 의미합니다.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"학습 시작!\")\n",
    "# 학습 사이클을 100번 반복 (100 Epochs).\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # 준비해둔 학습 데이터 쌍을 하나씩 가져와서 학습\n",
    "    for context_ids, center_id in cbow_pairs:\n",
    "        # Pytorch는 데이터를 Tensor라는 자료구조로 다룬다\n",
    "        # context_ids를 Tensor로 변환\n",
    "        context_t = torch.tensor(context_ids, dtype=torch.long)\n",
    "        \n",
    "        # 모델은 항상 미니배치(mini-batch) 단위로 입력을 받도록 설계되어 있으므로\n",
    "        # view(1, -1)를 통해 [1, vocab_size] 형태의 텐서로 만들어준다\n",
    "        target_t = torch.tensor([center_id], dtype=torch.long)\n",
    "        \n",
    "        # 이전 학습에서 계산된 기울기(gradient)를 모두 0으로 초기화\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # 1. 예측 (Forward Pass)\n",
    "        # 모델에 주변 단어 ID 텐서를 입력하여 예측 점수(logits)를 받는다\n",
    "        logits = model(context_t)\n",
    "\n",
    "        # 2. 손실 계산 (Calculate Loss)\n",
    "        # 예측 점수와 실제 정답 ID를 비교하여 손실을 계산함\n",
    "        loss = loss_function(logits.view(1, -1), target_t)\n",
    "        \n",
    "        # 3. 업데이트 (Backward Pass & Optimization)\n",
    "        # 손실을 기반으로 각 파라미터가 얼마나 변해야 하는지(기울기) 계산\n",
    "        loss.backward()\n",
    "        # 옵티마이저가 계산된 기울기를 바탕으로 모델의 파라미터를 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # 10번의 epoch마다 중간 손실 값을 출력\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/100 | Total Loss: {total_loss:.4f}\")\n",
    "\n",
    "print(\"학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc4b0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'language'와(과) 가장 유사한 단어 Top 5:\n",
      "model: 0.2510\n",
      "processing: 0.2300\n",
      "natural: 0.1447\n",
      "i: 0.0701\n",
      "love: 0.0039\n"
     ]
    }
   ],
   "source": [
    "# 8. 학습 결과 확인: 특정 단어와 가장 유사한 단어 찾기\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# 모델의 임베딩 파라미터를 가져오기\n",
    "# .detach().numpy()는 학습 그래프에서 분리하여 numpy 배열로 변환하는 코드\n",
    "embeddings = model.embeddings.weight.detach().numpy()\n",
    "\n",
    "# 기준 단어 설정\n",
    "target_word = 'language'\n",
    "\n",
    "# 기준 단어의 ID와 임베딩 벡터를 가져온다\n",
    "target_id = word_to_id[target_word]\n",
    "target_vec = embeddings[target_id]\n",
    "\n",
    "# 모든 단어와의 코사인 유사도를 저장할 딕셔너리\n",
    "similarities = {}\n",
    "\n",
    "# 모든 단어를 순회하며 기준 단어와의 유사도를 계산한다.\n",
    "for word, i in word_to_id.items():\n",
    "    if word == target_word:\n",
    "        continue\n",
    "    \n",
    "    # 코사인 거리를 계산 (1 - 코사인 유사도)\n",
    "    dist = cosine(target_vec, embeddings[i])\n",
    "    # 코사인 유사도는 1 - 거리\n",
    "    similarity = 1 - dist\n",
    "    similarities[word] = similarity\n",
    "\n",
    "# 유사도가 높은 순서대로 단어를 정렬.\n",
    "# key=lambda item: item[1]는 딕셔너리의 값(value)을 기준으로 정렬하라는 의미\n",
    "# reverse=True는 내림차순(큰 것부터)으로 정렬\n",
    "sorted_similar_words = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "print(f\"'{target_word}'와(과) 가장 유사한 단어 Top 5:\")\n",
    "for word, sim in sorted_similar_words[:5]:\n",
    "    print(f\"{word}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a829a41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
