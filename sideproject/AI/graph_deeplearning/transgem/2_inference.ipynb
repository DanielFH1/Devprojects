{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83391b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ÏûëÏóÖ Í≤ΩÎ°ú: /home/lsw0927/data/interns/lsw0927/transgem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lsw0927/miniconda3/envs/transgem/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Ïù¥ ÏÖÄÏùÑ Í∞ÄÏû• Î®ºÏ†Ä Ïã§ÌñâÌï¥Ïïº 'pd' ÏóêÎü¨Í∞Ä ÏÇ¨ÎùºÏßëÎãàÎã§!\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd          # <--- ÏñòÍ∞Ä ÏûàÏñ¥Ïïº 'pd'Î•º Ïïå Ïàò ÏûàÏñ¥Ïöî\n",
    "import selfies as sf\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# ÏûëÏóÖ Í≤ΩÎ°ú Í≥†Ï†ï (ÏïÑÍπå ÎìúÎ¶∞ ÏΩîÎìú)\n",
    "try:\n",
    "    PROJECT_DIR = os.path.expanduser(\"~/data/interns/lsw0927/transgem\")\n",
    "    os.chdir(PROJECT_DIR)\n",
    "    print(f\"‚úÖ ÏûëÏóÖ Í≤ΩÎ°ú: {os.getcwd()}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b78e5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device settings: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- 2. ÏÑ§Ï†ï Î∞è ÌÅ¥ÎûòÏä§ Ï†ïÏùò (Ïù¥ ÏÖÄÏùÑ Íº≠ Ïã§ÌñâÌï¥Ïïº Ìï©ÎãàÎã§!) ---\n",
    "\n",
    "import math\n",
    "\n",
    "# ÏÑ§Ï†ï Ï†ïÏùò\n",
    "CONFIG = {\n",
    "    \"data_path\": \"./TransGEM/data/subLINCS.csv\",\n",
    "    # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌååÏùºÎ™ÖÏù¥ ÎßûÎäîÏßÄ Îã§Ïãú ÌôïÏù∏!\n",
    "    \"model_path\": \"checkpoints/model_epoch_200.pt\", \n",
    "    \"output_file\": \"generated_molecules_PC3.txt\",\n",
    "    \"d_model\": 64,\n",
    "    \"nhead\": 8,\n",
    "    \"num_layers\": 6,\n",
    "    \"ff_dim\": 512,\n",
    "    \"max_len\": 100,\n",
    "    \"gene_dim\": 978 * 10\n",
    "}\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device settings: {DEVICE}\")\n",
    "\n",
    "# --- Î™®Îç∏ Î∞è Ìó¨Ìçº Ìï®Ïàò Ï†ïÏùò ---\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransGEM(nn.Module):\n",
    "    def __init__(self, num_genes_dim, num_cells, vocab_size, d_model=64, nhead=8, num_layers=6, dim_feedforward=512):\n",
    "        super(TransGEM, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.cell_embedding = nn.Embedding(num_cells, d_model)\n",
    "        self.gene_linear = nn.Sequential(\n",
    "            nn.Linear(num_genes_dim, d_model), nn.ReLU(), nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, d_model), nn.ReLU(), nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        self.mol_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.generator = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, cell_idx, gene_tensor, tgt):\n",
    "        cell_embed = self.cell_embedding(cell_idx)\n",
    "        gene_embed = self.gene_linear(gene_tensor)\n",
    "        context = torch.cat([cell_embed, gene_embed], dim=-1)\n",
    "        memory = self.fusion_layer(context).unsqueeze(1)\n",
    "        tgt_embed = self.mol_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_embed = self.pos_encoder(tgt_embed)\n",
    "        sz = tgt.size(1)\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(tgt.device)\n",
    "        output = self.transformer_decoder(tgt=tgt_embed, memory=memory, tgt_mask=mask)\n",
    "        return self.generator(output)\n",
    "\n",
    "def build_vocab(smiles_list):\n",
    "    vocab = set()\n",
    "    print(\"Building Vocabulary...\")\n",
    "    for smiles in tqdm(smiles_list, desc=\"Tokenizing\"):\n",
    "        try:\n",
    "            selfie = sf.encoder(smiles)\n",
    "            if selfie is None: continue\n",
    "            tokens = list(sf.split_selfies(selfie))\n",
    "            vocab.update(tokens)\n",
    "        except: continue\n",
    "    vocab = sorted(list(vocab))\n",
    "    token2idx = {'<pad>': 0, '<sos>': 1, '<eos>': 2}\n",
    "    for i, token in enumerate(vocab): token2idx[token] = i + 3\n",
    "    return token2idx\n",
    "\n",
    "def parse_gene_string(gene_str):\n",
    "    if isinstance(gene_str, str):\n",
    "        try: return [float(x) for x in gene_str.split('//') if x.strip()]\n",
    "        except: return []\n",
    "    return []\n",
    "\n",
    "def tenfold_binary_embedding(value):\n",
    "    try: value = float(value)\n",
    "    except: value = 0.0\n",
    "    sign_bit = 1 if value > 0 else 0\n",
    "    tenfold_value = int(abs(value) * 10)\n",
    "    binary_str = bin(tenfold_value)[2:].zfill(9)\n",
    "    if len(binary_str) > 9: binary_str = binary_str[-9:] \n",
    "    return [sign_bit] + [int(b) for b in binary_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aaaede1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Data Loading...\n",
      "Building Vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27378/27378 [00:09<00:00, 2765.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targeting Disease: Prostate Cancer (PC3)\n",
      "2. Model Loading...\n",
      "‚úÖ Checkpoint loaded: checkpoints/model_epoch_200.pt\n",
      "3. Generating Molecules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating New Drugs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [03:47<00:00,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Done! 1000 molecules saved to generated_molecules_PC3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Main Generation Logic ---\n",
    "\n",
    "print(\"1. Data Loading...\")\n",
    "df = pd.read_csv(CONFIG['data_path'])\n",
    "token2idx = build_vocab(df['smiles'].tolist())\n",
    "idx2token = {v: k for k, v in token2idx.items()}\n",
    "\n",
    "# Cell Line List ÌôïÏù∏\n",
    "cell_lines = sorted(df['cell_line'].unique().tolist())\n",
    "cell2idx = {name: i for i, name in enumerate(cell_lines)}\n",
    "\n",
    "# PC3 (Ï†ÑÎ¶ΩÏÑ†Ïïî) ÌÉÄÍ≤ü Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n",
    "print(\"Targeting Disease: Prostate Cancer (PC3)\")\n",
    "target_row = df[df['cell_line'] == 'PC3'].iloc[0]\n",
    "\n",
    "# Gene Embedding\n",
    "raw_gene = target_row['gene_e']\n",
    "gene_values = parse_gene_string(raw_gene)\n",
    "gene_embeds = []\n",
    "for val in gene_values:\n",
    "    gene_embeds.extend(tenfold_binary_embedding(val))\n",
    "\n",
    "# Tensor Î≥ÄÌôò\n",
    "expected_len = 978 * 10\n",
    "if len(gene_embeds) != expected_len:\n",
    "    gene_embeds = [0] * expected_len\n",
    "\n",
    "gene_tensor = torch.tensor(gene_embeds, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "cell_idx = torch.tensor([cell2idx['PC3']], dtype=torch.long).to(DEVICE)\n",
    "\n",
    "print(\"2. Model Loading...\")\n",
    "model = TransGEM(\n",
    "    num_genes_dim=CONFIG['gene_dim'],\n",
    "    num_cells=len(cell_lines),\n",
    "    vocab_size=len(token2idx),\n",
    "    d_model=CONFIG['d_model'], nhead=CONFIG['nhead'], num_layers=CONFIG['num_layers'], dim_feedforward=CONFIG['ff_dim']\n",
    ").to(DEVICE)\n",
    "\n",
    "if os.path.exists(CONFIG['model_path']):\n",
    "    state_dict = torch.load(CONFIG['model_path'])\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(f\"‚úÖ Checkpoint loaded: {CONFIG['model_path']}\")\n",
    "else:\n",
    "    print(f\"‚ùå Checkpoint NOT found at {CONFIG['model_path']}\")\n",
    "    raise FileNotFoundError(\"Î™®Îç∏ ÌååÏùº Í≤ΩÎ°úÎ•º Îã§Ïãú ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî!\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"3. Generating Molecules...\")\n",
    "generated_smiles = []\n",
    "\n",
    "# ‚ú® Ïó¨Í∏∞ÏÑú ÏßÑÌñâÎ∞î(Progress Bar)Î•º Íµ¨Í≤ΩÌïòÏãúÎ©¥ Îê©ÎãàÎã§! ‚ú®\n",
    "for _ in tqdm(range(1000), desc=\"Creating New Drugs\"):\n",
    "    curr_seq = torch.tensor([[token2idx['<sos>']]], dtype=torch.long).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(100): # Max Length\n",
    "            output = model(cell_idx, gene_tensor, curr_seq)\n",
    "            next_token_logits = output[:, -1, :]\n",
    "            \n",
    "            # Sampling\n",
    "            probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            \n",
    "            curr_seq = torch.cat([curr_seq, next_token], dim=1)\n",
    "            if next_token.item() == token2idx['<eos>']:\n",
    "                break\n",
    "    \n",
    "    # Decoding\n",
    "    indices = curr_seq[0].cpu().numpy()\n",
    "    tokens = []\n",
    "    for idx in indices:\n",
    "        if idx == token2idx['<eos>']: break\n",
    "        if idx not in [token2idx['<sos>'], token2idx['<pad>']]:\n",
    "            tokens.append(idx2token[idx])\n",
    "    \n",
    "    selfies_str = \"\".join(tokens)\n",
    "    try:\n",
    "        smiles = sf.decoder(selfies_str)\n",
    "        if smiles: generated_smiles.append(smiles)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Ï†ÄÏû•\n",
    "with open(CONFIG['output_file'], \"w\") as f:\n",
    "    for s in generated_smiles:\n",
    "        f.write(s + \"\\n\")\n",
    "        \n",
    "print(f\"üéâ Done! {len(generated_smiles)} molecules saved to {CONFIG['output_file']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0059af8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transgem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
