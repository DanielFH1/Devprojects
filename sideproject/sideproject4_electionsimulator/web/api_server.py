from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
import json
from pathlib import Path
from fastapi.responses import FileResponse, JSONResponse
import threading
import schedule
import time
from datetime import datetime, timedelta
import sys
import os
import logging
from typing import Dict, Any, Optional
from collections import defaultdict
import shutil  # íŒŒì¼ ë³µì‚¬ë¥¼ ìœ„í•œ ëª¨ë“ˆ ì¶”ê°€

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
BASE_DIR = Path(__file__).resolve().parent.parent
sys.path.append(str(BASE_DIR))

# OpenAI API í‚¤ ê²€ì‚¬ ì¶”ê°€
openai_api_key = os.getenv('OPENAI_API_KEY')
if not openai_api_key:
    logger.error("âŒ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‰´ìŠ¤ ë¶„ì„ ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
else:
    logger.info("âœ… OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

# !scrapper.pyì˜ í•¨ìˆ˜ë“¤ì„ ì„í¬íŠ¸
from news_scraper import run_news_pipeline, NewsPipeline, rank_news_by_importance, pipeline

app = FastAPI()

# --- ê²½ë¡œ ì„¤ì • ---
BASE_DIR = Path(__file__).resolve().parent.parent
FLUTTER_BUILD_DIR = BASE_DIR / "flutter_ui" / "build" / "web"
ASSETS_DIR = BASE_DIR / "assets"
# ì •ì  íŒŒì¼ ë””ë ‰í† ë¦¬ ì„¤ì • - Flutter ë¹Œë“œ íŒŒì¼ìš©
STATIC_DIR = Path(__file__).resolve().parent / "static"

# Render.com ì˜êµ¬ ì €ì¥ì†Œ ê²½ë¡œ ì„¤ì •
PERSISTENT_DIR = None
if os.environ.get('RENDER') == 'true':
    PERSISTENT_DIR = Path("/opt/render/project/src/persistent_data")
    PERSISTENT_DIR.mkdir(parents=True, exist_ok=True)
    logger.info(f"ğŸ“‚ Render.com ì˜êµ¬ ì €ì¥ì†Œ ê²½ë¡œ ì„¤ì •: {PERSISTENT_DIR}")
    
    # assets í´ë”ê°€ ë¹„ì–´ìˆê³  ì˜êµ¬ ì €ì¥ì†Œì— ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë³µì‚¬
    if PERSISTENT_DIR.exists():
        assets_files = list(ASSETS_DIR.glob("trend_summary_*.json"))
        if not assets_files:
            for json_file in PERSISTENT_DIR.glob("trend_summary_*.json"):
                dest_file = ASSETS_DIR / json_file.name
                if not dest_file.exists():
                    shutil.copy(json_file, dest_file)
                    logger.info(f"ğŸ“‹ ì˜êµ¬ ì €ì¥ì†Œì—ì„œ ë³µì›ëœ íŒŒì¼: {json_file.name}")

# assets ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
try:
    ASSETS_DIR.mkdir(parents=True, exist_ok=True)
    logger.info(f"âœ… assets ë””ë ‰í† ë¦¬ í™•ì¸/ìƒì„± ì™„ë£Œ: {ASSETS_DIR}")
except Exception as e:
    logger.error(f"âŒ assets ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {str(e)}")

# static ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
try:
    STATIC_DIR.mkdir(parents=True, exist_ok=True)
    logger.info(f"âœ… static ë””ë ‰í† ë¦¬ í™•ì¸/ìƒì„± ì™„ë£Œ: {STATIC_DIR}")
except Exception as e:
    logger.error(f"âŒ static ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {str(e)}")

# ê¸°ë³¸ ë°ì´í„° íŒŒì¼ ìƒì„±
DEFAULT_DATA_FILE = ASSETS_DIR / "trend_summary_default.json"
if not DEFAULT_DATA_FILE.exists():
    try:
        default_data = {
            "trend_summary": "ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤...",
            "candidate_stats": {
                "ì´ì¬ëª…": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0},
                "ê¹€ë¬¸ìˆ˜": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0},
                "ì´ì¤€ì„": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0}
            },
            "total_articles": 0,
            "time_range": "ë°ì´í„° ìˆ˜ì§‘ ì¤‘",
            "news_list": []  # ë¹ˆ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
        }
        with open(DEFAULT_DATA_FILE, "w", encoding="utf-8") as f:
            json.dump(default_data, f, ensure_ascii=False, indent=2)
        logger.info("âœ… ê¸°ë³¸ ë°ì´í„° íŒŒì¼ ìƒì„± ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ ê¸°ë³¸ ë°ì´í„° íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {str(e)}")

# --- CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì • ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- ì „ì—­ ë³€ìˆ˜ ë° ìºì‹œ ê´€ë¦¬ ---
class NewsCache:
    def __init__(self):
        self.latest_data: Optional[Dict[str, Any]] = None
        self.last_update: Optional[datetime] = None
        self.update_count: int = 0
        self.error_count: int = 0
        self.last_error: Optional[str] = None
        self.pipeline = NewsPipeline()
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ ìƒíƒœ ì¶”ì 
        self.scheduler_running = False
        self.daily_task_last_run = None
        # ê°•ì œ ì´ˆê¸°í™” ìƒíƒœ ì¶”ì 
        self.initial_fetch_done = False

    def update(self, data: Dict[str, Any]):
        self.latest_data = data
        self.last_update = datetime.now()
        self.update_count += 1
        self.error_count = 0
        self.last_error = None

    def record_error(self, error: str):
        self.error_count += 1
        self.last_error = error
        logger.error(f"ìºì‹œ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {error}")

    def get_status(self) -> Dict[str, Any]:
        return {
            "last_update": self.last_update.isoformat() if self.last_update else None,
            "update_count": self.update_count,
            "error_count": self.error_count,
            "last_error": self.last_error,
            "is_healthy": self.error_count < 3 and self.last_update is not None,
            "scheduler_running": self.scheduler_running,
            "daily_task_last_run": self.daily_task_last_run.isoformat() if self.daily_task_last_run else None,
            "initial_fetch_done": self.initial_fetch_done
        }

news_cache = NewsCache()

def update_news_cache():
    """assets í´ë”ì—ì„œ ìµœì‹  ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì½ì–´ì™€ ìºì‹œë¥¼ ì—…ë°ì´íŠ¸"""
    try:
        # ìµœì‹  ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ ì°¾ê¸° - ê¸°ë³¸ íŒŒì¼ ì œì™¸í•˜ê³  ë‚ ì§œ íŒŒì¼ ìš°ì„ 
        latest_file = None
        
        # ë¨¼ì € assets ë””ë ‰í† ë¦¬ì—ì„œ ë‚ ì§œê°€ í¬í•¨ëœ íŒŒì¼ë“¤ë§Œ ì°¾ê¸° (ê¸°ë³¸ íŒŒì¼ ì œì™¸)
        news_files = [
            f for f in ASSETS_DIR.glob("trend_summary_*.json")
            if f.name != "trend_summary_default.json"  # ê¸°ë³¸ íŒŒì¼ ì œì™¸
        ]
        
        if news_files:
            try:
                # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œì™€ ì‹œê°„ ì¶”ì¶œí•˜ì—¬ ì •ë ¬
                def extract_datetime_from_filename(filepath):
                    """íŒŒì¼ëª…ì—ì„œ ë‚ ì§œì‹œê°„ ì¶”ì¶œ"""
                    try:
                        # trend_summary_2025-05-26_13-03.json í˜•ì‹ì—ì„œ ë‚ ì§œì‹œê°„ ì¶”ì¶œ
                        parts = filepath.stem.split('_')
                        if len(parts) >= 3:
                            date_str = parts[2]  # 2025-05-26
                            time_str = parts[3] if len(parts) > 3 else "00-00"  # 13-03
                            datetime_str = f"{date_str} {time_str.replace('-', ':')}"
                            return datetime.strptime(datetime_str, "%Y-%m-%d %H:%M")
                    except:
                        pass
                    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ íŒŒì¼ ìˆ˜ì • ì‹œê°„ ì‚¬ìš©
                    return datetime.fromtimestamp(filepath.stat().st_mtime)
                
                # ë‚ ì§œì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ê°€ì¥ ìµœì‹  íŒŒì¼ ì„ íƒ
                latest_file = sorted(
                    news_files,
                    key=extract_datetime_from_filename,
                    reverse=True
                )[0]
                logger.info(f"ğŸ“‚ assets í´ë”ì—ì„œ ìµœì‹  ë‚ ì§œ íŒŒì¼ ë°œê²¬: {latest_file}")
                
            except Exception as e:
                # ì •ë ¬ ì‹¤íŒ¨ ì‹œ ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ í´ë°±
                latest_file = sorted(news_files, key=lambda p: p.stat().st_mtime, reverse=True)[0]
                logger.warning(f"âš ï¸ ë‚ ì§œ ê¸°ì¤€ ì •ë ¬ ì‹¤íŒ¨, ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ ì‚¬ìš©: {str(e)}")
                logger.info(f"ğŸ“‚ assets í´ë”ì—ì„œ ìµœì‹  íŒŒì¼ ë°œê²¬ (ìˆ˜ì • ì‹œê°„ ê¸°ì¤€): {latest_file}")
        
        # ë‚ ì§œ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì˜êµ¬ ì €ì¥ì†Œ ê²€ì‚¬
        if not latest_file and PERSISTENT_DIR and PERSISTENT_DIR.exists():
            latest_link = PERSISTENT_DIR / "trend_summary_latest.json"
            if latest_link.exists():
                latest_file = latest_link
                logger.info(f"ğŸ“‚ ì˜êµ¬ ì €ì¥ì†Œì—ì„œ ìµœì‹  íŒŒì¼ ë°œê²¬: {latest_file}")
                
                # ì˜êµ¬ ì €ì¥ì†Œì˜ íŒŒì¼ì„ assetsì— ë³µì‚¬
                dest_file = ASSETS_DIR / f"trend_summary_{datetime.now().strftime('%Y-%m-%d_%H-%M')}.json"
                shutil.copy(latest_file, dest_file)
                logger.info(f"ğŸ“‹ ì˜êµ¬ ì €ì¥ì†Œì˜ íŒŒì¼ì„ assetsì— ë³µì‚¬: {dest_file.name}")
                latest_file = dest_file
        
        # ì–´ë””ì—ë„ ë‚ ì§œ íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ íŒŒì¼ ì‚¬ìš©
        if not latest_file and DEFAULT_DATA_FILE.exists():
            latest_file = DEFAULT_DATA_FILE
            logger.warning("âš ï¸ ë‚ ì§œê°€ í¬í•¨ëœ ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            
        # íŒŒì¼ì„ ì°¾ì•˜ìœ¼ë©´ ì²˜ë¦¬
        if latest_file:
            try:
                with open(latest_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    # ê¸°ë³¸ê°’ ì„¤ì •ìœ¼ë¡œ null ë°©ì§€
                    processed_data = {
                        "trend_summary": data.get("trend_summary", "ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤..."),
                        "candidate_stats": data.get("candidate_stats", {
                            "ì´ì¬ëª…": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0},
                            "ê¹€ë¬¸ìˆ˜": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0},
                            "ì´ì¤€ì„": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0}
                        }),
                        "total_articles": data.get("total_articles", 0),
                        "time_range": data.get("time_range", "ë°ì´í„° ìˆ˜ì§‘ ì¤‘"),
                        "news_list": data.get("news_list", [])  # news_list í•„ë“œ ì¶”ê°€
                    }
                    
                    # ë‰´ìŠ¤ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¤‘ìš”ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                    if "news_list" in processed_data and processed_data["news_list"]:
                        try:
                            sorted_news = rank_news_by_importance(processed_data["news_list"], limit=30)
                            processed_data["news_list"] = sorted_news
                            logger.info(f"âœ… ë‰´ìŠ¤ ë°ì´í„° ì¤‘ìš”ë„ ì •ë ¬ ì™„ë£Œ: {len(sorted_news)}ê°œ")
                        except Exception as e:
                            logger.error(f"ë‰´ìŠ¤ ì •ë ¬ ì˜¤ë¥˜: {str(e)}")
                            # ì •ë ¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°ì´í„° ìœ ì§€
                    
                    news_cache.update(processed_data)
                    logger.info(f"âœ… ë‰´ìŠ¤ ìºì‹œ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {latest_file.name}")
                    
                    # ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì˜¤ë˜ëœ ê²½ìš° (3ì¼ ì´ìƒ) ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘ íŠ¸ë¦¬ê±°
                    if not processed_data["news_list"] or (
                        latest_file != DEFAULT_DATA_FILE and 
                        (datetime.now() - datetime.fromtimestamp(latest_file.stat().st_mtime)).days >= 3
                    ):
                        logger.warning("âš ï¸ ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì˜¤ë˜ë˜ì–´ ìƒˆ ë°ì´í„° ìˆ˜ì§‘ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                        if not news_cache.initial_fetch_done:
                            import threading
                            initial_fetch_thread = threading.Thread(target=run_initial_fetch, daemon=True)
                            initial_fetch_thread.start()
                            logger.info("ğŸ”„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                            news_cache.initial_fetch_done = True
            except json.JSONDecodeError as e:
                logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                news_cache.record_error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            except Exception as e:
                logger.error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
                news_cache.record_error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
        else:
            news_cache.record_error("ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            logger.error("ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # íŒŒì¼ì´ ì—†ì„ ê²½ìš° ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ íŠ¸ë¦¬ê±°
            if not news_cache.initial_fetch_done:
                import threading
                initial_fetch_thread = threading.Thread(target=run_initial_fetch, daemon=True)
                initial_fetch_thread.start()
                logger.info("ğŸ”„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì´ˆê¸° ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                news_cache.initial_fetch_done = True
            
    except Exception as e:
        news_cache.record_error(str(e))
        logger.error(f"ìºì‹œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")

def run_scheduled_news_pipeline():
    """ë§¤ì¼ ì˜¤ì „ 6ì‹œì— ì‹¤í–‰ë˜ëŠ” ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜"""
    try:
        # ì˜¤ëŠ˜ ì´ë¯¸ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸
        now = datetime.now()
        
        if news_cache.daily_task_last_run:
            last_run_date = news_cache.daily_task_last_run.date()
            if last_run_date == now.date():
                logger.info(f"â­ï¸ ì˜¤ëŠ˜({now.date()})ì€ ì´ë¯¸ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                return
        
        logger.info(f"ğŸ”” ì˜ˆì •ëœ ì¼ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {now}")
        run_news_pipeline()  # ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„ ì‹¤í–‰
        news_cache.daily_task_last_run = now
        news_cache.initial_fetch_done = True
        logger.info(f"âœ… ì¼ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {now}")
    except Exception as e:
        logger.error(f"âŒ ì˜ˆì •ëœ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")

def run_scheduler():
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰"""
    news_cache.scheduler_running = True
    logger.info("ğŸ•’ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ìŠ¤ì¼€ì¤„ ìƒíƒœ ë¡œê¹…
    logger.info(f"ğŸ“… ì„¤ì •ëœ ìŠ¤ì¼€ì¤„: {schedule.jobs}")
    
    while True:
        try:
            # í˜„ì¬ ì‹œê°„ê³¼ ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ë¡œê¹…
            now = datetime.now()
            next_run = schedule.next_run()
            if next_run:
                logger.info(f"â° í˜„ì¬ ì‹œê°„: {now.strftime('%Y-%m-%d %H:%M:%S')}, ë‹¤ìŒ ì‹¤í–‰: {next_run.strftime('%Y-%m-%d %H:%M:%S')}")
            
            schedule.run_pending()
            time.sleep(60)  # 1ë¶„ë§ˆë‹¤ í™•ì¸
        except Exception as e:
            logger.error(f"âŒ ìŠ¤ì¼€ì¤„ëŸ¬ ì˜¤ë¥˜: {str(e)}")
            time.sleep(300)  # ì˜¤ë¥˜ ë°œìƒ ì‹œ 5ë¶„ ëŒ€ê¸°

def run_news_pipeline():
    """ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ í˜¸ì¶œë  í•¨ìˆ˜"""
    logger.info("ğŸ”” run_news_pipeline í•¨ìˆ˜ê°€ í˜¸ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.")
    try:
        pipeline.run_daily_collection()
        # ìˆ˜ì§‘ í›„ ìºì‹œ ê°•ì œ ì—…ë°ì´íŠ¸
        update_news_cache()
        logger.info("âœ… ë‰´ìŠ¤ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ ë° ìºì‹œ ì—…ë°ì´íŠ¸")
    except Exception as e:
        logger.error(f"âŒ ë‰´ìŠ¤ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

# ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ ì¶”ê°€
def run_initial_fetch():
    """ì„œë²„ ì‹œì‘ ì‹œ í˜¸ì¶œë˜ëŠ” ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜"""
    try:
        logger.info("ğŸš€ ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸° ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰ - ì§ì ‘ pipeline ê°ì²´ì˜ ë©”ì„œë“œ í˜¸ì¶œ
        pipeline.run_daily_collection()
        
        # ìˆ˜ì§‘ í›„ ìƒì„±ëœ ìµœì‹  íŒŒì¼ì„ ì¦‰ì‹œ ì°¾ê¸°
        current_timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M")
        expected_file = ASSETS_DIR / f"trend_summary_{current_timestamp}.json"
        
        # ì •í™•í•œ íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼ì´ ì—†ë‹¤ë©´ ì˜¤ëŠ˜ ë‚ ì§œì˜ ìµœì‹  íŒŒì¼ ì°¾ê¸°
        today = datetime.now().strftime("%Y-%m-%d")
        today_files = list(ASSETS_DIR.glob(f"trend_summary_{today}_*.json"))
        
        if expected_file.exists():
            # ì˜ˆìƒ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ì´ íŒŒì¼ ì‚¬ìš©
            newest_file = expected_file
            logger.info(f"âœ… ìƒˆë¡œ ìƒì„±ëœ íŒŒì¼ ë°œê²¬: {newest_file}")
        elif today_files:
            # ì˜¤ëŠ˜ ìƒì„±ëœ íŒŒì¼ ì¤‘ ê°€ì¥ ìµœì‹  íŒŒì¼ ì‚¬ìš©
            newest_file = sorted(today_files, key=lambda p: p.stat().st_mtime, reverse=True)[0]
            logger.info(f"âœ… ì˜¤ëŠ˜ ìƒì„±ëœ ìµœì‹  íŒŒì¼ ë°œê²¬: {newest_file}")
        else:
            # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì¼ë°˜ ì—…ë°ì´íŠ¸ ë¡œì§ ì‚¬ìš©
            logger.warning("âš ï¸ ìƒˆë¡œ ìƒì„±ëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ ìºì‹œ ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            update_news_cache()
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            news_cache.initial_fetch_done = True
            news_cache.daily_task_last_run = datetime.now()
            
            logger.info("âœ… ì´ˆê¸° ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
            return
            
        # ì°¾ì€ íŒŒì¼ ì§ì ‘ ë¡œë“œ
        try:
            with open(newest_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                # ê¸°ë³¸ê°’ ì„¤ì •ìœ¼ë¡œ null ë°©ì§€
                processed_data = {
                    "trend_summary": data.get("trend_summary", "ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤..."),
                    "candidate_stats": data.get("candidate_stats", {
                        "ì´ì¬ëª…": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0},
                        "ê¹€ë¬¸ìˆ˜": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0},
                        "ì´ì¤€ì„": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0}
                    }),
                    "total_articles": data.get("total_articles", 0),
                    "time_range": data.get("time_range", "ë°ì´í„° ìˆ˜ì§‘ ì¤‘"),
                    "news_list": data.get("news_list", [])
                }
                
                # ë‰´ìŠ¤ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¤‘ìš”ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                if "news_list" in processed_data and processed_data["news_list"]:
                    try:
                        sorted_news = rank_news_by_importance(processed_data["news_list"], limit=30)
                        processed_data["news_list"] = sorted_news
                        logger.info(f"âœ… ë‰´ìŠ¤ ë°ì´í„° ì¤‘ìš”ë„ ì •ë ¬ ì™„ë£Œ: {len(sorted_news)}ê°œ")
                    except Exception as e:
                        logger.error(f"ë‰´ìŠ¤ ì •ë ¬ ì˜¤ë¥˜: {str(e)}")
                
                # ìºì‹œ ì§ì ‘ ì—…ë°ì´íŠ¸
                news_cache.update(processed_data)
                logger.info(f"âœ… ë‰´ìŠ¤ ìºì‹œ ì§ì ‘ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {newest_file.name}")
        except Exception as e:
            logger.error(f"âŒ ìµœì‹  íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì—…ë°ì´íŠ¸ ë¡œì§ ì‚¬ìš©
            update_news_cache()
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        news_cache.initial_fetch_done = True
        news_cache.daily_task_last_run = datetime.now()
        
        logger.info("âœ… ì´ˆê¸° ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # ì˜êµ¬ ì €ì¥ì†Œì— ìµœì‹  íŒŒì¼ ë³µì‚¬ (Render.com í™˜ê²½ì—ì„œë§Œ)
        if os.environ.get('RENDER') == 'true' and PERSISTENT_DIR:
            # ë°©ê¸ˆ ì°¾ì€ ìµœì‹  íŒŒì¼ ì‚¬ìš©
            latest_file = newest_file
            dest_file = PERSISTENT_DIR / "trend_summary_latest.json"
            shutil.copy(latest_file, dest_file)
            logger.info(f"ğŸ“‹ ìµœì‹  ë°ì´í„° íŒŒì¼ì„ ì˜êµ¬ ì €ì¥ì†Œì— ë³µì‚¬: {latest_file.name} -> trend_summary_latest.json")
            
            # ì›ë³¸ íŒŒì¼ë„ ì˜êµ¬ ì €ì¥ì†Œì— ë³µì‚¬
            perm_file = PERSISTENT_DIR / latest_file.name
            shutil.copy(latest_file, perm_file)
            logger.info(f"ğŸ“‹ ë°ì´í„° íŒŒì¼ì„ ì˜êµ¬ ì €ì¥ì†Œì— ë³µì‚¬: {latest_file.name}")
    except Exception as e:
        logger.error(f"âŒ ì´ˆê¸° ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")

# ê°•ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜ ì¶”ê°€
def force_news_collection():
    """ê°•ì œë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜"""
    try:
        logger.info("ğŸ”¥ ê°•ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # ê¸°ì¡´ ìƒíƒœ ì´ˆê¸°í™”
        news_cache.initial_fetch_done = False
        news_cache.daily_task_last_run = None
        
        # ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰
        pipeline.run_daily_collection()
        
        # ìºì‹œ ì—…ë°ì´íŠ¸
        update_news_cache()
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        news_cache.initial_fetch_done = True
        news_cache.daily_task_last_run = datetime.now()
        
        logger.info("âœ… ê°•ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        return True
    except Exception as e:
        logger.error(f"âŒ ê°•ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
        return False

# --- API ì—”ë“œí¬ì¸íŠ¸ ---
@app.get("/status")
async def get_status():
    """ì„œë²„ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸"""
    # í˜„ì¬ ì‹œê°„
    now = datetime.now()
    
    # ì˜¤ëŠ˜ ë‚ ì§œì˜ íŒŒì¼ í™•ì¸
    today = now.strftime("%Y-%m-%d")
    today_files = list(ASSETS_DIR.glob(f"trend_summary_{today}_*.json"))
    
    # ë‹¤ìŒ ìŠ¤ì¼€ì¤„ ì‹¤í–‰ ì‹œê°„
    next_run = schedule.next_run() if schedule.jobs else None
    
    # ìºì‹œ ìƒíƒœ
    cache_status = news_cache.get_status()
    
    return {
        "status": "healthy" if cache_status["is_healthy"] else "degraded",
        "server_time": now.isoformat(),
        "timezone": "UTC",
        "cache": cache_status,
        "scheduler": {
            "running": news_cache.scheduler_running,
            "jobs_count": len(schedule.jobs),
            "jobs": [str(job) for job in schedule.jobs],
            "next_run": next_run.isoformat() if next_run else None,
            "time_until_next_run": str(next_run - now) if next_run else None
        },
        "files": {
            "today_files_count": len(today_files),
            "today_files": [f.name for f in today_files],
            "assets_dir_exists": ASSETS_DIR.exists(),
            "persistent_dir_exists": PERSISTENT_DIR.exists() if PERSISTENT_DIR else False
        },
        "environment": {
            "render": os.environ.get('RENDER') == 'true',
            "openai_api_key_set": bool(os.getenv('OPENAI_API_KEY')),
            "assets_dir": str(ASSETS_DIR),
            "persistent_dir": str(PERSISTENT_DIR) if PERSISTENT_DIR else None
        },
        "uptime": str(now - cache_status.get("last_update", now)) if cache_status.get("last_update") else None
    }

@app.get("/news")
async def get_news_data():
    """ë‰´ìŠ¤ ë°ì´í„° ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # ìºì‹œì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì—…ë°ì´íŠ¸
        if not news_cache.latest_data:
            update_news_cache()
                
            if not news_cache.latest_data:
                # ê¸°ë³¸ ë°ì´í„° ë°˜í™˜
                if DEFAULT_DATA_FILE.exists():
                    with open(DEFAULT_DATA_FILE, "r", encoding="utf-8") as f:
                        default_data = json.load(f)
                else:
                    default_data = {
                        "trend_summary": "ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤...",
                        "candidate_stats": {
                            "ì´ì¬ëª…": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0},
                            "ê¹€ë¬¸ìˆ˜": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0},
                            "ì´ì¤€ì„": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0}
                        },
                        "total_articles": 0,
                        "time_range": "ë°ì´í„° ìˆ˜ì§‘ ì¤‘",
                        "news_list": []  # ë¹ˆ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
                    }
                
                # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘
                if not news_cache.initial_fetch_done:
                    import threading
                    initial_fetch_thread = threading.Thread(target=run_initial_fetch, daemon=True)
                    initial_fetch_thread.start()
                    logger.info("ğŸ”„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì´ˆê¸° ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                    news_cache.initial_fetch_done = True
                
                return {
                    "data": default_data,
                    "metadata": {
                        "last_updated": datetime.now().isoformat(),
                        "next_update": (datetime.now() + timedelta(hours=1)).isoformat(),
                        "status": "using_default",
                        "message": "ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”."
                    }
                }
        
        # news_list í•„ë“œê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ìµœì‹  íŒŒì¼ ê°•ì œ í™•ì¸
        if "news_list" not in news_cache.latest_data or not news_cache.latest_data["news_list"]:
            logger.warning("âš ï¸ news_listê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ìµœì‹  íŒŒì¼ì„ ê°•ì œë¡œ í™•ì¸í•©ë‹ˆë‹¤.")
            
            # ì˜¤ëŠ˜ ë‚ ì§œì˜ íŒŒì¼ì„ ì°¾ì•„ì„œ ì§ì ‘ ë¡œë“œ
            today = datetime.now().strftime("%Y-%m-%d")
            today_files = list(ASSETS_DIR.glob(f"trend_summary_{today}_*.json"))
            
            if today_files:
                # ì˜¤ëŠ˜ ìƒì„±ëœ íŒŒì¼ ì¤‘ ê°€ì¥ ìµœì‹  íŒŒì¼ ì‚¬ìš©
                newest_file = sorted(today_files, key=lambda p: p.stat().st_mtime, reverse=True)[0]
                logger.info(f"âœ… ì˜¤ëŠ˜ ìƒì„±ëœ ìµœì‹  íŒŒì¼ ë°œê²¬: {newest_file}")
                
                try:
                    with open(newest_file, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        
                        # ë‰´ìŠ¤ ë°ì´í„° í™•ì¸
                        if "news_list" in data and data["news_list"]:
                            # ì¤‘ìš”ë„ë¡œ ì •ë ¬
                            sorted_news = rank_news_by_importance(data["news_list"], limit=30)
                            data["news_list"] = sorted_news
                            
                            # ìºì‹œ ì—…ë°ì´íŠ¸
                            news_cache.update(data)
                            logger.info(f"âœ… ìµœì‹  íŒŒì¼ì—ì„œ ë‰´ìŠ¤ {len(sorted_news)}ê°œ ë¡œë“œ ì™„ë£Œ")
                        else:
                            logger.warning(f"âš ï¸ ìµœì‹  íŒŒì¼ì—ë„ news_listê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤: {newest_file}")
                except Exception as e:
                    logger.error(f"âŒ ìµœì‹  íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            
            # ì—¬ì „íˆ ë‰´ìŠ¤ ëª©ë¡ì´ ì—†ìœ¼ë©´ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘
            if not news_cache.latest_data or "news_list" not in news_cache.latest_data or not news_cache.latest_data["news_list"]:
                logger.warning("âš ï¸ news_listê°€ ì—¬ì „íˆ ë¹„ì–´ìˆì–´ ê¸°ë³¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                
                # ê°•ì œë¡œ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)
                if not news_cache.initial_fetch_done:
                    import threading
                    initial_fetch_thread = threading.Thread(target=run_initial_fetch, daemon=True)
                    initial_fetch_thread.start()
                    logger.info("ğŸ”„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì´ˆê¸° ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                    news_cache.initial_fetch_done = True
                
                # news_list í•„ë“œê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ ì¶”ê°€
                if "news_list" not in news_cache.latest_data:
                    news_cache.latest_data["news_list"] = []
                
                # ì‚¬ìš©ìì—ê²Œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ì„ì„ ì•Œë¦¬ëŠ” ë©”ì‹œì§€ ì¶”ê°€
                return {
                    "data": news_cache.latest_data,
                    "metadata": {
                        "last_updated": news_cache.last_update.isoformat() if news_cache.last_update else datetime.now().isoformat(),
                        "next_update": (datetime.now() + timedelta(hours=1)).isoformat(),
                        "status": "collecting",
                        "message": "ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”."
                    }
                }
        
        # ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ìˆì§€ë§Œ ì•„ì§ ì¤‘ìš”ë„ ì •ë ¬ì´ ì•ˆëœ ê²½ìš°
        elif news_cache.latest_data["news_list"] and not any("importance_score" in news for news in news_cache.latest_data["news_list"]):
            try:
                # ì§ì ‘ í•¨ìˆ˜ í˜¸ì¶œ
                sorted_news = rank_news_by_importance(news_cache.latest_data["news_list"], limit=30)
                news_cache.latest_data["news_list"] = sorted_news
                logger.info(f"âœ… API ìš”ì²­ ì‹œ ë‰´ìŠ¤ ë°ì´í„° ì¤‘ìš”ë„ ì •ë ¬ ì™„ë£Œ: {len(sorted_news)}ê°œ")
            except Exception as e:
                logger.error(f"ë‰´ìŠ¤ ì •ë ¬ ì˜¤ë¥˜: {str(e)}")
            
        return {
            "data": news_cache.latest_data,
            "metadata": {
                "last_updated": news_cache.last_update.isoformat() if news_cache.last_update else datetime.now().isoformat(),
                "next_update": (news_cache.last_update + timedelta(hours=24)).isoformat() if news_cache.last_update else (datetime.now() + timedelta(hours=24)).isoformat(),
                "status": "success"
            }
        }
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return {
            "data": {
                "trend_summary": "ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "candidate_stats": {
                    "ì´ì¬ëª…": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0},
                    "ê¹€ë¬¸ìˆ˜": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0},
                    "ì´ì¤€ì„": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0}
                },
                "total_articles": 0,
                "time_range": "ì˜¤ë¥˜ ë°œìƒ",
                "news_list": []  # ë¹ˆ ë°°ì—´ ì¶”ê°€
            },
            "metadata": {
                "last_updated": datetime.now().isoformat(),
                "next_update": (datetime.now() + timedelta(hours=24)).isoformat(),
                "status": "error",
                "error": str(e)
            }
        }

@app.post("/refresh")
async def force_refresh(background_tasks: BackgroundTasks):
    """ìˆ˜ë™ìœ¼ë¡œ ë‰´ìŠ¤ ë°ì´í„° ìƒˆë¡œê³ ì¹¨"""
    # ë§ˆì§€ë§‰ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œë¶€í„° ìµœì†Œ 30ë¶„ì´ ì§€ë‚¬ëŠ”ì§€ í™•ì¸ (ë‚¨ìš© ë°©ì§€, 1ì‹œê°„ì—ì„œ 30ë¶„ìœ¼ë¡œ ë‹¨ì¶•)
    if news_cache.daily_task_last_run:
        time_since_last_run = datetime.now() - news_cache.daily_task_last_run
        if time_since_last_run.total_seconds() < 1800:  # 30ë¶„ = 1800ì´ˆ
            return {
                "message": f"ìƒˆë¡œê³ ì¹¨ ìš”ì²­ì´ ë„ˆë¬´ ë¹ˆë²ˆí•©ë‹ˆë‹¤. {1800 - int(time_since_last_run.total_seconds())}ì´ˆ í›„ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "status": "rate_limited",
                "last_run": news_cache.daily_task_last_run.isoformat(),
                "next_available": (news_cache.daily_task_last_run + timedelta(minutes=30)).isoformat()
            }
    
    logger.info("ğŸ”„ ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨ ìš”ì²­ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.")
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê°•ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰
    background_tasks.add_task(force_news_collection)
    
    return {
        "message": "ë‰´ìŠ¤ ë°ì´í„° ìƒˆë¡œê³ ì¹¨ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì•½ 2-3ë¶„ í›„ì— ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "status": "started",
        "estimated_completion": (datetime.now() + timedelta(minutes=3)).isoformat()
    }

@app.post("/update-cache")
async def force_update_cache():
    """ìºì‹œë¥¼ ê°•ì œë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸"""
    try:
        logger.info("ğŸ”„ ìºì‹œ ê°•ì œ ì—…ë°ì´íŠ¸ ìš”ì²­ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.")
        
        # ìºì‹œ ì—…ë°ì´íŠ¸ ì‹¤í–‰
        update_news_cache()
        
        # ì—…ë°ì´íŠ¸ í›„ ìƒíƒœ í™•ì¸
        if news_cache.latest_data:
            return {
                "message": "ìºì‹œê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "status": "success",
                "last_updated": news_cache.last_update.isoformat() if news_cache.last_update else None,
                "news_count": len(news_cache.latest_data.get("news_list", [])),
                "time_range": news_cache.latest_data.get("time_range", "ì•Œ ìˆ˜ ì—†ìŒ")
            }
        else:
            return {
                "message": "ìºì‹œ ì—…ë°ì´íŠ¸ëŠ” ì™„ë£Œë˜ì—ˆì§€ë§Œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                "status": "no_data",
                "last_updated": news_cache.last_update.isoformat() if news_cache.last_update else None
            }
    except Exception as e:
        logger.error(f"âŒ ìºì‹œ ê°•ì œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return {
            "message": f"ìºì‹œ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "status": "error"
        }

# --- ì„œë²„ ì‹œì‘ ì´ë²¤íŠ¸ ---
@app.on_event("startup")
async def startup_event():
    # ì´ˆê¸° ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ
    try:
        logger.info("ï¿½ï¿½ ì´ˆê¸° ë°ì´í„° ë¡œë“œ ì‹œì‘...")
        
        # ë¨¼ì € ìºì‹œ ê°•ì œ ì—…ë°ì´íŠ¸
        update_news_cache()
        logger.info("âœ… ì´ˆê¸° ìºì‹œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • - ëª…í™•í•˜ê²Œ í•˜ë‚˜ì˜ ì‘ì—…ë§Œ ì§€ì •
        schedule.clear()  # ê¸°ì¡´ ìŠ¤ì¼€ì¤„ ì´ˆê¸°í™”
        schedule.every().day.at("06:00").do(run_scheduled_news_pipeline)  # ë§¤ì¼ ì˜¤ì „ 6ì‹œì— ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„
        schedule.every(30).minutes.do(update_news_cache)  # ìºì‹œ ì—…ë°ì´íŠ¸ëŠ” 30ë¶„ë§ˆë‹¤ ìœ ì§€
        
        logger.info("ğŸ“… ìŠ¤ì¼€ì¤„ ì„¤ì • ì™„ë£Œ:")
        for job in schedule.jobs:
            logger.info(f"  - {job}")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
        if not news_cache.scheduler_running:
            # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰
            scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
            scheduler_thread.start()
            logger.info("ğŸ•’ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ë¨")
        
        # ì˜¤ëŠ˜ ë‚ ì§œì˜ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
        today = datetime.now().strftime("%Y-%m-%d")
        today_files = list(ASSETS_DIR.glob(f"trend_summary_{today}_*.json"))
        
        # ì˜¤ëŠ˜ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ê¸°ë³¸ ë°ì´í„°ë§Œ ìˆìœ¼ë©´ ê°•ì œë¡œ ìƒˆë¡œ ìˆ˜ì§‘
        if not today_files:
            logger.info("ğŸ”„ ì˜¤ëŠ˜ ìƒì„±ëœ ë°ì´í„°ê°€ ì—†ì–´ ê°•ì œë¡œ ìƒˆë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê°•ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘
            import threading
            force_fetch_thread = threading.Thread(target=force_news_collection, daemon=True)
            force_fetch_thread.start()
            logger.info("ğŸ”„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê°•ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        else:
            logger.info(f"âœ… ì˜¤ëŠ˜ ìƒì„±ëœ ë°ì´í„° íŒŒì¼ì´ ì´ë¯¸ ìˆìŠµë‹ˆë‹¤: {today_files[0].name}")
            # ê·¸ë˜ë„ ìºì‹œì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê°•ì œ ìˆ˜ì§‘
            if not news_cache.latest_data or "news_list" not in news_cache.latest_data or not news_cache.latest_data["news_list"]:
                logger.warning("âš ï¸ ìºì‹œì— ë‰´ìŠ¤ ëª©ë¡ì´ ì—†ìŠµë‹ˆë‹¤. ê°•ì œ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                import threading
                force_fetch_thread = threading.Thread(target=force_news_collection, daemon=True)
                force_fetch_thread.start()
                logger.info("ğŸ”„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê°•ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            
        # ìºì‹œì— ë°ì´í„°ê°€ ìˆëŠ”ì§€, news_listê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if not news_cache.latest_data or "news_list" not in news_cache.latest_data or not news_cache.latest_data["news_list"]:
            logger.warning("âš ï¸ ìºì‹œì— ë‰´ìŠ¤ ëª©ë¡ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
            # ê¸°ë³¸ ë°ì´í„° íŒŒì¼ì—ì„œ ë‹¤ì‹œ ë¡œë“œ ì‹œë„
            default_data_path = ASSETS_DIR / "trend_summary_default.json"
            if default_data_path.exists():
                try:
                    with open(default_data_path, "r", encoding="utf-8") as f:
                        default_data = json.load(f)
                        if "news_list" in default_data and default_data["news_list"]:
                            logger.info("âœ… ê¸°ë³¸ ë°ì´í„° íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ëª©ë¡ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                            news_cache.update(default_data)
                except Exception as e:
                    logger.error(f"âŒ ê¸°ë³¸ ë°ì´í„° íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                    
        # ì„œë²„ ì‹œì‘ í›„ 5ë¶„ ë’¤ì— í•œ ë²ˆ ë” ê°•ì œ ìˆ˜ì§‘ ì‹œë„ (ë³´í—˜ìš©)
        def delayed_force_collection():
            time.sleep(300)  # 5ë¶„ ëŒ€ê¸°
            if not news_cache.latest_data or "news_list" not in news_cache.latest_data or not news_cache.latest_data["news_list"]:
                logger.warning("âš ï¸ 5ë¶„ í›„ì—ë„ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ê°•ì œ ìˆ˜ì§‘ì„ ì‹œë„í•©ë‹ˆë‹¤.")
                force_news_collection()
        
        delayed_thread = threading.Thread(target=delayed_force_collection, daemon=True)
        delayed_thread.start()
        logger.info("â° 5ë¶„ í›„ ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ ì²´í¬ê°€ ì˜ˆì•½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        logger.info("âœ… ì„œë²„ ì‹œì‘ ì´ë²¤íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ì„œë²„ ì‹œì‘ ì´ë²¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        # ì‹¤íŒ¨í•´ë„ ì„œë²„ëŠ” ê³„ì† ì‹¤í–‰ë˜ë„ë¡ í•¨

# --- Flutter ì›¹ ì•± ì œê³µ ---
if not FLUTTER_BUILD_DIR.exists() or not (FLUTTER_BUILD_DIR / "index.html").exists():
    logger.warning(f"Flutter ë¹Œë“œ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {FLUTTER_BUILD_DIR}")
    
    @app.get("/")
    async def fallback_root():
        return JSONResponse({
            "status": "warning",
            "message": "Flutter UI ë¹Œë“œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. APIëŠ” /news ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.",
            "endpoints": {
                "/news": "ë‰´ìŠ¤ ë°ì´í„° ì¡°íšŒ",
                "/status": "ì„œë²„ ìƒíƒœ í™•ì¸",
                "/refresh": "ìˆ˜ë™ ë°ì´í„° ìƒˆë¡œê³ ì¹¨"
            }
        })
else:
    @app.get("/")
    async def root():
        return FileResponse(str(FLUTTER_BUILD_DIR / "index.html"))

    # ì •ì  íŒŒì¼ ì œê³µ - ê° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ ë§ˆìš´íŠ¸
    # 1. Flutter ì›¹ ë¹Œë“œ íŒŒì¼ì„ static ë””ë ‰í† ë¦¬ë¡œ ì œê³µ
    if STATIC_DIR.exists():
        try:
            app.mount("/", StaticFiles(directory=str(STATIC_DIR), html=True), name="static")
            logger.info(f"âœ… ì •ì  íŒŒì¼ ë””ë ‰í† ë¦¬ ë§ˆìš´íŠ¸ ì™„ë£Œ: {STATIC_DIR}")
        except Exception as e:
            logger.error(f"âŒ ì •ì  íŒŒì¼ ë””ë ‰í† ë¦¬ ë§ˆìš´íŠ¸ ì‹¤íŒ¨: {str(e)}")
    else:
        logger.warning(f"âš ï¸ ì •ì  íŒŒì¼ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {STATIC_DIR}")
        # 2. ëŒ€ì•ˆìœ¼ë¡œ Flutter ë¹Œë“œ ë””ë ‰í† ë¦¬ ì§ì ‘ ë§ˆìš´íŠ¸ ì‹œë„
        try:
            app.mount("/", StaticFiles(directory=str(FLUTTER_BUILD_DIR), html=True), name="flutter_web")
            logger.info(f"âœ… Flutter ì›¹ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ë§ˆìš´íŠ¸ ì™„ë£Œ: {FLUTTER_BUILD_DIR}")
        except Exception as e:
            logger.error(f"âŒ Flutter ì›¹ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ë§ˆìš´íŠ¸ ì‹¤íŒ¨: {str(e)}")
    
    # 3. ê°œë³„ ë””ë ‰í† ë¦¬ ë§ˆìš´íŠ¸ (ì—¬ì „íˆ ìœ íš¨í•˜ì§€ë§Œ ìœ„ì˜ ì „ì²´ ë§ˆìš´íŠ¸ê°€ ë¨¼ì € ì‹œë„ë¨)
    try:
        if (FLUTTER_BUILD_DIR / "assets").exists():
            app.mount("/assets", StaticFiles(directory=str(FLUTTER_BUILD_DIR / "assets")), name="flutter_assets")
            logger.info(f"âœ… Flutter assets ë””ë ‰í† ë¦¬ ë§ˆìš´íŠ¸ ì™„ë£Œ: {FLUTTER_BUILD_DIR / 'assets'}")
        else:
            logger.warning(f"âš ï¸ Flutter assets ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {FLUTTER_BUILD_DIR / 'assets'}")
    except Exception as e:
        logger.error(f"âŒ assets ë””ë ‰í† ë¦¬ ë§ˆìš´íŠ¸ ì‹¤íŒ¨: {str(e)}")
    
    try:
        if (FLUTTER_BUILD_DIR / "icons").exists():
            app.mount("/icons", StaticFiles(directory=str(FLUTTER_BUILD_DIR / "icons")), name="flutter_icons")
            logger.info(f"âœ… Flutter icons ë””ë ‰í† ë¦¬ ë§ˆìš´íŠ¸ ì™„ë£Œ: {FLUTTER_BUILD_DIR / 'icons'}")
        else:
            logger.warning(f"âš ï¸ Flutter icons ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {FLUTTER_BUILD_DIR / 'icons'}")
    except Exception as e:
        logger.error(f"âŒ icons ë””ë ‰í† ë¦¬ ë§ˆìš´íŠ¸ ì‹¤íŒ¨: {str(e)}")
    
    try:
        if (FLUTTER_BUILD_DIR / "canvaskit").exists():
            app.mount("/canvaskit", StaticFiles(directory=str(FLUTTER_BUILD_DIR / "canvaskit")), name="flutter_canvaskit")
            logger.info(f"âœ… Flutter canvaskit ë””ë ‰í† ë¦¬ ë§ˆìš´íŠ¸ ì™„ë£Œ: {FLUTTER_BUILD_DIR / 'canvaskit'}")
        else:
            logger.warning(f"âš ï¸ Flutter canvaskit ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {FLUTTER_BUILD_DIR / 'canvaskit'}")
    except Exception as e:
        logger.error(f"âŒ canvaskit ë””ë ‰í† ë¦¬ ë§ˆìš´íŠ¸ ì‹¤íŒ¨: {str(e)}")
    
    # 4. ëª¨ë“  íŒŒì¼ì— ëŒ€í•œ ëª…ì‹œì ì¸ MIME íƒ€ì… ì„¤ì •ì´ ìˆëŠ” catch-all ë¼ìš°íŠ¸
    @app.get("/{path:path}")
    async def serve_flutter_web(path: str):
        # ë¨¼ì € static ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
        static_path = STATIC_DIR / path
        if static_path.exists():
            # íŒŒì¼ì˜ MIME íƒ€ì…ì„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
            mime_type = None
            if path.endswith('.js'):
                mime_type = 'application/javascript'
            elif path.endswith('.css'):
                mime_type = 'text/css'
            elif path.endswith('.html'):
                mime_type = 'text/html'
            elif path.endswith('.json'):
                mime_type = 'application/json'
            return FileResponse(str(static_path), media_type=mime_type)
            
        # ë‹¤ìŒìœ¼ë¡œ Flutter ë¹Œë“œ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
        flutter_path = FLUTTER_BUILD_DIR / path
        if flutter_path.exists():
            # íŒŒì¼ì˜ MIME íƒ€ì…ì„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
            mime_type = None
            if path.endswith('.js'):
                mime_type = 'application/javascript'
            elif path.endswith('.css'):
                mime_type = 'text/css'
            elif path.endswith('.html'):
                mime_type = 'text/html'
            elif path.endswith('.json'):
                mime_type = 'application/json'
            return FileResponse(str(flutter_path), media_type=mime_type)
            
        # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ index.html ë°˜í™˜
        if (STATIC_DIR / "index.html").exists():
            return FileResponse(str(STATIC_DIR / "index.html"))
        return FileResponse(str(FLUTTER_BUILD_DIR / "index.html"))