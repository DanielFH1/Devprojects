"""
ëŒ€ì„  ì‹œë®¬ë ˆì´í„° API ì„œë²„
- ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„
- Flutter ì›¹ ì•± ì„œë¹™
- ì‹¤ì‹œê°„ ë°ì´í„° ìºì‹±
"""

import os
import sys
import json
import time
import shutil
import schedule
import threading
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_dir = Path(__file__).parent
parent_dir = current_dir.parent
sys.path.insert(0, str(parent_dir))

import uvicorn
from fastapi import FastAPI, BackgroundTasks, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware

import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# news_scraper ëª¨ë“ˆ import (ì˜¤ë¥˜ ì²˜ë¦¬ í¬í•¨)
try:
    from news_scraper import NewsPipeline, rank_news_by_importance
    NEWS_SCRAPER_AVAILABLE = True
    logger.info("âœ… news_scraper ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    logger.error(f"âŒ news_scraper ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    NEWS_SCRAPER_AVAILABLE = False
    
    # ë”ë¯¸ í´ë˜ìŠ¤ ìƒì„±
    class NewsPipeline:
        def __init__(self):
            pass
        def run_daily_collection(self):
            logger.warning("âš ï¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def rank_news_by_importance(news_data, limit=30):
        return news_data[:limit]

# === ìƒìˆ˜ ë° ì„¤ì • ===
ASSETS_DIR = parent_dir / "assets"
ASSETS_DIR.mkdir(parents=True, exist_ok=True)

DEFAULT_DATA_FILE = ASSETS_DIR / "trend_summary_default.json"

# Render.com ì˜êµ¬ ì €ì¥ì†Œ ì„¤ì •
PERSISTENT_DIR = None
if os.environ.get('RENDER') == 'true':
    PERSISTENT_DIR = Path("/opt/render/project/src/persistent_data")
    PERSISTENT_DIR.mkdir(parents=True, exist_ok=True)
    logger.info(f"ğŸ“‚ Render.com ì˜êµ¬ ì €ì¥ì†Œ ì„¤ì •: {PERSISTENT_DIR}")

# Flutter ì›¹ ì•± ê²½ë¡œ
FLUTTER_WEB_DIR = parent_dir / "flutter_ui/web"

# === ë‰´ìŠ¤ ìºì‹œ ê´€ë¦¬ í´ë˜ìŠ¤ ===
class NewsCache:
    """ë‰´ìŠ¤ ë°ì´í„° ìºì‹œ ê´€ë¦¬"""
    
    def __init__(self):
        self.latest_data: Optional[Dict[str, Any]] = None
        self.last_update: Optional[datetime] = None
        self.update_count: int = 0
        self.error_count: int = 0
        self.last_error: Optional[str] = None
        self.pipeline = NewsPipeline()
        self.scheduler_running = False
        self.daily_task_last_run = None
        self.initial_fetch_done = False

    def update(self, data: Dict[str, Any]) -> None:
        """ìºì‹œ ë°ì´í„° ì—…ë°ì´íŠ¸"""
        self.latest_data = data
        self.last_update = datetime.now()
        self.update_count += 1
        self.error_count = 0
        self.last_error = None
        logger.info(f"âœ… ìºì‹œ ì—…ë°ì´íŠ¸ ì™„ë£Œ (#{self.update_count})")

    def record_error(self, error: str) -> None:
        """ì˜¤ë¥˜ ê¸°ë¡"""
        self.error_count += 1
        self.last_error = error
        logger.error(f"âŒ ìºì‹œ ì˜¤ë¥˜ #{self.error_count}: {error}")

    def get_status(self) -> Dict[str, Any]:
        """ìºì‹œ ìƒíƒœ ë°˜í™˜"""
        return {
            "last_update": self.last_update.isoformat() if self.last_update else None,
            "update_count": self.update_count,
            "error_count": self.error_count,
            "last_error": self.last_error,
            "is_healthy": self.error_count < 3 and self.last_update is not None,
            "scheduler_running": self.scheduler_running,
            "daily_task_last_run": self.daily_task_last_run.isoformat() if self.daily_task_last_run else None,
            "initial_fetch_done": self.initial_fetch_done
        }

    def is_today_data(self) -> bool:
        """í˜„ì¬ ìºì‹œ ë°ì´í„°ê°€ ì˜¤ëŠ˜ ê²ƒì¸ì§€ í™•ì¸"""
        if not self.latest_data:
            return False
        
        today = datetime.now().strftime("%Y-%m-%d")
        time_range = self.latest_data.get("time_range", "")
        return today in time_range

# === íŒŒì¼ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹° ===
class FileManager:
    """íŒŒì¼ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°"""
    
    @staticmethod
    def extract_datetime_from_filename(filepath: Path) -> datetime:
        """íŒŒì¼ëª…ì—ì„œ ë‚ ì§œì‹œê°„ ì¶”ì¶œ"""
        try:
            # trend_summary_2025-05-26_13-03.json í˜•ì‹ì—ì„œ ë‚ ì§œì‹œê°„ ì¶”ì¶œ
            parts = filepath.stem.split('_')
            if len(parts) >= 3:
                date_str = parts[2]  # 2025-05-26
                time_str = parts[3] if len(parts) > 3 else "00-00"  # 13-03
                datetime_str = f"{date_str} {time_str.replace('-', ':')}"
                return datetime.strptime(datetime_str, "%Y-%m-%d %H:%M")
        except Exception:
            pass
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ íŒŒì¼ ìˆ˜ì • ì‹œê°„ ì‚¬ìš©
        return datetime.fromtimestamp(filepath.stat().st_mtime)

    @staticmethod
    def find_latest_news_file() -> Optional[Path]:
        """ìµœì‹  ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ ì°¾ê¸°"""
        # ê¸°ë³¸ íŒŒì¼ ì œì™¸í•˜ê³  ë‚ ì§œê°€ í¬í•¨ëœ íŒŒì¼ë“¤ë§Œ ì°¾ê¸°
        news_files = [
            f for f in ASSETS_DIR.glob("trend_summary_*.json")
            if f.name != "trend_summary_default.json"
        ]
        
        if not news_files:
            return None
            
        try:
            # ë‚ ì§œì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ê°€ì¥ ìµœì‹  íŒŒì¼ ì„ íƒ
            latest_file = sorted(
                news_files,
                key=FileManager.extract_datetime_from_filename,
                reverse=True
            )[0]
            logger.info(f"ğŸ“‚ ìµœì‹  ë‚ ì§œ íŒŒì¼ ë°œê²¬: {latest_file}")
            return latest_file
        except Exception as e:
            # ì •ë ¬ ì‹¤íŒ¨ ì‹œ ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ í´ë°±
            latest_file = sorted(news_files, key=lambda p: p.stat().st_mtime, reverse=True)[0]
            logger.warning(f"âš ï¸ ë‚ ì§œ ê¸°ì¤€ ì •ë ¬ ì‹¤íŒ¨, ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ ì‚¬ìš©: {str(e)}")
            return latest_file

    @staticmethod
    def get_today_files() -> List[Path]:
        """ì˜¤ëŠ˜ ë‚ ì§œì˜ íŒŒì¼ë“¤ ë°˜í™˜"""
        today = datetime.now().strftime("%Y-%m-%d")
        return list(ASSETS_DIR.glob(f"trend_summary_{today}_*.json"))

    @staticmethod
    def load_json_file(filepath: Path) -> Optional[Dict[str, Any]]:
        """JSON íŒŒì¼ ë¡œë“œ"""
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {filepath}: {str(e)}")
            return None

# === ë°ì´í„° ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° ===
class DataProcessor:
    """ë°ì´í„° ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°"""
    
    @staticmethod
    def create_default_data(message: str = "ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤...") -> Dict[str, Any]:
        """ê¸°ë³¸ ë°ì´í„° êµ¬ì¡° ìƒì„±"""
        current_time = datetime.now()
        return {
            "trend_summary": message,
            "candidate_stats": {
                "ì´ì¬ëª…": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0},
                "ê¹€ë¬¸ìˆ˜": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0},
                "ì´ì¤€ì„": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0}
            },
            "total_articles": 0,
            "time_range": f"{current_time.strftime('%Y-%m-%d')} ì—…ë°ì´íŠ¸",
            "news_list": []
        }

    @staticmethod
    def process_news_data(data: Dict[str, Any]) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ë°ì´í„° ì²˜ë¦¬ ë° ì •ê·œí™”"""
        processed_data = {
            "trend_summary": data.get("trend_summary", "ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤..."),
            "candidate_stats": data.get("candidate_stats", DataProcessor.create_default_data()["candidate_stats"]),
            "total_articles": data.get("total_articles", 0),
            "time_range": data.get("time_range", "ë°ì´í„° ìˆ˜ì§‘ ì¤‘"),
            "news_list": data.get("news_list", [])
        }
        
        # ë‰´ìŠ¤ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¤‘ìš”ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        if processed_data["news_list"]:
            try:
                sorted_news = rank_news_by_importance(processed_data["news_list"], limit=30)
                processed_data["news_list"] = sorted_news
                logger.info(f"âœ… ë‰´ìŠ¤ ë°ì´í„° ì¤‘ìš”ë„ ì •ë ¬ ì™„ë£Œ: {len(sorted_news)}ê°œ")
            except Exception as e:
                logger.error(f"âŒ ë‰´ìŠ¤ ì •ë ¬ ì˜¤ë¥˜: {str(e)}")
        
        return processed_data

# === ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ===
news_cache = NewsCache()
file_manager = FileManager()
data_processor = DataProcessor()

# === ìºì‹œ ê´€ë¦¬ í•¨ìˆ˜ ===
def update_news_cache() -> None:
    """ë‰´ìŠ¤ ìºì‹œ ì—…ë°ì´íŠ¸"""
    try:
        # ìµœì‹  íŒŒì¼ ì°¾ê¸°
        latest_file = file_manager.find_latest_news_file()
        
        # ì˜êµ¬ ì €ì¥ì†Œì—ì„œ í™•ì¸
        if not latest_file and PERSISTENT_DIR and PERSISTENT_DIR.exists():
            latest_link = PERSISTENT_DIR / "trend_summary_latest.json"
            if latest_link.exists():
                latest_file = latest_link
                logger.info(f"ğŸ“‚ ì˜êµ¬ ì €ì¥ì†Œì—ì„œ ìµœì‹  íŒŒì¼ ë°œê²¬: {latest_file}")
                
                # ì˜êµ¬ ì €ì¥ì†Œì˜ íŒŒì¼ì„ assetsì— ë³µì‚¬
                dest_file = ASSETS_DIR / f"trend_summary_{datetime.now().strftime('%Y-%m-%d_%H-%M')}.json"
                shutil.copy(latest_file, dest_file)
                logger.info(f"ğŸ“‹ ì˜êµ¬ ì €ì¥ì†Œì˜ íŒŒì¼ì„ assetsì— ë³µì‚¬: {dest_file.name}")
                latest_file = dest_file
        
        # ê¸°ë³¸ íŒŒì¼ ì‚¬ìš©
        if not latest_file and DEFAULT_DATA_FILE.exists():
            latest_file = DEFAULT_DATA_FILE
            logger.warning("âš ï¸ ë‚ ì§œê°€ í¬í•¨ëœ ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # íŒŒì¼ ì²˜ë¦¬
        if latest_file:
            data = file_manager.load_json_file(latest_file)
            if data:
                processed_data = data_processor.process_news_data(data)
                news_cache.update(processed_data)
                logger.info(f"âœ… ë‰´ìŠ¤ ìºì‹œ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {latest_file.name}")
            else:
                news_cache.record_error(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {latest_file}")
        else:
            news_cache.record_error("ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        news_cache.record_error(str(e))
        logger.error(f"âŒ ìºì‹œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")

# === ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜ ===
def force_news_collection() -> bool:
    """ê°•ì œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    if not NEWS_SCRAPER_AVAILABLE:
        logger.warning("âš ï¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return False
        
    try:
        logger.info("ğŸ”¥ ê°•ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # ê°•ì œ ì‹¤í–‰ ëª¨ë“œ í™œì„±í™”
        os.environ['FORCE_NEWS_COLLECTION'] = 'true'
        
        # ìƒíƒœ ì´ˆê¸°í™”
        news_cache.initial_fetch_done = False
        news_cache.daily_task_last_run = None
        
        # ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰
        news_cache.pipeline.run_daily_collection()
        
        # í™˜ê²½ë³€ìˆ˜ í•´ì œ
        os.environ.pop('FORCE_NEWS_COLLECTION', None)
        
        # ìˆ˜ì§‘ í›„ ìºì‹œ ì—…ë°ì´íŠ¸
        today_files = file_manager.get_today_files()
        if today_files:
            newest_file = sorted(today_files, key=lambda p: p.stat().st_mtime, reverse=True)[0]
            logger.info(f"âœ… ìƒˆë¡œ ìƒì„±ëœ ì˜¤ëŠ˜ íŒŒì¼ ë°œê²¬: {newest_file}")
        
        update_news_cache()
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        news_cache.initial_fetch_done = True
        news_cache.daily_task_last_run = datetime.now()
        
        logger.info("âœ… ê°•ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ê°•ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
        os.environ.pop('FORCE_NEWS_COLLECTION', None)
        return False

def run_scheduled_news_pipeline() -> None:
    """ì˜ˆì •ëœ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    try:
        now = datetime.now()
        
        if news_cache.daily_task_last_run:
            last_run_date = news_cache.daily_task_last_run.date()
            if last_run_date == now.date():
                logger.info(f"â­ï¸ ì˜¤ëŠ˜({now.date()})ì€ ì´ë¯¸ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤.")
                return
        
        logger.info(f"ğŸ”” ì˜ˆì •ëœ ì¼ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {now}")
        force_news_collection()
        logger.info(f"âœ… ì¼ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {now}")
        
    except Exception as e:
        logger.error(f"âŒ ì˜ˆì •ëœ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")

def run_scheduler() -> None:
    """ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰"""
    news_cache.scheduler_running = True
    logger.info("ğŸ•’ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    while True:
        try:
            now = datetime.now()
            next_run = schedule.next_run()
            if next_run:
                logger.info(f"â° í˜„ì¬: {now.strftime('%Y-%m-%d %H:%M:%S')}, ë‹¤ìŒ ì‹¤í–‰: {next_run.strftime('%Y-%m-%d %H:%M:%S')}")
            
            schedule.run_pending()
            time.sleep(60)  # 1ë¶„ë§ˆë‹¤ í™•ì¸
        except Exception as e:
            logger.error(f"âŒ ìŠ¤ì¼€ì¤„ëŸ¬ ì˜¤ë¥˜: {str(e)}")
            time.sleep(300)  # ì˜¤ë¥˜ ë°œìƒ ì‹œ 5ë¶„ ëŒ€ê¸°

# === FastAPI ì•± ì„¤ì • ===
app = FastAPI(
    title="ëŒ€ì„  ì‹œë®¬ë ˆì´í„° API",
    description="2025ë…„ ëŒ€ì„  ë‰´ìŠ¤ ë¶„ì„ ë° ì˜ˆì¸¡ ì‹œë®¬ë ˆì´í„°",
    version="2.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# === API ì—”ë“œí¬ì¸íŠ¸ ===
@app.get("/api/status")
async def get_status():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    now = datetime.now()
    today = now.strftime("%Y-%m-%d")
    today_files = file_manager.get_today_files()
    next_run = schedule.next_run() if schedule.jobs else None
    cache_status = news_cache.get_status()
    
    return {
        "status": "healthy" if cache_status["is_healthy"] else "degraded",
        "server_time": now.isoformat(),
        "timezone": "UTC",
        "cache": cache_status,
        "scheduler": {
            "running": news_cache.scheduler_running,
            "jobs_count": len(schedule.jobs),
            "jobs": [str(job) for job in schedule.jobs],
            "next_run": next_run.isoformat() if next_run else None,
        },
        "files": {
            "today_files_count": len(today_files),
            "today_files": [f.name for f in today_files],
            "latest_file": file_manager.find_latest_news_file().name if file_manager.find_latest_news_file() else None,
        },
        "data_status": {
            "has_today_data": news_cache.is_today_data(),
            "news_count": len(news_cache.latest_data.get("news_list", [])) if news_cache.latest_data else 0,
            "time_range": news_cache.latest_data.get("time_range", "ì—†ìŒ") if news_cache.latest_data else "ì—†ìŒ"
        }
    }

@app.get("/api/trend-summary")
async def get_news_data():
    """ë‰´ìŠ¤ ë°ì´í„° ì¡°íšŒ"""
    try:
        # ìºì‹œì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì—…ë°ì´íŠ¸
        if not news_cache.latest_data:
            update_news_cache()
        
        # ì—¬ì „íˆ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë°ì´í„° ë°˜í™˜
        if not news_cache.latest_data:
            default_data = data_processor.create_default_data()
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘
            if not news_cache.initial_fetch_done:
                threading.Thread(target=force_news_collection, daemon=True).start()
                news_cache.initial_fetch_done = True
            
            return default_data
        
        # ì˜¤ëŠ˜ ë°ì´í„°ê°€ ì•„ë‹ˆë©´ ìƒˆë¡œ ìˆ˜ì§‘
        if not news_cache.is_today_data():
            logger.warning("âš ï¸ ìºì‹œì˜ ë°ì´í„°ê°€ ì˜¤ëŠ˜ ê²ƒì´ ì•„ë‹™ë‹ˆë‹¤. ìƒˆë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
            threading.Thread(target=force_news_collection, daemon=True).start()
        
        return news_cache.latest_data
        
    except Exception as e:
        logger.error(f"âŒ ë‰´ìŠ¤ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/prediction")
async def get_prediction_data():
    """ì˜ˆì¸¡ ë°ì´í„° ì¡°íšŒ"""
    try:
        # ìºì‹œì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì—…ë°ì´íŠ¸
        if not news_cache.latest_data:
            update_news_cache()
        
        # ê¸°ë³¸ ì˜ˆì¸¡ ë°ì´í„° ìƒì„±
        if not news_cache.latest_data:
            return {
                "predictions": {
                    "ì´ì¬ëª…": 35.0,
                    "ê¹€ë¬¸ìˆ˜": 30.0,
                    "ì´ì¤€ì„": 25.0
                },
                "analysis": "ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.",
                "total_articles": 0,
                "time_range": "ë°ì´í„° ìˆ˜ì§‘ ì¤‘"
            }
        
        # ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡ ìƒì„±
        candidate_stats = news_cache.latest_data.get("candidate_stats", {})
        total_articles = news_cache.latest_data.get("total_articles", 0)
        
        # ê°„ë‹¨í•œ ì˜ˆì¸¡ ì•Œê³ ë¦¬ì¦˜ (ê°ì„± ë¶„ì„ ê¸°ë°˜)
        predictions = {}
        base_score = 30.0  # ê¸°ë³¸ ì ìˆ˜
        
        for candidate, stats in candidate_stats.items():
            positive = stats.get("ê¸ì •", 0)
            negative = stats.get("ë¶€ì •", 0)
            neutral = stats.get("ì¤‘ë¦½", 0)
            total = positive + negative + neutral
            
            if total > 0:
                sentiment_score = (positive - negative) / total * 10
                predictions[candidate] = max(10.0, min(50.0, base_score + sentiment_score))
            else:
                predictions[candidate] = base_score
        
        # ì •ê·œí™” (ì´í•© 100%)
        total_pred = sum(predictions.values())
        if total_pred > 0:
            predictions = {k: (v / total_pred) * 100 for k, v in predictions.items()}
        
        return {
            "predictions": predictions,
            "analysis": news_cache.latest_data.get("trend_summary", "ë¶„ì„ ì¤‘..."),
            "total_articles": total_articles,
            "time_range": news_cache.latest_data.get("time_range", "")
        }
        
    except Exception as e:
        logger.error(f"âŒ ì˜ˆì¸¡ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/refresh")
async def force_refresh(background_tasks: BackgroundTasks):
    """ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨"""
    # ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ…
    if news_cache.daily_task_last_run:
        time_since_last_run = datetime.now() - news_cache.daily_task_last_run
        if time_since_last_run.total_seconds() < 1800:  # 30ë¶„
            return {
                "message": f"ìƒˆë¡œê³ ì¹¨ ìš”ì²­ì´ ë„ˆë¬´ ë¹ˆë²ˆí•©ë‹ˆë‹¤. {1800 - int(time_since_last_run.total_seconds())}ì´ˆ í›„ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "status": "rate_limited"
            }
    
    logger.info("ğŸ”„ ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨ ìš”ì²­")
    background_tasks.add_task(force_news_collection)
    
    return {
        "message": "ë‰´ìŠ¤ ë°ì´í„° ìƒˆë¡œê³ ì¹¨ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "status": "started",
        "estimated_completion": (datetime.now() + timedelta(minutes=3)).isoformat()
    }

@app.post("/api/update-cache")
async def force_update_cache():
    """ìºì‹œ ê°•ì œ ì—…ë°ì´íŠ¸"""
    try:
        logger.info("ğŸ”„ ìºì‹œ ê°•ì œ ì—…ë°ì´íŠ¸ ìš”ì²­")
        update_news_cache()
        
        if news_cache.latest_data:
            return {
                "message": "ìºì‹œê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "status": "success",
                "last_updated": news_cache.last_update.isoformat() if news_cache.last_update else None,
                "news_count": len(news_cache.latest_data.get("news_list", [])),
                "time_range": news_cache.latest_data.get("time_range", "ì•Œ ìˆ˜ ì—†ìŒ")
            }
        else:
            return {
                "message": "ìºì‹œ ì—…ë°ì´íŠ¸ëŠ” ì™„ë£Œë˜ì—ˆì§€ë§Œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                "status": "no_data"
            }
    except Exception as e:
        logger.error(f"âŒ ìºì‹œ ê°•ì œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return {"message": f"ìºì‹œ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}", "status": "error"}

@app.post("/api/force-today-collection")
async def force_today_collection(background_tasks: BackgroundTasks):
    """ì˜¤ëŠ˜ ë°ì´í„° ê°•ì œ ìˆ˜ì§‘"""
    try:
        today = datetime.now().strftime("%Y-%m-%d")
        logger.info(f"ğŸ”¥ ì˜¤ëŠ˜({today}) ë°ì´í„° ê°•ì œ ìˆ˜ì§‘ ìš”ì²­")
        
        background_tasks.add_task(force_news_collection)
        
        return {
            "message": f"ì˜¤ëŠ˜({today}) ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "status": "started",
            "target_date": today,
            "estimated_completion": (datetime.now() + timedelta(minutes=5)).isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ ì˜¤ëŠ˜ ë°ì´í„° ê°•ì œ ìˆ˜ì§‘ ìš”ì²­ ì‹¤íŒ¨: {str(e)}")
        return {"message": f"ì˜¤ëŠ˜ ë°ì´í„° ìˆ˜ì§‘ ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {str(e)}", "status": "error"}

# ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ë“¤ë„ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
@app.get("/status")
async def get_status_legacy():
    """ì„œë²„ ìƒíƒœ í™•ì¸ (ë ˆê±°ì‹œ)"""
    return await get_status()

@app.get("/news")
async def get_news_data_legacy():
    """ë‰´ìŠ¤ ë°ì´í„° ì¡°íšŒ (ë ˆê±°ì‹œ)"""
    result = await get_news_data()
    return {
        "data": result,
        "metadata": {
            "last_updated": news_cache.last_update.isoformat() if news_cache.last_update else datetime.now().isoformat(),
            "status": "success"
        }
    }

@app.post("/refresh")
async def force_refresh_legacy(background_tasks: BackgroundTasks):
    """ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨ (ë ˆê±°ì‹œ)"""
    return await force_refresh(background_tasks)

@app.post("/update-cache")
async def force_update_cache_legacy():
    """ìºì‹œ ê°•ì œ ì—…ë°ì´íŠ¸ (ë ˆê±°ì‹œ)"""
    return await force_update_cache()

@app.post("/force-today-collection")
async def force_today_collection_legacy(background_tasks: BackgroundTasks):
    """ì˜¤ëŠ˜ ë°ì´í„° ê°•ì œ ìˆ˜ì§‘ (ë ˆê±°ì‹œ)"""
    return await force_today_collection(background_tasks)

# === ì„œë²„ ì‹œì‘ ì´ë²¤íŠ¸ ===
@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    try:
        logger.info("ğŸš€ ì„œë²„ ì‹œì‘ - ì´ˆê¸°í™” ì¤‘...")
        
        # ë‰´ìŠ¤ ìˆ˜ì§‘ ê¸°ëŠ¥ ìƒíƒœ í™•ì¸
        if not NEWS_SCRAPER_AVAILABLE:
            logger.warning("âš ï¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ë°ì´í„°ë§Œ ì œê³µë©ë‹ˆë‹¤.")
            # ê¸°ë³¸ ë°ì´í„°ë¡œ ìºì‹œ ì´ˆê¸°í™”
            default_data = data_processor.create_default_data("ë‰´ìŠ¤ ìˆ˜ì§‘ ê¸°ëŠ¥ì´ ì¼ì‹œì ìœ¼ë¡œ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            news_cache.update(default_data)
            return
        
        # ìºì‹œ ì´ˆê¸° ì—…ë°ì´íŠ¸
        update_news_cache()
        logger.info("âœ… ì´ˆê¸° ìºì‹œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        schedule.clear()
        schedule.every().day.at("06:00").do(run_scheduled_news_pipeline)
        schedule.every(30).minutes.do(update_news_cache)
        
        logger.info("ğŸ“… ìŠ¤ì¼€ì¤„ ì„¤ì • ì™„ë£Œ:")
        for job in schedule.jobs:
            logger.info(f"  - {job}")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
        if not news_cache.scheduler_running:
            scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
            scheduler_thread.start()
            logger.info("ğŸ•’ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ë¨")
        
        # ì˜¤ëŠ˜ ë°ì´í„° í™•ì¸ ë° ìˆ˜ì§‘
        today = datetime.now().strftime("%Y-%m-%d")
        today_files = file_manager.get_today_files()
        
        logger.info(f"ğŸ“… ì˜¤ëŠ˜ ë‚ ì§œ: {today}")
        logger.info(f"ğŸ“‚ ì˜¤ëŠ˜ ìƒì„±ëœ íŒŒì¼ ê°œìˆ˜: {len(today_files)}")
        
        # ì˜¤ëŠ˜ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ìºì‹œê°€ ì˜¤ëŠ˜ ê²ƒì´ ì•„ë‹ˆë©´ ìˆ˜ì§‘
        if not today_files or not news_cache.is_today_data():
            logger.info("ğŸ”„ ì˜¤ëŠ˜ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
            force_fetch_thread = threading.Thread(target=force_news_collection, daemon=True)
            force_fetch_thread.start()
        
        # ì§€ì—°ëœ ì²´í¬ (2ë¶„ í›„)
        def delayed_check():
            time.sleep(120)
            if not news_cache.is_today_data():
                logger.warning(f"âš ï¸ 2ë¶„ í›„ì—ë„ ì˜¤ëŠ˜({today}) ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                force_news_collection()
        
        threading.Thread(target=delayed_check, daemon=True).start()
        logger.info("â° 2ë¶„ í›„ ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ ì²´í¬ ì˜ˆì•½ë¨")
        
        logger.info("âœ… ì„œë²„ ì‹œì‘ ì´ë²¤íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ì„œë²„ ì‹œì‘ ì´ë²¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ê¸°ë³¸ ë°ì´í„°ë¡œ ì´ˆê¸°í™”
        try:
            default_data = data_processor.create_default_data(f"ì„œë²„ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            news_cache.update(default_data)
            logger.info("ğŸ”„ ê¸°ë³¸ ë°ì´í„°ë¡œ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e2:
            logger.error(f"âŒ ê¸°ë³¸ ë°ì´í„° ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {str(e2)}")

# === Flutter ì›¹ ì•± ì„œë¹™ ===
if FLUTTER_WEB_DIR.exists():
    app.mount("/static", StaticFiles(directory=str(FLUTTER_WEB_DIR)), name="static")
    
    @app.get("/")
    async def root():
        return FileResponse(str(FLUTTER_WEB_DIR / "index.html"))
    
    @app.get("/{path:path}")
    async def serve_flutter_web(path: str):
        file_path = FLUTTER_WEB_DIR / path
        if file_path.exists() and file_path.is_file():
            return FileResponse(str(file_path))
        return FileResponse(str(FLUTTER_WEB_DIR / "index.html"))
else:
    @app.get("/")
    async def fallback_root():
        return {"message": "ëŒ€ì„  ì‹œë®¬ë ˆì´í„° API ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.", "status": "running"}

# === ì„œë²„ ì‹¤í–‰ ===
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 10000))
    uvicorn.run(app, host="0.0.0.0", port=port)