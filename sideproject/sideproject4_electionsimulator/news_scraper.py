from gnews import GNews
import openai
import datetime
import json
import schedule
import time
from collections import defaultdict
from pathlib import Path
import os
from dotenv import load_dotenv
from typing import List, Dict, Any, Optional
import hashlib
from dataclasses import dataclass
import logging
import backoff  # ë°±ì˜¤í”„ ì¬ì‹œë„ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
import shutil   # íŒŒì¼ ë³µì‚¬ë¥¼ ìœ„í•œ ëª¨ë“ˆ ì¶”ê°€

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
openai.api_key = os.getenv('OPENAI_API_KEY')

# API í‚¤ê°€ ì—†ì„ ê²½ìš° ì—ëŸ¬ ë°œìƒ
if not openai.api_key:
    raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

@dataclass
class NewsArticle:
    """ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° í´ë˜ìŠ¤"""
    title: str
    description: str
    url: str
    published_date: str
    source: str
    query: str  # ì–´ë–¤ ê²€ìƒ‰ì–´ë¡œ ì°¾ì•˜ëŠ”ì§€ ê¸°ë¡
    
    @property
    def unique_id(self) -> str:
        """ê¸°ì‚¬ ê³ ìœ  ID ìƒì„± (URL ê¸°ë°˜)"""
        return hashlib.md5(self.url.encode()).hexdigest()

class NewsCollector:
    """ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° í´ë˜ìŠ¤"""
    def __init__(self, period: str = "12h", max_results: int = 20):
        self.gnews = GNews(language="ko", country="KR", period=period, max_results=max_results)
        self.search_queries = [
            "ëŒ€ì„ ", "ì´ì¬ëª…", "ê¹€ë¬¸ìˆ˜", "ì´ì¤€ì„", 
            "TVí† ë¡ ", "ê³µì•½", "ì—¬ë¡ ì¡°ì‚¬", "í›„ë³´", 
            "ì •ì±…", "ì„ ê±°"
        ]
        self.collected_articles: Dict[str, NewsArticle] = {}
    
    def fetch_news(self, query: str) -> List[Dict[str, Any]]:
        """ë‹¨ì¼ ì¿¼ë¦¬ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        try:
            return self.gnews.get_news(query)
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ (ì¿¼ë¦¬: {query}): {str(e)}")
            return []
    
    def collect_all_news(self) -> List[NewsArticle]:
        """ëª¨ë“  ì¿¼ë¦¬ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì¤‘ë³µ ì œê±°"""
        for query in self.search_queries:
            articles = self.fetch_news(query)
            for article in articles:
                news = NewsArticle(
                    title=article['title'],
                    description=article['description'],
                    url=article['url'],
                    published_date=article.get('published date', ''),
                    source=article.get('publisher', {}).get('title', ''),
                    query=query
                )
                self.collected_articles[news.unique_id] = news
        
        return list(self.collected_articles.values())

# ë‰´ìŠ¤ ì¤‘ìš”ë„ ì •ë ¬ ì „ì—­ í•¨ìˆ˜ - í´ë˜ìŠ¤ ë°–ìœ¼ë¡œ ì´ë™
def rank_news_by_importance(news_data: List[Dict[str, Any]], limit: int = 30) -> List[Dict[str, Any]]:
    """ë‰´ìŠ¤ë¥¼ ì¤‘ìš”ë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ Nê°œë§Œ ë°˜í™˜"""
    if not news_data:
        return []
        
    # í›„ë³´ì ë° í‚¤ì›Œë“œ ì •ì˜
    candidates = ["ì´ì¬ëª…", "ê¹€ë¬¸ìˆ˜", "ì´ì¤€ì„"]
    important_keywords = ["ëŒ€ì„ ", "TVí† ë¡ ", "ê³µì•½", "ì—¬ë¡ ì¡°ì‚¬", "ì •ì±…", "ì§€ì§€ìœ¨", "ì„ ê±°", "ë‹¹ì„ ", "íˆ¬í‘œ"]
    
    # ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
    for news in news_data:
        importance_score = 0
        
        # í›„ë³´ì ì–¸ê¸‰ ì ìˆ˜
        for candidate in candidates:
            if candidate in news['title']:
                importance_score += 15  # ì œëª©ì— í›„ë³´ìê°€ ìˆìœ¼ë©´ ë†’ì€ ì ìˆ˜
            elif candidate in news['summary']:
                importance_score += 8   # ìš”ì•½ì— í›„ë³´ìê°€ ìˆìœ¼ë©´ ì¤‘ê°„ ì ìˆ˜
                
        # ê°ì„± ê°•ë„ ì ìˆ˜ (ì¤‘ë¦½ë³´ë‹¤ ê¸ì •/ë¶€ì •ì´ ë” ì¤‘ìš”í•  ìˆ˜ ìˆìŒ)
        if news['sentiment'] == "ê¸ì •":
            importance_score += 10
        elif news['sentiment'] == "ë¶€ì •":
            importance_score += 12  # ë¶€ì • ë‰´ìŠ¤ê°€ ë³´í†µ ë” ì£¼ëª©ë°›ìŒ
            
        # ì£¼ìš” í‚¤ì›Œë“œ ì ìˆ˜
        for keyword in important_keywords:
            if keyword in news['title']:
                importance_score += 8
            elif keyword in news['summary']:
                importance_score += 4
        
        # ì œëª© ê¸¸ì´ ë³´ë„ˆìŠ¤ (ë³´í†µ ì¤‘ìš”í•œ ê¸°ì‚¬ëŠ” ì œëª©ì´ ê¸¸ë‹¤)
        title_length = len(news['title'])
        if title_length > 30:
            importance_score += 5
        
        # ìµœì‹  ê¸°ì‚¬ ë³´ë„ˆìŠ¤
        try:
            if '2025' in news['published_date']:  # ìµœì‹  ì—°ë„ ê¸°ì‚¬
                importance_score += 10
        except:
            pass
            
        news['importance_score'] = importance_score
    
    # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ Nê°œë§Œ ë°˜í™˜
    sorted_news = sorted(news_data, key=lambda x: x.get('importance_score', 0), reverse=True)
    logger.info(f"âœ… ë‰´ìŠ¤ {len(news_data)}ê°œ ì¤‘ ì¤‘ìš”ë„ìˆœìœ¼ë¡œ ìƒìœ„ {limit}ê°œ ì„ ë³„ ì™„ë£Œ")
    return sorted_news[:limit]

class NewsAnalyzer:
    """ë‰´ìŠ¤ ë¶„ì„ê¸° í´ë˜ìŠ¤"""
    def __init__(self):
        self.candidate_list = ["ì´ì¬ëª…", "ê¹€ë¬¸ìˆ˜", "ì´ì¤€ì„"]
        self.cache_dir = Path("cache")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        # API ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ì¹´ìš´í„° ì¶”ê°€
        self.api_calls_count = 0
        self.last_reset_time = datetime.datetime.now()
    
    def _get_cache_path(self, article_id: str, analysis_type: str) -> Path:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
        return self.cache_dir / f"{article_id}_{analysis_type}.json"
    
    def _load_from_cache(self, article_id: str, analysis_type: str) -> Optional[str]:
        """ìºì‹œì—ì„œ ë¶„ì„ ê²°ê³¼ ë¡œë“œ"""
        cache_path = self._get_cache_path(article_id, analysis_type)
        if cache_path.exists():
            try:
                with open(cache_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    # ìºì‹œ ë§Œë£Œ ì‹œê°„ì„ 24ì‹œê°„ì—ì„œ 7ì¼ë¡œ ì—°ì¥
                    if datetime.datetime.now().timestamp() - data.get("timestamp", 0) < 7 * 86400:
                        logger.info(f"ìºì‹œì—ì„œ {analysis_type} ê²°ê³¼ ë¡œë“œ: {article_id}")
                        return data.get("result")
                    else:
                        logger.info(f"ìºì‹œ ë§Œë£Œë¨: {article_id}_{analysis_type}")
            except Exception as e:
                logger.error(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None
    
    def _save_to_cache(self, article_id: str, analysis_type: str, result: str):
        """ë¶„ì„ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
        cache_path = self._get_cache_path(article_id, analysis_type)
        try:
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump({
                    "result": result,
                    "timestamp": datetime.datetime.now().timestamp()
                }, f, ensure_ascii=False, indent=2)
            logger.info(f"ìºì‹œì— ì €ì¥ ì™„ë£Œ: {article_id}_{analysis_type}")
        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

    def _track_api_usage(self):
        """API ì‚¬ìš©ëŸ‰ ì¶”ì """
        self.api_calls_count += 1
        
        # ì¼ì¼ ì‚¬ìš©ëŸ‰ ë¦¬ì…‹ (ë§¤ì¼ ìì •)
        now = datetime.datetime.now()
        if now.date() > self.last_reset_time.date():
            logger.info(f"ì¼ì¼ API ì‚¬ìš©ëŸ‰ ë¦¬ì…‹: ì´ì „ ì¹´ìš´íŠ¸ {self.api_calls_count}")
            self.api_calls_count = 1
            self.last_reset_time = now
            
        # ì‚¬ìš©ëŸ‰ ë¡œê¹…
        if self.api_calls_count % 10 == 0:
            logger.warning(f"ì£¼ì˜: OpenAI API í˜¸ì¶œ íšŸìˆ˜ê°€ {self.api_calls_count}íšŒ ë„ë‹¬í–ˆìŠµë‹ˆë‹¤")

    @backoff.on_exception(
        backoff.expo,
        (openai.RateLimitError, openai.APIError, openai.AuthenticationError),
        max_tries=3,
        max_time=30
    )
    def summarize_news(self, article_id: str, title: str, description: str) -> str:
        """ê¸°ì‚¬ ìš”ì•½ (ìºì‹œ ì‚¬ìš©)"""
        # ìºì‹œ í™•ì¸
        cached_result = self._load_from_cache(article_id, "summary")
        if cached_result:
            return cached_result

        # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ì€ ê²½ìš° API í˜¸ì¶œ ë°©ì§€
        if len(description) < 50:
            simple_summary = f"{title}ì— ëŒ€í•œ ê°„ëµí•œ ë‚´ìš©."
            self._save_to_cache(article_id, "summary", simple_summary)
            return simple_summary

        system_msg = "ë‹¹ì‹ ì€ ë˜‘ë˜‘í•œ ë‰´ìŠ¤ ìš”ì•½ê°€ì…ë‹ˆë‹¤. í•œêµ­ ì •ì¹˜ë‰´ìŠ¤ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì¤ë‹ˆë‹¤."
        user_msg = f"""[ë‰´ìŠ¤ ì œëª©]: {title}\n[ë‚´ìš© ìš”ì•½ ëŒ€ìƒ]: {description}\n\nì´ ê¸°ì‚¬ë¥¼ 3ì¤„ ì´ë‚´ë¡œ ìš”ì•½í•´ì¤˜."""

        try:
            # API ì‚¬ìš©ëŸ‰ ì¶”ì 
            self._track_api_usage()
            
            logger.info(f"OpenAI API í˜¸ì¶œ: summarize_news - {article_id}")
            response = openai.chat.completions.create(
                model="gpt-4-turbo",
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_msg}
                ],
                temperature=0.5
            )
            result = response.choices[0].message.content.strip()
            # ê²°ê³¼ ìºì‹œì— ì €ì¥
            self._save_to_cache(article_id, "summary", result)
            return result
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ìš”ì•½ ì‹¤íŒ¨: {str(e)}")
            return f"{title}ì˜ ìš”ì•½ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    @backoff.on_exception(
        backoff.expo,
        (openai.RateLimitError, openai.APIError, openai.AuthenticationError),
        max_tries=3,
        max_time=30
    )
    def analyze_sentiment(self, article_id: str, title: str, description: str) -> str:
        """ê°ì„± ë¶„ì„ (ìºì‹œ ì‚¬ìš©)"""
        # ìºì‹œ í™•ì¸
        cached_result = self._load_from_cache(article_id, "sentiment")
        if cached_result:
            return cached_result

        # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ì€ ê²½ìš° API í˜¸ì¶œ ë°©ì§€
        if len(description) < 50:
            default_sentiment = "ì¤‘ë¦½"
            self._save_to_cache(article_id, "sentiment", default_sentiment)
            return default_sentiment

        system_msg = "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ê°ì„± ë¶„ì„ê°€ì…ë‹ˆë‹¤."
        user_msg = f"""[ë‰´ìŠ¤ ì œëª©]: {title}\n[ë‰´ìŠ¤ ì„¤ëª…]: {description}\n\nì´ ë‰´ìŠ¤ëŠ” ê¸ì •ì ì¸ê°€ìš”, ë¶€ì •ì ì¸ê°€ìš”, ì¤‘ë¦½ì ì¸ê°€ìš”? ì•„ë˜ ì¤‘ í•˜ë‚˜ë¡œë§Œ ëŒ€ë‹µí•˜ì„¸ìš”.\n- ê¸ì •\n- ë¶€ì •\n- ì¤‘ë¦½"""

        try:
            # API ì‚¬ìš©ëŸ‰ ì¶”ì 
            self._track_api_usage()
            
            logger.info(f"OpenAI API í˜¸ì¶œ: analyze_sentiment - {article_id}")
            response = openai.chat.completions.create(
                model="gpt-4-turbo",
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_msg}
                ],
                temperature=0.3
            )
            result = response.choices[0].message.content.strip()
            result = result.replace('-', '').replace(':', '').strip()
            # ê²°ê³¼ ìºì‹œì— ì €ì¥
            self._save_to_cache(article_id, "sentiment", result)
            return result
        except Exception as e:
            logger.error(f"ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return "ì¤‘ë¦½"

    def _summarize_news_batch(self, news_batch: List[Dict[str, Any]], batch_num: int, total_batches: int) -> str:
        """ë‰´ìŠ¤ ë°°ì¹˜ ìš”ì•½ (Map ë‹¨ê³„)"""
        # ë°°ì¹˜ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
        if not news_batch:
            return "ë°°ì¹˜ì— ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."
            
        # ìºì‹œ í‚¤ ìƒì„±
        batch_key = hashlib.md5(
            json.dumps(
                [(n['title'], n['url']) for n in news_batch], 
                sort_keys=True
            ).encode()
        ).hexdigest()
        
        # ìºì‹œ í™•ì¸
        cached_result = self._load_from_cache(batch_key, "batch_summary")
        if cached_result:
            return cached_result
        
        system_msg = "ë‹¹ì‹ ì€ ëŒ€ì„  ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ëŠ” ì •ì¹˜ ì „ëµê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë‰´ìŠ¤ë“¤ì„ ìƒì„¸í•˜ê²Œ ë¶„ì„í•´ì£¼ì„¸ìš”."
        
        news_list_str = "\n\n".join([
            f"{i+1}. ì œëª©: {news['title']}\nìš”ì•½: {news['summary']}\nê°ì„±: {news['sentiment']}"
            for i, news in enumerate(news_batch)
        ])

        user_msg = f"""ì•„ë˜ëŠ” ì „ì²´ {total_batches}ê°œ ë°°ì¹˜ ì¤‘ {batch_num}ë²ˆì§¸ ë°°ì¹˜ì˜ ë‰´ìŠ¤ì…ë‹ˆë‹¤:\n\n{news_list_str}\n\nì´ ë‰´ìŠ¤ë“¤ì„ ìƒì„¸í•˜ê²Œ ë¶„ì„í•´ì£¼ì„¸ìš”. ë‹¤ìŒ í•­ëª©ë“¤ì„ í¬í•¨í•´ì£¼ì„¸ìš”:\n1. ì£¼ìš” ì´ìŠˆì™€ í‚¤ì›Œë“œ\n2. í›„ë³´ìë³„ ì–¸ê¸‰ ë¹ˆë„ì™€ ì´ë¯¸ì§€\n3. ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ê¸°ì‚¬ì˜ ë¹„ìœ¨\n4. íŠ¹ì´ì‚¬í•­ì´ë‚˜ ì£¼ëª©í•  ë§Œí•œ íŠ¸ë Œë“œ"""

        try:
            # API ì‚¬ìš©ëŸ‰ ì¶”ì 
            self._track_api_usage()
            
            logger.info(f"OpenAI API í˜¸ì¶œ: batch_summary - ë°°ì¹˜ {batch_num}/{total_batches}")
            response = openai.chat.completions.create(
                model="gpt-4-turbo",
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_msg}
                ],
                temperature=0.5
            )
            result = response.choices[0].message.content.strip()
            # ê²°ê³¼ ìºì‹œì— ì €ì¥
            self._save_to_cache(batch_key, "batch_summary", result)
            return result
        except Exception as e:
            logger.error(f"ë°°ì¹˜ {batch_num} ìš”ì•½ ì‹¤íŒ¨: {str(e)}")
            return f"ë°°ì¹˜ {batch_num} ë¶„ì„ ì‹¤íŒ¨"

    def _create_final_summary(self, batch_summaries: List[str], time_range: str) -> str:
        """ìµœì¢… ìš”ì•½ ìƒì„± (Reduce ë‹¨ê³„)"""
        # ë°°ì¹˜ ìš”ì•½ì´ ë¹„ì–´ìˆìœ¼ë©´ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
        if not batch_summaries:
            return "ë¶„ì„í•  ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            
        # ìºì‹œ í‚¤ ìƒì„±
        summary_key = hashlib.md5(
            json.dumps(
                [s[:100] for s in batch_summaries], 
                sort_keys=True
            ).encode()
        ).hexdigest()
        
        # ìºì‹œ í™•ì¸
        cached_result = self._load_from_cache(summary_key, "final_summary")
        if cached_result:
            return cached_result
            
        system_msg = "ë‹¹ì‹ ì€ ëŒ€ì„  ë‰´ìŠ¤ë¥¼ ì¢…í•© ë¶„ì„í•˜ëŠ” ì •ì¹˜ ì „ëµê°€ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ë°°ì¹˜ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… íŠ¸ë Œë“œë¥¼ ë„ì¶œí•´ì£¼ì„¸ìš”."
        
        summaries_str = "\n\n=== ë°°ì¹˜ë³„ ë¶„ì„ ===\n\n" + "\n\n".join([
            f"[ë°°ì¹˜ {i+1} ë¶„ì„]\n{summary}"
            for i, summary in enumerate(batch_summaries)
        ])

        user_msg = f"""ì•„ë˜ëŠ” {time_range} ë™ì•ˆ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ë¥¼ ì—¬ëŸ¬ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤:\n\n{summaries_str}\n\nì´ ë¶„ì„ ê²°ê³¼ë“¤ì„ ì¢…í•©í•˜ì—¬ ë‹¤ìŒ í•­ëª©ë“¤ì„ í¬í•¨í•œ ìµœì¢… íŠ¸ë Œë“œ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”:\n1. ì „ì²´ì ì¸ ì—¬ë¡  ë™í–¥\n2. í›„ë³´ìë³„ ì´ë¯¸ì§€ì™€ ì§€ì§€ìœ¨ ë³€í™” ì¶”ì´\n3. ì£¼ìš” ì´ìŠˆì™€ í‚¤ì›Œë“œì˜ ë³€í™”\n4. í–¥í›„ ì „ë§"""

        try:
            # API ì‚¬ìš©ëŸ‰ ì¶”ì 
            self._track_api_usage()
            
            logger.info(f"OpenAI API í˜¸ì¶œ: final_summary - {time_range}")
            response = openai.chat.completions.create(
                model="gpt-4-turbo",
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_msg}
                ],
                temperature=0.5
            )
            result = response.choices[0].message.content.strip()
            # ê²°ê³¼ ìºì‹œì— ì €ì¥ 
            self._save_to_cache(summary_key, "final_summary", result)
            return result
        except Exception as e:
            logger.error(f"ìµœì¢… ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return "ìµœì¢… íŠ¸ë Œë“œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def analyze_trends(self, news_data: List[Dict[str, Any]], time_range: str) -> Dict[str, Any]:
        """íŠ¸ë Œë“œ ë¶„ì„ (Map-Reduce ë°©ì‹)"""
        # ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¶„ì„í•˜ì§€ ì•ŠìŒ
        if not news_data:
            logger.warning("ë¶„ì„í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "trend_summary": "í˜„ì¬ ë¶„ì„í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                "candidate_stats": {candidate: {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0} for candidate in self.candidate_list},
                "total_articles": 0,
                "time_range": time_range
            }
            
        # í›„ë³´ë³„ ê°ì„± í†µê³„ ê³„ì‚°
        candidate_stats = defaultdict(lambda: {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0})
        for news in news_data:
            for candidate in self.candidate_list:
                if candidate in news['title'] or candidate in news['summary']:
                    sentiment = news['sentiment']
                    candidate_stats[candidate][sentiment] += 1

        # ë‰´ìŠ¤ ë°ì´í„°ë¥¼ 50ê°œì”© ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
        batch_size = 50
        batches = [news_data[i:i + batch_size] for i in range(0, len(news_data), batch_size)]
        total_batches = len(batches)
        
        # ë¶„ì„í•  ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ë°°ì¹˜ ì²˜ë¦¬ ìƒëµ
        if len(news_data) < 10:
            logger.info(f"ë‰´ìŠ¤ ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ ê°„ë‹¨í•œ ë¶„ì„ë§Œ ìˆ˜í–‰: {len(news_data)}ê°œ")
            return {
                "trend_summary": "í˜„ì¬ ë¶„ì„í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë” ë§ì€ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ë©´ ìƒì„¸í•œ ë¶„ì„ì´ ì œê³µë©ë‹ˆë‹¤.",
                "candidate_stats": dict(candidate_stats),
                "total_articles": len(news_data),
                "time_range": time_range,
                "news_list": news_data[:20]  # ìµœëŒ€ 20ê°œë§Œ í¬í•¨
            }
        
        logger.info(f"ğŸ“Š {total_batches}ê°œ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ë¶„ì„ ì‹œì‘...")
        
        # Map ë‹¨ê³„: ê° ë°°ì¹˜ë³„ ìš”ì•½
        batch_summaries = []
        for i, batch in enumerate(batches, 1):
            logger.info(f"ğŸ”„ ë°°ì¹˜ {i}/{total_batches} ë¶„ì„ ì¤‘...")
            summary = self._summarize_news_batch(batch, i, total_batches)
            batch_summaries.append(summary)
        
        # Reduce ë‹¨ê³„: ìµœì¢… ìš”ì•½ ìƒì„±
        logger.info("ğŸ”„ ìµœì¢… ìš”ì•½ ìƒì„± ì¤‘...")
        final_summary = self._create_final_summary(batch_summaries, time_range)
        
        # ì „ì—­ í•¨ìˆ˜ë¡œ ë‰´ìŠ¤ ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        important_news = rank_news_by_importance(news_data, limit=30)
        
        return {
            "trend_summary": final_summary,
            "candidate_stats": dict(candidate_stats),
            "total_articles": len(news_data),
            "time_range": time_range,
            "news_list": important_news  # ì¤‘ìš”ë„ìˆœìœ¼ë¡œ ì •ë ¬ëœ ë‰´ìŠ¤ ëª©ë¡
        }

class NewsPipeline:
    """ë‰´ìŠ¤ íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    def __init__(self):
        self.collector = NewsCollector()
        self.analyzer = NewsAnalyzer()
        
        # ê²½ë¡œ ì„¤ì • - Render.com í˜¸í™˜ì„± ì¶”ê°€
        self.assets_path = Path("assets")
        self.assets_path.mkdir(parents=True, exist_ok=True)
        
        # Render.com ì˜êµ¬ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        # Render.comì€ /opt/render/project/src/ ê²½ë¡œê°€ ì˜êµ¬ì ìœ¼ë¡œ ìœ ì§€ë¨
        self.render_persistent_dir = None
        if os.environ.get('RENDER') == 'true':  # Render.com í™˜ê²½ ê°ì§€
            self.render_persistent_dir = Path("/opt/render/project/src/persistent_data")
            self.render_persistent_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸ“‚ Render.com ì˜êµ¬ ì €ì¥ì†Œ ê²½ë¡œ ì„¤ì •: {self.render_persistent_dir}")
            
            # ì´ë¯¸ ì €ì¥ëœ ë°ì´í„°ê°€ ìˆë‹¤ë©´ assetsë¡œ ë³µì‚¬
            if self.render_persistent_dir.exists():
                for json_file in self.render_persistent_dir.glob("trend_summary_*.json"):
                    dest_file = self.assets_path / json_file.name
                    if not dest_file.exists():
                        shutil.copy(json_file, dest_file)
                        logger.info(f"ğŸ“‹ ì˜êµ¬ ì €ì¥ì†Œì—ì„œ ë³µì›ëœ íŒŒì¼: {json_file.name}")
        
        self.temp_storage: List[Dict[str, Any]] = []
        self.last_trend_summary_time = None
        self.last_run_date = None
    
    def process_articles(self, articles: List[NewsArticle]) -> List[Dict[str, Any]]:
        """ê¸°ì‚¬ ì²˜ë¦¬ (ìš”ì•½ ë° ê°ì„± ë¶„ì„)"""
        processed_news = []
        for article in articles:
            summary = self.analyzer.summarize_news(article.unique_id, article.title, article.description)
            sentiment = self.analyzer.analyze_sentiment(article.unique_id, article.title, article.description)
            
            processed_news.append({
                "title": article.title,
                "description": article.description,
                "summary": summary,
                "sentiment": sentiment,
                "url": article.url,
                "published_date": article.published_date,
                "source": article.source,
                "query": article.query
            })
        return processed_news
    
    def save_trend_summary(self, trend_data: Dict[str, Any]):
        """íŠ¸ë Œë“œ ìš”ì•½ ì €ì¥"""
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M")
        filename = self.assets_path / f"trend_summary_{timestamp}.json"
        
        # ë¡œì»¬ assets ë””ë ‰í† ë¦¬ì— ì €ì¥
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(trend_data, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ… íŠ¸ë Œë“œ ìš”ì•½ ì €ì¥ ì™„ë£Œ: {filename}")
        
        # Render.com í™˜ê²½ì´ë¼ë©´ ì˜êµ¬ ì €ì¥ì†Œì—ë„ ì €ì¥
        if self.render_persistent_dir:
            persistent_file = self.render_persistent_dir / f"trend_summary_{timestamp}.json"
            try:
                with open(persistent_file, "w", encoding="utf-8") as f:
                    json.dump(trend_data, f, ensure_ascii=False, indent=2)
                logger.info(f"âœ… ì˜êµ¬ ì €ì¥ì†Œì— íŠ¸ë Œë“œ ìš”ì•½ ì €ì¥ ì™„ë£Œ: {persistent_file}")
                
                # ìµœì‹  íŒŒì¼ì„ ê°€ë¦¬í‚¤ëŠ” ë§í¬ íŒŒì¼ ìƒì„± (latest.json)
                latest_link = self.render_persistent_dir / "trend_summary_latest.json"
                with open(latest_link, "w", encoding="utf-8") as f:
                    json.dump(trend_data, f, ensure_ascii=False, indent=2)
                logger.info(f"âœ… ìµœì‹  íŠ¸ë Œë“œ ìš”ì•½ ë§í¬ ìƒì„± ì™„ë£Œ: {latest_link}")
            except Exception as e:
                logger.error(f"âŒ ì˜êµ¬ ì €ì¥ì†Œ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def run_daily_collection(self):
        """ë§¤ì¼ ì‹¤í–‰ë˜ëŠ” ë‰´ìŠ¤ ìˆ˜ì§‘ (ì˜¤ì „ 6ì‹œ)"""
        # ì˜¤ëŠ˜ ì´ë¯¸ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸
        today = datetime.datetime.now().date()
        if self.last_run_date == today:
            logger.info(f"â­ï¸ ì˜¤ëŠ˜({today})ì€ ì´ë¯¸ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            return
            
        logger.info("â³ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        
        # ë‰´ìŠ¤ ìˆ˜ì§‘
        articles = self.collector.collect_all_news()
        logger.info(f"ğŸ“° {len(articles)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # ê¸°ì‚¬ ì²˜ë¦¬
        processed_news = self.process_articles(articles)
        self.temp_storage.extend(processed_news)
        
        # íŠ¸ë Œë“œ ë¶„ì„
        current_time = datetime.datetime.now()
        time_range = f"{current_time.strftime('%Y-%m-%d')} ì—…ë°ì´íŠ¸"
        trend_data = self.analyzer.analyze_trends(self.temp_storage, time_range)
        
        # íŠ¸ë Œë“œ ìš”ì•½ ì €ì¥
        self.save_trend_summary(trend_data)
        
        # ì„ì‹œ ì €ì¥ì†Œ ì´ˆê¸°í™” ë° ì‹œê°„ ì—…ë°ì´íŠ¸
        self.temp_storage = []
        self.last_trend_summary_time = current_time
        self.last_run_date = today
        
        logger.info(f"âœ… ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„ ì™„ë£Œ: {today}")

# ì „ì—­ íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
pipeline = NewsPipeline()

def run_news_pipeline():
    """ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ í˜¸ì¶œë  í•¨ìˆ˜"""
    pipeline.run_daily_collection()

# ì§ì ‘ ì‹¤í–‰ ì‹œ (api_server.pyì—ì„œ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •í•¨)
if __name__ == "__main__":
    logger.info("ğŸ›‘ ì£¼ì˜: ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì§ì ‘ ì‹¤í–‰í•˜ì§€ ë§ê³  api_server.pyë¥¼ í†µí•´ ì‹¤í–‰í•˜ì„¸ìš”.")
    logger.info("ğŸ•’ í…ŒìŠ¤íŠ¸ ëª©ì ìœ¼ë¡œ í•œ ë²ˆì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    run_news_pipeline()
