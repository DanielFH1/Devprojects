"""
ëŒ€ì„  ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„ ì‹œìŠ¤í…œ
- ë‰´ìŠ¤ ìˆ˜ì§‘ (GNews)
- ê°ì„± ë¶„ì„ (OpenAI GPT)
- íŠ¸ë Œë“œ ë¶„ì„ ë° ìš”ì•½
"""

import os
import json
import time
import hashlib
import logging
from pathlib import Path
from datetime import datetime, timedelta
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

import openai
import backoff
import requests
from gnews import GNews

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# === ì„¤ì • ë° ìƒìˆ˜ ===
ASSETS_DIR = Path("assets")
CACHE_DIR = Path("cache")
ASSETS_DIR.mkdir(parents=True, exist_ok=True)
CACHE_DIR.mkdir(parents=True, exist_ok=True)

# API í‚¤ ì„¤ì •
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

if not OPENAI_API_KEY:
    logger.error("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    logger.error("   ë‰´ìŠ¤ ë¶„ì„ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
else:
    logger.info("âœ… OpenAI API í‚¤ í™•ì¸ë¨")

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = None
if OPENAI_API_KEY:
    try:
        openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)
        logger.info("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        openai_client = None

# ê²€ìƒ‰ í‚¤ì›Œë“œ ì„¤ì •
SEARCH_QUERIES = [
    "ì´ì¬ëª… ëŒ€ì„ ",
    "ê¹€ë¬¸ìˆ˜ ëŒ€ì„ ", 
    "ì´ì¤€ì„ ëŒ€ì„ ",
    "2025 ëŒ€ì„ ",
    "21ëŒ€ ëŒ€ì„ "
]

# === ë°ì´í„° í´ë˜ìŠ¤ ===
@dataclass
class NewsArticle:
    """ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° í´ë˜ìŠ¤"""
    title: str
    description: str
    url: str
    published_date: str
    source: str
    query: str
    
    @property
    def unique_id(self) -> str:
        """ê¸°ì‚¬ ê³ ìœ  ID ìƒì„± (URL ê¸°ë°˜)"""
        return hashlib.md5(self.url.encode()).hexdigest()

# === ë‰´ìŠ¤ ìˆ˜ì§‘ í´ë˜ìŠ¤ ===
class NewsCollector:
    """ë‰´ìŠ¤ ìˆ˜ì§‘ ë‹´ë‹¹ í´ë˜ìŠ¤ (GNews ì‚¬ìš©)"""
    
    def __init__(self, period: str = "12h", max_results: int = 20):
        self.period = period
        self.max_results = max_results
        self.gnews = GNews(
            language='ko',
            country='KR',
            max_results=max_results,
            period=period
        )
        logger.info("âœ… GNews í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def fetch_news(self, query: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰"""
        try:
            logger.info(f"ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘: '{query}'")
            
            # GNewsë¡œ ë‰´ìŠ¤ ê²€ìƒ‰
            articles = self.gnews.get_news(query)
            
            # ê²°ê³¼ í¬ë§· ë³€í™˜
            formatted_articles = []
            for article in articles:
                formatted_article = {
                    'title': article.get('title', ''),
                    'description': article.get('description', ''),
                    'url': article.get('url', ''),
                    'publishedAt': article.get('published date', ''),
                    'source': {'name': article.get('publisher', {}).get('title', '') if isinstance(article.get('publisher'), dict) else str(article.get('publisher', ''))}
                }
                formatted_articles.append(formatted_article)
            
            logger.info(f"âœ… '{query}' ê²€ìƒ‰ ê²°ê³¼: {len(formatted_articles)}ê°œ ê¸°ì‚¬")
            return formatted_articles
            
        except Exception as e:
            logger.error(f"âŒ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨ '{query}': {str(e)}")
            return []

    def collect_all_news(self) -> List[NewsArticle]:
        """ëª¨ë“  í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        all_articles = []
        seen_urls = set()
        
        logger.info(f"ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ - í‚¤ì›Œë“œ: {SEARCH_QUERIES}")
        
        for query in SEARCH_QUERIES:
            articles = self.fetch_news(query)
            
            for article in articles:
                url = article.get('url', '')
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    
                    news_article = NewsArticle(
                        title=article.get('title', ''),
                        description=article.get('description', ''),
                        url=url,
                        published_date=article.get('publishedAt', ''),
                        source=article.get('source', {}).get('name', ''),
                        query=query
                    )
                    all_articles.append(news_article)
        
        logger.info(f"âœ… ì´ {len(all_articles)}ê°œì˜ ê³ ìœ  ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_articles

# === ë‰´ìŠ¤ ì¤‘ìš”ë„ í‰ê°€ í•¨ìˆ˜ ===
def rank_news_by_importance(news_data: List[Dict[str, Any]], limit: int = 30) -> List[Dict[str, Any]]:
    """ë‰´ìŠ¤ ì¤‘ìš”ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬"""
    try:
        logger.info(f"ğŸ“Š ë‰´ìŠ¤ ì¤‘ìš”ë„ í‰ê°€ ì‹œì‘: {len(news_data)}ê°œ ê¸°ì‚¬")
        
        for article in news_data:
            score = 0
            title = article.get('title', '').lower()
            summary = article.get('summary', '').lower()
            
            # í›„ë³´ì ì–¸ê¸‰ ì ìˆ˜
            candidates = ['ì´ì¬ëª…', 'ê¹€ë¬¸ìˆ˜', 'ì´ì¤€ì„']
            for candidate in candidates:
                if candidate in title or candidate in summary:
                    score += 10
            
            # í‚¤ì›Œë“œ ì ìˆ˜
            important_keywords = ['ëŒ€ì„ ', 'ì„ ê±°', 'í›„ë³´', 'ì •ì¹˜', 'ì—¬ë¡ ì¡°ì‚¬', 'ì§€ì§€ìœ¨']
            for keyword in important_keywords:
                if keyword in title:
                    score += 5
                if keyword in summary:
                    score += 3
            
            # ê°ì„± ì ìˆ˜
            sentiment = article.get('sentiment', 'ì¤‘ë¦½')
            if sentiment in ['ê¸ì •', 'ë¶€ì •']:
                score += 3
            
            # ì œëª© ê¸¸ì´ ì ìˆ˜ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ì œëª©ì€ ê°ì )
            title_length = len(title)
            if 10 <= title_length <= 50:
                score += 2
            
            article['importance_score'] = score
        
        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_news = sorted(news_data, key=lambda x: x.get('importance_score', 0), reverse=True)
        result = sorted_news[:limit]
        
        logger.info(f"âœ… ì¤‘ìš”ë„ í‰ê°€ ì™„ë£Œ: ìƒìœ„ {len(result)}ê°œ ê¸°ì‚¬ ì„ ë³„")
        return result
        
    except Exception as e:
        logger.error(f"âŒ ë‰´ìŠ¤ ì¤‘ìš”ë„ í‰ê°€ ì‹¤íŒ¨: {str(e)}")
        return news_data[:limit]

# === ë‰´ìŠ¤ ë¶„ì„ í´ë˜ìŠ¤ ===
class NewsAnalyzer:
    """ë‰´ìŠ¤ ë¶„ì„ ë‹´ë‹¹ í´ë˜ìŠ¤ (OpenAI GPT ì‚¬ìš©)"""
    
    def __init__(self):
        self.api_usage_count = 0
        self.daily_limit = 100
        self.cache_enabled = True

    def _get_cache_path(self, article_id: str, analysis_type: str) -> Path:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
        return CACHE_DIR / f"{analysis_type}_{article_id}.json"

    def _load_from_cache(self, article_id: str, analysis_type: str) -> Optional[str]:
        """ìºì‹œì—ì„œ ë¶„ì„ ê²°ê³¼ ë¡œë“œ"""
        if not self.cache_enabled:
            return None
            
        cache_path = self._get_cache_path(article_id, analysis_type)
        if cache_path.exists():
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # ìºì‹œê°€ 24ì‹œê°„ ì´ë‚´ì¸ì§€ í™•ì¸
                    cache_time = datetime.fromisoformat(data['timestamp'])
                    if datetime.now() - cache_time < timedelta(hours=24):
                        logger.debug(f"ğŸ“‹ ìºì‹œì—ì„œ ë¡œë“œ: {analysis_type}_{article_id}")
                        return data['result']
                    else:
                        cache_path.unlink()  # ì˜¤ë˜ëœ ìºì‹œ ì‚­ì œ
            except Exception as e:
                logger.warning(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None

    def _save_to_cache(self, article_id: str, analysis_type: str, result: str):
        """ë¶„ì„ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
        if not self.cache_enabled:
            return
            
        cache_path = self._get_cache_path(article_id, analysis_type)
        try:
            cache_data = {
                'timestamp': datetime.now().isoformat(),
                'result': result
            }
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            logger.debug(f"ğŸ’¾ ìºì‹œì— ì €ì¥: {analysis_type}_{article_id}")
        except Exception as e:
            logger.warning(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

    def _track_api_usage(self):
        """API ì‚¬ìš©ëŸ‰ ì¶”ì """
        self.api_usage_count += 1
        if self.api_usage_count >= self.daily_limit:
            logger.warning(f"âš ï¸ ì¼ì¼ API ì‚¬ìš© í•œë„ ë„ë‹¬: {self.api_usage_count}/{self.daily_limit}")
            return False
        return True

    @backoff.on_exception(
        backoff.expo,
        (openai.RateLimitError, openai.APIError, openai.AuthenticationError),
        max_tries=3,
        max_time=30
    )
    def summarize_news(self, article_id: str, title: str, description: str) -> str:
        """ë‰´ìŠ¤ ìš”ì•½"""
        # ìºì‹œ í™•ì¸
        cached_result = self._load_from_cache(article_id, 'summary')
        if cached_result:
            return cached_result

        if not openai_client:
            return description[:200] + "..." if len(description) > 200 else description

        if not self._track_api_usage():
            return description[:200] + "..."

        try:
            prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”. 2-3ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ë§Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ì œëª©: {title}
ë‚´ìš©: {description}

ìš”ì•½:"""

            response = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=150,
                temperature=0.3
            )
            
            summary = response.choices[0].message.content.strip()
            
            # ìºì‹œì— ì €ì¥
            self._save_to_cache(article_id, 'summary', summary)
            
            logger.debug(f"âœ… ë‰´ìŠ¤ ìš”ì•½ ì™„ë£Œ: {article_id}")
            return summary
            
        except Exception as e:
            logger.error(f"âŒ ë‰´ìŠ¤ ìš”ì•½ ì‹¤íŒ¨: {str(e)}")
            return description[:200] + "..." if len(description) > 200 else description

    @backoff.on_exception(
        backoff.expo,
        (openai.RateLimitError, openai.APIError, openai.AuthenticationError),
        max_tries=3,
        max_time=30
    )
    def analyze_sentiment(self, article_id: str, title: str, description: str) -> str:
        """ê°ì„± ë¶„ì„"""
        # ìºì‹œ í™•ì¸
        cached_result = self._load_from_cache(article_id, 'sentiment')
        if cached_result:
            return cached_result

        if not openai_client:
            return "ì¤‘ë¦½"

        if not self._track_api_usage():
            return "ì¤‘ë¦½"

        try:
            prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ê°ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”. ì •ì¹˜ì  í›„ë³´ìë‚˜ ì •ë‹¹ì— ëŒ€í•œ ì „ë°˜ì ì¸ í†¤ì„ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•´ì£¼ì„¸ìš”.

ì œëª©: {title}
ë‚´ìš©: {description}

ê°ì„±ì„ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:
- ê¸ì •: í›„ë³´ìë‚˜ ì •ë‹¹ì— ëŒ€í•´ í˜¸ì˜ì ì´ê±°ë‚˜ ê¸ì •ì ì¸ ë‚´ìš©
- ë¶€ì •: í›„ë³´ìë‚˜ ì •ë‹¹ì— ëŒ€í•´ ë¹„íŒì ì´ê±°ë‚˜ ë¶€ì •ì ì¸ ë‚´ìš©  
- ì¤‘ë¦½: ê°ê´€ì ì´ê±°ë‚˜ ì¤‘ë¦½ì ì¸ ë³´ë„

ë‹µë³€ì€ "ê¸ì •", "ë¶€ì •", "ì¤‘ë¦½" ì¤‘ í•˜ë‚˜ë§Œ ë‹µí•´ì£¼ì„¸ìš”."""

            response = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=10,
                temperature=0.1
            )
            
            sentiment = response.choices[0].message.content.strip()
            
            # ìœ íš¨í•œ ê°ì„±ì¸ì§€ í™•ì¸
            valid_sentiments = ["ê¸ì •", "ë¶€ì •", "ì¤‘ë¦½"]
            if sentiment not in valid_sentiments:
                sentiment = "ì¤‘ë¦½"
            
            # ìºì‹œì— ì €ì¥
            self._save_to_cache(article_id, 'sentiment', sentiment)
            
            logger.debug(f"âœ… ê°ì„± ë¶„ì„ ì™„ë£Œ: {article_id} -> {sentiment}")
            return sentiment
            
        except Exception as e:
            logger.error(f"âŒ ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return "ì¤‘ë¦½"

    def _summarize_news_batch(self, news_batch: List[Dict[str, Any]], batch_num: int, total_batches: int) -> str:
        """ë‰´ìŠ¤ ë°°ì¹˜ ìš”ì•½"""
        if not openai_client:
            return f"ë°°ì¹˜ {batch_num}: ì´ {len(news_batch)}ê°œì˜ ë‰´ìŠ¤ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤."

        try:
            # ë‰´ìŠ¤ ì œëª©ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
            news_titles = []
            for news in news_batch:
                title = news.get('title', '')
                sentiment = news.get('sentiment', 'ì¤‘ë¦½')
                news_titles.append(f"- {title} ({sentiment})")
            
            news_text = "\n".join(news_titles[:10])  # ìµœëŒ€ 10ê°œë§Œ ì‚¬ìš©
            
            prompt = f"""
ë‹¤ìŒì€ ëŒ€ì„  ê´€ë ¨ ë‰´ìŠ¤ ì œëª©ë“¤ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í˜„ì¬ ì •ì¹˜ ìƒí™©ê³¼ íŠ¸ë Œë“œë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ëª©ë¡:
{news_text}

ìš”ì•½ (2-3ë¬¸ì¥):"""

            response = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=200,
                temperature=0.5
            )
            
            summary = response.choices[0].message.content.strip()
            logger.info(f"âœ… ë°°ì¹˜ {batch_num}/{total_batches} ìš”ì•½ ì™„ë£Œ")
            return summary
            
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ìš”ì•½ ì‹¤íŒ¨: {str(e)}")
            return f"ë°°ì¹˜ {batch_num}: ì´ {len(news_batch)}ê°œì˜ ë‰´ìŠ¤ê°€ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤."

    def _create_final_summary(self, batch_summaries: List[str], time_range: str) -> str:
        """ìµœì¢… íŠ¸ë Œë“œ ìš”ì•½ ìƒì„±"""
        if not openai_client or not batch_summaries:
            return f"{time_range} ê¸°ê°„ ë™ì•ˆì˜ ëŒ€ì„  ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤."

        try:
            combined_summaries = "\n\n".join(batch_summaries)
            
            prompt = f"""
ë‹¤ìŒì€ {time_range} ê¸°ê°„ ë™ì•ˆì˜ ëŒ€ì„  ê´€ë ¨ ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. 
ì´ë¥¼ ì¢…í•©í•˜ì—¬ í˜„ì¬ ëŒ€ì„  ìƒí™©ì˜ ì£¼ìš” íŠ¸ë Œë“œì™€ ì´ìŠˆë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.

ë¶„ì„ ê²°ê³¼:
{combined_summaries}

ì¢…í•© ìš”ì•½ (3-4ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ íŠ¸ë Œë“œ ì •ë¦¬):"""

            response = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=300,
                temperature=0.4
            )
            
            final_summary = response.choices[0].message.content.strip()
            logger.info("âœ… ìµœì¢… íŠ¸ë Œë“œ ìš”ì•½ ìƒì„± ì™„ë£Œ")
            return final_summary
            
        except Exception as e:
            logger.error(f"âŒ ìµœì¢… ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return f"{time_range} ê¸°ê°„ ë™ì•ˆì˜ ëŒ€ì„  ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì¢…í•© ë¶„ì„í–ˆìŠµë‹ˆë‹¤."

    def analyze_trends(self, news_data: List[Dict[str, Any]], time_range: str) -> Dict[str, Any]:
        """ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„"""
        logger.info(f"ğŸ“ˆ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘: {len(news_data)}ê°œ ê¸°ì‚¬")
        
        # í›„ë³´ë³„ í†µê³„ ê³„ì‚°
        candidate_stats = {
            "ì´ì¬ëª…": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0},
            "ê¹€ë¬¸ìˆ˜": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0},
            "ì´ì¤€ì„": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0}
        }
        
        for article in news_data:
            title = article.get('title', '')
            summary = article.get('summary', '')
            sentiment = article.get('sentiment', 'ì¤‘ë¦½')
            
            # í›„ë³´ìë³„ ê°ì„± í†µê³„
            for candidate in candidate_stats.keys():
                if candidate in title or candidate in summary:
                    candidate_stats[candidate][sentiment] += 1
        
        # ë°°ì¹˜ë³„ ìš”ì•½ ìƒì„±
        batch_size = 10
        batches = [news_data[i:i + batch_size] for i in range(0, len(news_data), batch_size)]
        batch_summaries = []
        
        for i, batch in enumerate(batches, 1):
            if len(batch_summaries) >= 5:  # ìµœëŒ€ 5ê°œ ë°°ì¹˜ë§Œ ì²˜ë¦¬
                break
            summary = self._summarize_news_batch(batch, i, len(batches))
            batch_summaries.append(summary)
        
        # ìµœì¢… íŠ¸ë Œë“œ ìš”ì•½
        trend_summary = self._create_final_summary(batch_summaries, time_range)
        
        result = {
            "trend_summary": trend_summary,
            "candidate_stats": candidate_stats,
            "total_articles": len(news_data),
            "time_range": time_range,
            "news_list": news_data
        }
        
        logger.info("âœ… íŠ¸ë Œë“œ ë¶„ì„ ì™„ë£Œ")
        return result

# === ë‰´ìŠ¤ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤ ===
class NewsPipeline:
    """ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        self.collector = NewsCollector()
        self.analyzer = NewsAnalyzer()
        self.last_run_date = None

    def _should_run_today(self) -> bool:
        """ì˜¤ëŠ˜ ì‹¤í–‰í•´ì•¼ í•˜ëŠ”ì§€ í™•ì¸"""
        today = datetime.now().date()
        
        # ê°•ì œ ì‹¤í–‰ ëª¨ë“œ í™•ì¸
        if os.environ.get('FORCE_NEWS_COLLECTION') == 'true':
            logger.info("ğŸ”¥ ê°•ì œ ì‹¤í–‰ ëª¨ë“œ - ì˜¤ëŠ˜ ì‹¤í–‰ ì—¬ë¶€ ë¬´ì‹œ")
            return True
        
        # ì˜¤ëŠ˜ ì´ë¯¸ ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸
        if self.last_run_date == today:
            logger.info(f"â­ï¸ ì˜¤ëŠ˜({today})ì€ ì´ë¯¸ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤.")
            return False
        
        return True

    def process_articles(self, articles: List[NewsArticle]) -> List[Dict[str, Any]]:
        """ê¸°ì‚¬ ì²˜ë¦¬ (ìš”ì•½ ë° ê°ì„± ë¶„ì„)"""
        processed_articles = []
        
        logger.info(f"ğŸ”„ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘: {len(articles)}ê°œ")
        
        for i, article in enumerate(articles, 1):
            try:
                logger.info(f"ğŸ“ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ({i}/{len(articles)}): {article.title[:50]}...")
                
                # ìš”ì•½ ìƒì„±
                summary = self.analyzer.summarize_news(
                    article.unique_id, 
                    article.title, 
                    article.description
                )
                
                # ê°ì„± ë¶„ì„
                sentiment = self.analyzer.analyze_sentiment(
                    article.unique_id,
                    article.title,
                    article.description
                )
                
                processed_article = {
                    "title": article.title,
                    "summary": summary,
                    "url": article.url,
                    "published_date": article.published_date,
                    "source": article.source,
                    "sentiment": sentiment,
                    "query": article.query
                }
                
                processed_articles.append(processed_article)
                
            except Exception as e:
                logger.error(f"âŒ ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                continue
        
        logger.info(f"âœ… ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ: {len(processed_articles)}ê°œ")
        return processed_articles

    def save_trend_summary(self, trend_data: Dict[str, Any]):
        """íŠ¸ë Œë“œ ìš”ì•½ ì €ì¥"""
        try:
            # íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼ëª… ìƒì„±
            timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M")
            filename = f"trend_summary_{timestamp}.json"
            filepath = ASSETS_DIR / filename
            
            # íŒŒì¼ ì €ì¥
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(trend_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ íŠ¸ë Œë“œ ìš”ì•½ ì €ì¥ ì™„ë£Œ: {filename}")
            
            # ì˜êµ¬ ì €ì¥ì†Œì—ë„ ë³µì‚¬ (Render.com í™˜ê²½)
            if os.environ.get('RENDER') == 'true':
                persistent_dir = Path("/opt/render/project/src/persistent_data")
                if persistent_dir.exists():
                    import shutil
                    # ìµœì‹  íŒŒì¼ë¡œ ë³µì‚¬
                    latest_file = persistent_dir / "trend_summary_latest.json"
                    shutil.copy(filepath, latest_file)
                    
                    # ì›ë³¸ íŒŒì¼ë„ ë³µì‚¬
                    perm_file = persistent_dir / filename
                    shutil.copy(filepath, perm_file)
                    
                    logger.info(f"ğŸ“‹ ì˜êµ¬ ì €ì¥ì†Œì— ë³µì‚¬ ì™„ë£Œ: {filename}")
            
        except Exception as e:
            logger.error(f"âŒ íŠ¸ë Œë“œ ìš”ì•½ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

    def run_daily_collection(self):
        """ì¼ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„ ì‹¤í–‰"""
        try:
            start_time = datetime.now()
            logger.info(f"ğŸš€ ì¼ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {start_time}")
            
            # ì‹¤í–‰ ì—¬ë¶€ í™•ì¸
            if not self._should_run_today():
                return
            
            # 1. ë‰´ìŠ¤ ìˆ˜ì§‘
            articles = self.collector.collect_all_news()
            if not articles:
                logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                # ë¹ˆ ë°ì´í„°ë¼ë„ ì˜¤ëŠ˜ ë‚ ì§œë¡œ ì €ì¥
                empty_data = {
                    "trend_summary": "ì˜¤ëŠ˜ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "candidate_stats": {
                        "ì´ì¬ëª…": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0},
                        "ê¹€ë¬¸ìˆ˜": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0},
                        "ì´ì¤€ì„": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0}
                    },
                    "total_articles": 0,
                    "time_range": f"{start_time.strftime('%Y-%m-%d')} ìˆ˜ì§‘",
                    "news_list": []
                }
                self.save_trend_summary(empty_data)
                self.last_run_date = start_time.date()
                return
            
            # 2. ê¸°ì‚¬ ì²˜ë¦¬ (ìš”ì•½ ë° ê°ì„± ë¶„ì„)
            processed_articles = self.process_articles(articles)
            
            # 3. íŠ¸ë Œë“œ ë¶„ì„
            time_range = f"{start_time.strftime('%Y-%m-%d')} ìˆ˜ì§‘"
            trend_data = self.analyzer.analyze_trends(processed_articles, time_range)
            
            # 4. ê²°ê³¼ ì €ì¥
            self.save_trend_summary(trend_data)
            
            # 5. ì‹¤í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.last_run_date = start_time.date()
            
            end_time = datetime.now()
            duration = end_time - start_time
            logger.info(f"âœ… ì¼ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {duration.total_seconds():.1f}ì´ˆ ì†Œìš”")
            
        except Exception as e:
            logger.error(f"âŒ ì¼ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì˜¤ëŠ˜ ë‚ ì§œë¡œ ê¸°ë³¸ ë°ì´í„° ì €ì¥
            try:
                error_data = {
                    "trend_summary": f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                    "candidate_stats": {
                        "ì´ì¬ëª…": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0},
                        "ê¹€ë¬¸ìˆ˜": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0},
                        "ì´ì¤€ì„": {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0}
                    },
                    "total_articles": 0,
                    "time_range": f"{datetime.now().strftime('%Y-%m-%d')} ì˜¤ë¥˜",
                    "news_list": []
                }
                self.save_trend_summary(error_data)
                self.last_run_date = datetime.now().date()
            except Exception as e2:
                logger.error(f"âŒ ì˜¤ë¥˜ ë°ì´í„° ì €ì¥ë„ ì‹¤íŒ¨: {str(e2)}")

# === ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ===
pipeline = NewsPipeline()

# === ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ===
def run_news_pipeline():
    """ë‰´ìŠ¤ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì™¸ë¶€ í˜¸ì¶œìš©)"""
    pipeline.run_daily_collection()

# === ë©”ì¸ ì‹¤í–‰ ===
if __name__ == "__main__":
    logger.info("ğŸ¯ ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ ì§ì ‘ ì‹¤í–‰")
    run_news_pipeline()
