from gnews import GNews
import openai
import datetime
import json
import schedule
import time
from collections import defaultdict
from pathlib import Path
import os
from dotenv import load_dotenv
from typing import List, Dict, Any, Optional
import hashlib
from dataclasses import dataclass
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
openai.api_key = os.getenv('OPENAI_API_KEY')

# API í‚¤ê°€ ì—†ì„ ê²½ìš° ì—ëŸ¬ ë°œìƒ
if not openai.api_key:
    raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

@dataclass
class NewsArticle:
    """ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° í´ë˜ìŠ¤"""
    title: str
    description: str
    url: str
    published_date: str
    source: str
    query: str  # ì–´ë–¤ ê²€ìƒ‰ì–´ë¡œ ì°¾ì•˜ëŠ”ì§€ ê¸°ë¡
    
    @property
    def unique_id(self) -> str:
        """ê¸°ì‚¬ ê³ ìœ  ID ìƒì„± (URL ê¸°ë°˜)"""
        return hashlib.md5(self.url.encode()).hexdigest()

class NewsCollector:
    """ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° í´ë˜ìŠ¤"""
    def __init__(self, period: str = "12h", max_results: int = 20):
        self.gnews = GNews(language="ko", country="KR", period=period, max_results=max_results)
        self.search_queries = [
            "ëŒ€ì„ ", "ì´ì¬ëª…", "ê¹€ë¬¸ìˆ˜", "ì´ì¤€ì„", 
            "TVí† ë¡ ", "ê³µì•½", "ì—¬ë¡ ì¡°ì‚¬", "í›„ë³´", 
            "ì •ì±…", "ì„ ê±°"
        ]
        self.collected_articles: Dict[str, NewsArticle] = {}
    
    def fetch_news(self, query: str) -> List[Dict[str, Any]]:
        """ë‹¨ì¼ ì¿¼ë¦¬ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        try:
            return self.gnews.get_news(query)
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ (ì¿¼ë¦¬: {query}): {str(e)}")
            return []
    
    def collect_all_news(self) -> List[NewsArticle]:
        """ëª¨ë“  ì¿¼ë¦¬ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì¤‘ë³µ ì œê±°"""
        for query in self.search_queries:
            articles = self.fetch_news(query)
            for article in articles:
                news = NewsArticle(
                    title=article['title'],
                    description=article['description'],
                    url=article['url'],
                    published_date=article.get('published date', ''),
                    source=article.get('publisher', {}).get('title', ''),
                    query=query
                )
                self.collected_articles[news.unique_id] = news
        
        return list(self.collected_articles.values())

class NewsAnalyzer:
    """ë‰´ìŠ¤ ë¶„ì„ê¸° í´ë˜ìŠ¤"""
    def __init__(self):
        self.candidate_list = ["ì´ì¬ëª…", "ê¹€ë¬¸ìˆ˜", "ì´ì¤€ì„"]
    
    def summarize_news(self, title: str, description: str) -> str:
        """ê¸°ì‚¬ ìš”ì•½"""
        system_msg = "ë‹¹ì‹ ì€ ë˜‘ë˜‘í•œ ë‰´ìŠ¤ ìš”ì•½ê°€ì…ë‹ˆë‹¤. í•œêµ­ ì •ì¹˜ë‰´ìŠ¤ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì¤ë‹ˆë‹¤."
        user_msg = f"""[ë‰´ìŠ¤ ì œëª©]: {title}\n[ë‚´ìš© ìš”ì•½ ëŒ€ìƒ]: {description}\n\nì´ ê¸°ì‚¬ë¥¼ 3ì¤„ ì´ë‚´ë¡œ ìš”ì•½í•´ì¤˜."""

        try:
            response = openai.chat.completions.create(
                model="gpt-4-turbo",
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_msg}
                ],
                temperature=0.5
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ìš”ì•½ ì‹¤íŒ¨: {str(e)}")
            return ""

    def analyze_sentiment(self, title: str, description: str) -> str:
        """ê°ì„± ë¶„ì„"""
        system_msg = "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ê°ì„± ë¶„ì„ê°€ì…ë‹ˆë‹¤."
        user_msg = f"""[ë‰´ìŠ¤ ì œëª©]: {title}\n[ë‰´ìŠ¤ ì„¤ëª…]: {description}\n\nì´ ë‰´ìŠ¤ëŠ” ê¸ì •ì ì¸ê°€ìš”, ë¶€ì •ì ì¸ê°€ìš”, ì¤‘ë¦½ì ì¸ê°€ìš”? ì•„ë˜ ì¤‘ í•˜ë‚˜ë¡œë§Œ ëŒ€ë‹µí•˜ì„¸ìš”.\n- ê¸ì •\n- ë¶€ì •\n- ì¤‘ë¦½"""

        try:
            response = openai.chat.completions.create(
                model="gpt-4-turbo",
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_msg}
                ],
                temperature=0.3
            )
            result = response.choices[0].message.content.strip()
            return result.replace('-', '').replace(':', '').strip()
        except Exception as e:
            logger.error(f"ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return "ì¤‘ë¦½"

    def analyze_trends(self, news_data: List[Dict[str, Any]], time_range: str) -> Dict[str, Any]:
        """íŠ¸ë Œë“œ ë¶„ì„"""
        system_msg = "ë‹¹ì‹ ì€ ëŒ€ì„  ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ëŠ” ì •ì¹˜ ì „ëµê°€ì…ë‹ˆë‹¤."
        
        # í›„ë³´ë³„ ê°ì„± í†µê³„ ê³„ì‚°
        candidate_stats = defaultdict(lambda: {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0})
        for news in news_data:
            for candidate in self.candidate_list:
                if candidate in news['title'] or candidate in news['summary']:
                    sentiment = news['sentiment']
                    candidate_stats[candidate][sentiment] += 1

        # íŠ¸ë Œë“œ ìš”ì•½ ìƒì„±
        news_list_str = "\n\n".join([
            f"{i+1}. ì œëª©: {news['title']}\nìš”ì•½: {news['summary']}\nê°ì„±: {news['sentiment']}"
            for i, news in enumerate(news_data)
        ])

        user_msg = f"""ì•„ë˜ëŠ” {time_range} ë™ì•ˆ ìˆ˜ì§‘ëœ ëŒ€ì„  ê´€ë ¨ ë‰´ìŠ¤ ìš”ì•½ê³¼ ê°ì„± ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:\n\n{news_list_str}\n\nì´ ë‰´ìŠ¤ë“¤ì„ ì¢…í•©í•´ë³¼ ë•Œ, ìµœê·¼ ì—¬ë¡  íë¦„ì„ ë¶„ì„í•´ì¤˜. í›„ë³´ìë³„ ì´ë¯¸ì§€, ì£¼ìš” ì´ìŠˆ, ê°ì„± íŠ¸ë Œë“œ ë“±ì„ ì•Œë ¤ì¤˜."""

        try:
            response = openai.chat.completions.create(
                model="gpt-4-turbo",
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_msg}
                ],
                temperature=0.5
            )
            trend_summary = response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"íŠ¸ë Œë“œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            trend_summary = "íŠ¸ë Œë“œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        return {
            "trend_summary": trend_summary,
            "candidate_stats": dict(candidate_stats),
            "total_articles": len(news_data),
            "time_range": time_range
        }

class NewsPipeline:
    """ë‰´ìŠ¤ íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    def __init__(self):
        self.collector = NewsCollector()
        self.analyzer = NewsAnalyzer()
        self.assets_path = Path("assets")
        self.assets_path.mkdir(parents=True, exist_ok=True)
        self.temp_storage: List[Dict[str, Any]] = []
        self.last_trend_summary_time = None
    
    def process_articles(self, articles: List[NewsArticle]) -> List[Dict[str, Any]]:
        """ê¸°ì‚¬ ì²˜ë¦¬ (ìš”ì•½ ë° ê°ì„± ë¶„ì„)"""
        processed_news = []
        for article in articles:
            summary = self.analyzer.summarize_news(article.title, article.description)
            sentiment = self.analyzer.analyze_sentiment(article.title, article.description)
            
            processed_news.append({
                "title": article.title,
                "description": article.description,
                "summary": summary,
                "sentiment": sentiment,
                "url": article.url,
                "published_date": article.published_date,
                "source": article.source,
                "query": article.query
            })
        return processed_news
    
    def save_trend_summary(self, trend_data: Dict[str, Any]):
        """íŠ¸ë Œë“œ ìš”ì•½ ì €ì¥"""
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M")
        filename = self.assets_path / f"trend_summary_{timestamp}.json"
        
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(trend_data, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ… íŠ¸ë Œë“œ ìš”ì•½ ì €ì¥ ì™„ë£Œ: {filename}")
    
    def run_hourly_collection(self):
        """1ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ë˜ëŠ” ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì²˜ë¦¬"""
        logger.info("â³ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„ ì‹œì‘...")
        
        # ë‰´ìŠ¤ ìˆ˜ì§‘
        articles = self.collector.collect_all_news()
        logger.info(f"ğŸ“° {len(articles)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # ê¸°ì‚¬ ì²˜ë¦¬
        processed_news = self.process_articles(articles)
        self.temp_storage.extend(processed_news)
        
        # í˜„ì¬ ì‹œê°„ì´ ë§ˆì§€ë§‰ íŠ¸ë Œë“œ ìš”ì•½ìœ¼ë¡œë¶€í„° 3ì‹œê°„ì´ ì§€ë‚¬ëŠ”ì§€ í™•ì¸
        current_time = datetime.datetime.now()
        if (self.last_trend_summary_time is None or 
            (current_time - self.last_trend_summary_time).total_seconds() >= 3 * 3600):
            
            # íŠ¸ë Œë“œ ë¶„ì„
            time_range = f"{self.last_trend_summary_time.strftime('%H:%M') if self.last_trend_summary_time else 'ì‹œì‘'} ~ {current_time.strftime('%H:%M')}"
            trend_data = self.analyzer.analyze_trends(self.temp_storage, time_range)
            
            # íŠ¸ë Œë“œ ìš”ì•½ ì €ì¥
            self.save_trend_summary(trend_data)
            
            # ì„ì‹œ ì €ì¥ì†Œ ì´ˆê¸°í™” ë° ì‹œê°„ ì—…ë°ì´íŠ¸
            self.temp_storage = []
            self.last_trend_summary_time = current_time

# ì „ì—­ íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
pipeline = NewsPipeline()

def run_news_pipeline():
    """ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ í˜¸ì¶œë  í•¨ìˆ˜"""
    pipeline.run_hourly_collection()

# ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
#schedule.every(1).hours.do(run_news_pipeline)
schedule.every().day.at("11:41").do(run_news_pipeline)

if __name__ == "__main__":
    logger.info("ğŸ•’ ìë™ ì‹¤í–‰ ì‹œì‘. ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+C")
    while True:
        schedule.run_pending()
        time.sleep(1)
